{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from filelock import FileLock\n",
    "import numpy as np\n",
    "import ray\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from consistent_hashing import ConsistentHash\n",
    "import math \n",
    "from time import time\n",
    "def get_data_loader():\n",
    "    \n",
    "    \"\"\"Safely downloads data. Returns training/validation set dataloader.\"\"\"\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    \n",
    "    class MNISTEvenOddDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, ready_data):\n",
    "            self.img_data = ready_data.data\n",
    "            self.labels = ready_data.targets % 2\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "        def __getitem__(self, ind):\n",
    "            return torch.true_divide(self.img_data[ind].view(-1, 28 * 28).squeeze(), 255), torch.tensor([self.labels[ind]])\n",
    "\n",
    "\n",
    "    \n",
    "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
    "        \n",
    "        train_dataset = datasets.MNIST(\n",
    "                \"~/data\", train=True, download=True, transform=mnist_transforms\n",
    "            )\n",
    "        \n",
    "        test_dataset = datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            MNISTEvenOddDataset(train_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "             MNISTEvenOddDataset(test_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Evaluates the accuracy of the model on a validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # This is only set to finish evaluation faster.\n",
    "            if batch_idx * len(data) > 1024:\n",
    "                break\n",
    "            outputs = nn.Sigmoid()(model(data))\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = outputs > 0.5\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    \"\"\"Small Linear Network for MNIST.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc_weights = nn.ParameterList([nn.Parameter(torch.empty(1)) for weight in range(784)])\n",
    "        init_fc = [nn.init.uniform_(x) for x in self.fc_weights]\n",
    "        \n",
    "        self.fc_bias = nn.Parameter(torch.empty(1))\n",
    "        nn.init.uniform_(self.fc_bias)\n",
    "        \n",
    "    #def __init__(self):\n",
    "    #    super(LinearNet, self).__init__()\n",
    "    #    self.fc = nn.Linear(28*28, 1)\n",
    "    #    nn.init.normal(self.fc.weight)\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    x = self.fc(x)\n",
    "    #    return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #fc_layer = torch.cat(tuple(self.fc_weights)).unsqueeze(0)\n",
    "        #x = x @ fc_layer.T + self.fc_bias\n",
    "        for i, param in enumerate(self.fc_weights):\n",
    "            if i==0:\n",
    "                p=x[:,i]*param\n",
    "            else:\n",
    "                p += x[:,i]*param\n",
    "        x = p.unsqueeze(1) + self.fc_bias\n",
    "        return x\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "\n",
    "    def set_weights(self, keys, weights): \n",
    "        flatten_weights =  [item for sublist in weights for item in sublist]\n",
    "        self.load_state_dict({keys[i]:flatten_weights[i] for i in range(len(keys))})\n",
    "        \n",
    "    def get_gradients(self, keys):\n",
    "        grads = {}\n",
    "\n",
    "        for name, p in self.named_parameters():\n",
    "            if name in keys:\n",
    "                grad = None if p.grad is None else p.grad.data.cpu().numpy()\n",
    "                grads[name] = grad\n",
    "\n",
    "        return [grads[key] for key in keys]\n",
    "\n",
    "    def set_gradients(self, gradients):\n",
    "        for g, p in zip(gradients, self.parameters()):\n",
    "            if g is not None:\n",
    "                p.grad = torch.from_numpy(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote  \n",
    "class ParameterServer(object):\n",
    "    def __init__(self, keys, values):\n",
    "        self.weights = dict(zip(keys, values))\n",
    "\n",
    "    def apply_gradients(self, keys, lr, *values):\n",
    "        summed_gradients = [\n",
    "            np.stack(gradient_zip).sum(axis=0) for gradient_zip in zip(*values)\n",
    "        ]\n",
    "    \n",
    "        idx = 0\n",
    "        for key, value in zip(keys, summed_gradients):\n",
    "            self.weights[key] -= lr * torch.from_numpy(summed_gradients[idx])\n",
    "            idx+=1\n",
    "\n",
    "        return [self.weights[key] for key in keys]\n",
    "    \n",
    "    def add_weight(self, key, value):\n",
    "        self.weights[key] = value\n",
    "    \n",
    "    def get_len(self):\n",
    "        return len(self.weights)\n",
    "    \n",
    "    def get_weights(self, keys):\n",
    "        return [self.weights[key] for key in keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class DataWorker(object):\n",
    "    def __init__(self, keys):\n",
    "        self.model = LinearNet()\n",
    "        self.data_iterator = iter(get_data_loader()[0])\n",
    "        self.keys = keys\n",
    "        self.key_set = set(self.keys)\n",
    "        for key, value in dict(self.model.named_parameters()).items():\n",
    "            if key not in self.key_set:\n",
    "                value.requires_grad=False\n",
    "\n",
    "        \n",
    "    def update_weights(self, keys, *weights):\n",
    "        self.model.set_weights(keys, weights)\n",
    "        \n",
    "#     def update_weights_selected(self, keys, *weights):\n",
    "#         curr_state_dict = dict(self.model.state_dict().items())\n",
    "#         flatten_weights =  [item for sublist in weights for item in sublist]\n",
    "#         for i, key in enumerate(keys):\n",
    "#             curr_state_dict[keys] = flatten_weights[i]\n",
    "#         self.model.load_state_dict(curr_state_dict)\n",
    "        \n",
    "    def update_trainable(self, keys):\n",
    "        self.keys = keys\n",
    "        self.key_set = set(self.keys)\n",
    "        for key, value in dict(self.model.named_parameters()).items():\n",
    "            if key in self.key_set:\n",
    "                value.requires_grad = True\n",
    "            else:\n",
    "                value.requires_grad = False\n",
    "       \n",
    "\n",
    "    def compute_gradients(self):\n",
    "        #self.model.set_weights(keys, weights)\n",
    "        try:\n",
    "            data, target = next(self.data_iterator)\n",
    "        except StopIteration:  # When the epoch ends, start a new epoch.\n",
    "            self.data_iterator = iter(get_data_loader()[0])\n",
    "            data, target = next(self.data_iterator)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        output = self.model(data)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, target.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        return self.model.get_gradients(self.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 09:13:57,597\tINFO worker.py:963 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "iterations = 500\n",
    "num_workers = 1 # number of workers per server\n",
    "num_servers = 5 # number of servers\n",
    "hashes_per_server = 100\n",
    "\n",
    "def Scheduler(num_servers, hashes_per_server=50):\n",
    "    \n",
    "    model = LinearNet()\n",
    "    key_values = model.get_weights()\n",
    "    keys = np.array(list(key_values.keys()))\n",
    "    #print(keys)\n",
    "    #print(key_values) z\n",
    "    values = [key_values[key] for key in keys]\n",
    "    #values = [key_values[key] for key in keys]\n",
    "    \n",
    "    key_indices = {key: x for x, key in enumerate(keys)}\n",
    "   \n",
    "    # distributing weights across servers - do this using consistency hashing\n",
    "    server_ids = [\"server\" + str(ind) for ind in range(num_servers)]\n",
    "    hasher = ConsistentHash(keys, server_ids, hashes_per_server)\n",
    "    servers = [ParameterServer.remote(keys[[key_indices[key] for key in hasher.get_keys_per_node()[serv]]], \n",
    "                                      [values[key_indices[key]] for key in hasher.get_keys_per_node()[serv]]) for serv in server_ids]\n",
    "    # servers = [ParameterServer.remote(keys[0:1], values[0:1]), ParameterServer.remote(keys[1:2], values[1:2])]\n",
    "    \n",
    "    return hasher, servers, keys, model, hasher.get_keys_per_node(), server_ids.copy()\n",
    "\n",
    "hasher, servers, keys, model, weight_assignments, server_ids =  Scheduler(num_servers, hashes_per_server)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# creating equal workers per server\n",
    "\n",
    "workers = [[DataWorker.remote(weight_assignments[\"server\" + str(j)]) for i in range(num_workers)] for j in range(num_servers)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight_assignments[\"server0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_data_loader()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running synchronous parameter server training.\n",
      "Iter 0: \taccuracy is 51.9\n",
      "Iter 1: \taccuracy is 51.9\n",
      "Iter 2: \taccuracy is 51.9\n",
      "Iter 3: \taccuracy is 51.9\n",
      "Iter 4: \taccuracy is 51.9\n",
      "Iter 5: \taccuracy is 51.9\n",
      "Iter 6: \taccuracy is 51.9\n",
      "Iter 7: \taccuracy is 51.9\n",
      "Iter 8: \taccuracy is 51.9\n",
      "Iter 9: \taccuracy is 51.9\n",
      "Iter 10: \taccuracy is 51.9\n",
      "Iter 11: \taccuracy is 51.9\n",
      "Iter 12: \taccuracy is 51.9\n",
      "Iter 13: \taccuracy is 51.9\n",
      "Iter 14: \taccuracy is 51.9\n",
      "Iter 15: \taccuracy is 51.9\n",
      "Iter 16: \taccuracy is 51.9\n",
      "Iter 17: \taccuracy is 51.9\n",
      "Iter 18: \taccuracy is 51.9\n",
      "Iter 19: \taccuracy is 51.9\n",
      "Iter 20: \taccuracy is 52.9\n",
      "Iter 21: \taccuracy is 55.4\n",
      "Iter 22: \taccuracy is 59.4\n",
      "Iter 23: \taccuracy is 57.9\n",
      "Iter 24: \taccuracy is 54.3\n",
      "Iter 25: \taccuracy is 52.8\n",
      "Iter 26: \taccuracy is 50.5\n",
      "Iter 27: \taccuracy is 49.7\n",
      "Iter 28: \taccuracy is 48.4\n",
      "Iter 29: \taccuracy is 48.2\n",
      "Iter 30: \taccuracy is 48.9\n",
      "Iter 31: \taccuracy is 49.7\n",
      "Iter 32: \taccuracy is 51.0\n",
      "Iter 33: \taccuracy is 54.9\n",
      "Iter 34: \taccuracy is 58.8\n",
      "Iter 35: \taccuracy is 64.1\n",
      "Iter 36: \taccuracy is 67.4\n",
      "Iter 37: \taccuracy is 68.8\n",
      "Iter 38: \taccuracy is 65.5\n",
      "Iter 39: \taccuracy is 61.9\n",
      "Iter 40: \taccuracy is 66.1\n",
      "Iter 41: \taccuracy is 69.9\n",
      "Iter 42: \taccuracy is 70.2\n",
      "Iter 43: \taccuracy is 70.7\n",
      "Iter 44: \taccuracy is 67.4\n",
      "Iter 45: \taccuracy is 61.5\n",
      "Iter 46: \taccuracy is 57.5\n",
      "Iter 47: \taccuracy is 53.4\n",
      "Iter 48: \taccuracy is 50.9\n",
      "Iter 49: \taccuracy is 49.8\n",
      "Iter 50: \taccuracy is 51.3\n",
      "Iter 51: \taccuracy is 55.3\n",
      "Iter 52: \taccuracy is 61.1\n",
      "Iter 53: \taccuracy is 67.4\n",
      "Iter 54: \taccuracy is 72.8\n",
      "Iter 55: \taccuracy is 76.0\n",
      "Iter 56: \taccuracy is 76.0\n",
      "Iter 57: \taccuracy is 71.6\n",
      "Iter 58: \taccuracy is 68.1\n",
      "Iter 59: \taccuracy is 64.9\n",
      "Iter 60: \taccuracy is 68.2\n",
      "Iter 61: \taccuracy is 71.8\n",
      "Iter 62: \taccuracy is 75.6\n",
      "Iter 63: \taccuracy is 77.5\n",
      "Iter 64: \taccuracy is 76.7\n",
      "Iter 65: \taccuracy is 76.0\n",
      "Iter 66: \taccuracy is 71.9\n",
      "Iter 67: \taccuracy is 67.4\n",
      "Iter 68: \taccuracy is 62.6\n",
      "Iter 69: \taccuracy is 57.7\n",
      "Iter 70: \taccuracy is 62.4\n",
      "Iter 71: \taccuracy is 67.7\n",
      "Iter 72: \taccuracy is 73.5\n",
      "Iter 73: \taccuracy is 77.0\n",
      "Iter 74: \taccuracy is 79.2\n",
      "Iter 75: \taccuracy is 78.4\n",
      "Iter 76: \taccuracy is 77.6\n",
      "Iter 77: \taccuracy is 76.0\n",
      "Iter 78: \taccuracy is 73.4\n",
      "Iter 79: \taccuracy is 70.8\n",
      "Iter 80: \taccuracy is 73.5\n",
      "Iter 81: \taccuracy is 75.0\n",
      "Iter 82: \taccuracy is 77.2\n",
      "Iter 83: \taccuracy is 78.3\n",
      "Iter 84: \taccuracy is 80.1\n",
      "Iter 85: \taccuracy is 79.9\n",
      "Iter 86: \taccuracy is 79.6\n",
      "Iter 87: \taccuracy is 77.7\n",
      "Iter 88: \taccuracy is 74.7\n",
      "Iter 89: \taccuracy is 70.1\n",
      "Iter 90: \taccuracy is 74.2\n",
      "Iter 91: \taccuracy is 77.9\n",
      "Iter 92: \taccuracy is 79.1\n",
      "Iter 93: \taccuracy is 80.9\n",
      "Iter 94: \taccuracy is 81.1\n",
      "Iter 95: \taccuracy is 80.7\n",
      "Iter 96: \taccuracy is 79.8\n",
      "Iter 97: \taccuracy is 78.6\n",
      "Iter 98: \taccuracy is 77.0\n",
      "Iter 99: \taccuracy is 75.3\n",
      "Iter 100: \taccuracy is 76.8\n",
      "Iter 101: \taccuracy is 77.4\n",
      "Iter 102: \taccuracy is 79.3\n",
      "Iter 103: \taccuracy is 80.1\n",
      "Iter 104: \taccuracy is 81.2\n",
      "Iter 105: \taccuracy is 81.9\n",
      "Iter 106: \taccuracy is 81.4\n",
      "Iter 107: \taccuracy is 81.5\n",
      "Iter 108: \taccuracy is 79.9\n",
      "Iter 109: \taccuracy is 78.0\n",
      "Iter 110: \taccuracy is 78.9\n",
      "Iter 111: \taccuracy is 80.4\n",
      "Iter 112: \taccuracy is 81.3\n",
      "Iter 113: \taccuracy is 81.7\n",
      "Iter 114: \taccuracy is 82.2\n",
      "Iter 115: \taccuracy is 81.9\n",
      "Iter 116: \taccuracy is 81.2\n",
      "Iter 117: \taccuracy is 80.6\n",
      "Iter 118: \taccuracy is 80.3\n",
      "Iter 119: \taccuracy is 78.0\n",
      "Iter 120: \taccuracy is 79.9\n",
      "Iter 121: \taccuracy is 80.8\n",
      "Iter 122: \taccuracy is 81.3\n",
      "Iter 123: \taccuracy is 81.2\n",
      "Iter 124: \taccuracy is 82.1\n",
      "Iter 125: \taccuracy is 82.2\n",
      "Iter 126: \taccuracy is 82.2\n",
      "Iter 127: \taccuracy is 82.0\n",
      "Iter 128: \taccuracy is 81.9\n",
      "Iter 129: \taccuracy is 81.5\n",
      "Iter 130: \taccuracy is 81.9\n",
      "Iter 131: \taccuracy is 82.2\n",
      "Iter 132: \taccuracy is 82.0\n",
      "Iter 133: \taccuracy is 82.3\n",
      "Iter 134: \taccuracy is 82.6\n",
      "Iter 135: \taccuracy is 82.2\n",
      "Iter 136: \taccuracy is 82.3\n",
      "Iter 137: \taccuracy is 81.4\n",
      "Iter 138: \taccuracy is 81.2\n",
      "Iter 139: \taccuracy is 80.2\n",
      "Iter 140: \taccuracy is 81.1\n",
      "Iter 141: \taccuracy is 81.7\n",
      "Iter 142: \taccuracy is 82.5\n",
      "Iter 143: \taccuracy is 82.7\n",
      "Iter 144: \taccuracy is 82.7\n",
      "Iter 145: \taccuracy is 82.7\n",
      "Iter 146: \taccuracy is 82.2\n",
      "Iter 147: \taccuracy is 82.5\n",
      "Iter 148: \taccuracy is 82.5\n",
      "Iter 149: \taccuracy is 82.4\n",
      "Iter 150: \taccuracy is 82.6\n",
      "Iter 151: \taccuracy is 82.6\n",
      "Iter 152: \taccuracy is 82.6\n",
      "Iter 153: \taccuracy is 82.8\n",
      "Iter 154: \taccuracy is 82.7\n",
      "Iter 155: \taccuracy is 82.9\n",
      "Iter 156: \taccuracy is 83.2\n",
      "Iter 157: \taccuracy is 82.8\n",
      "Iter 158: \taccuracy is 82.6\n",
      "Iter 159: \taccuracy is 81.5\n",
      "Iter 160: \taccuracy is 82.7\n",
      "Iter 161: \taccuracy is 82.7\n",
      "Iter 162: \taccuracy is 83.1\n",
      "Iter 163: \taccuracy is 83.5\n",
      "Iter 164: \taccuracy is 82.9\n",
      "Iter 165: \taccuracy is 83.2\n",
      "Iter 166: \taccuracy is 83.1\n",
      "Iter 167: \taccuracy is 83.2\n",
      "Iter 168: \taccuracy is 82.9\n",
      "Iter 169: \taccuracy is 82.6\n",
      "Iter 170: \taccuracy is 83.0\n",
      "Iter 171: \taccuracy is 83.2\n",
      "Iter 172: \taccuracy is 83.3\n",
      "Iter 173: \taccuracy is 83.7\n",
      "Iter 174: \taccuracy is 83.3\n",
      "Iter 175: \taccuracy is 83.0\n",
      "Iter 176: \taccuracy is 83.4\n",
      "Iter 177: \taccuracy is 83.9\n",
      "Iter 178: \taccuracy is 83.2\n",
      "Iter 179: \taccuracy is 82.8\n",
      "Iter 180: \taccuracy is 83.2\n",
      "Iter 181: \taccuracy is 83.3\n",
      "Iter 182: \taccuracy is 83.6\n",
      "Iter 183: \taccuracy is 83.6\n",
      "Iter 184: \taccuracy is 83.8\n",
      "Iter 185: \taccuracy is 83.7\n",
      "Iter 186: \taccuracy is 83.4\n",
      "Iter 187: \taccuracy is 83.8\n",
      "Iter 188: \taccuracy is 83.9\n",
      "Iter 189: \taccuracy is 83.6\n",
      "Iter 190: \taccuracy is 83.9\n",
      "Iter 191: \taccuracy is 84.0\n",
      "Iter 192: \taccuracy is 83.9\n",
      "Iter 193: \taccuracy is 83.9\n",
      "Iter 194: \taccuracy is 84.1\n",
      "Iter 195: \taccuracy is 83.9\n",
      "Iter 196: \taccuracy is 84.1\n",
      "Iter 197: \taccuracy is 84.2\n",
      "Iter 198: \taccuracy is 84.3\n",
      "Iter 199: \taccuracy is 84.0\n",
      "Iter 200: \taccuracy is 84.2\n",
      "Iter 201: \taccuracy is 84.2\n",
      "Iter 202: \taccuracy is 84.2\n",
      "Iter 203: \taccuracy is 84.2\n",
      "Iter 204: \taccuracy is 84.3\n",
      "Iter 205: \taccuracy is 84.5\n",
      "Iter 206: \taccuracy is 84.4\n",
      "Iter 207: \taccuracy is 84.1\n",
      "Iter 208: \taccuracy is 84.0\n",
      "Iter 209: \taccuracy is 84.3\n",
      "Iter 210: \taccuracy is 84.1\n",
      "Iter 211: \taccuracy is 84.4\n",
      "Iter 212: \taccuracy is 84.6\n",
      "Iter 213: \taccuracy is 84.2\n",
      "Iter 214: \taccuracy is 84.5\n",
      "Iter 215: \taccuracy is 84.4\n",
      "Iter 216: \taccuracy is 84.7\n",
      "Iter 217: \taccuracy is 84.6\n",
      "Iter 218: \taccuracy is 84.4\n",
      "Iter 219: \taccuracy is 84.7\n",
      "Iter 220: \taccuracy is 84.7\n",
      "Iter 221: \taccuracy is 84.5\n",
      "Iter 222: \taccuracy is 84.5\n",
      "Iter 223: \taccuracy is 84.7\n",
      "Iter 224: \taccuracy is 84.7\n",
      "Iter 225: \taccuracy is 84.6\n",
      "Iter 226: \taccuracy is 84.6\n",
      "Iter 227: \taccuracy is 84.6\n",
      "Iter 228: \taccuracy is 84.5\n",
      "Iter 229: \taccuracy is 84.6\n",
      "Iter 230: \taccuracy is 84.5\n",
      "Iter 231: \taccuracy is 84.6\n",
      "Iter 232: \taccuracy is 84.5\n",
      "Iter 233: \taccuracy is 84.5\n",
      "Iter 234: \taccuracy is 84.6\n",
      "Iter 235: \taccuracy is 84.8\n",
      "Iter 236: \taccuracy is 84.5\n",
      "Iter 237: \taccuracy is 84.6\n",
      "Iter 238: \taccuracy is 84.7\n",
      "Iter 239: \taccuracy is 84.9\n",
      "Iter 240: \taccuracy is 85.0\n",
      "Iter 241: \taccuracy is 84.6\n",
      "Iter 242: \taccuracy is 84.8\n",
      "Iter 243: \taccuracy is 84.7\n",
      "Iter 244: \taccuracy is 84.6\n",
      "Iter 245: \taccuracy is 84.6\n",
      "Iter 246: \taccuracy is 84.7\n",
      "Iter 247: \taccuracy is 84.8\n",
      "Iter 248: \taccuracy is 84.6\n",
      "Iter 249: \taccuracy is 84.8\n",
      "Iter 250: \taccuracy is 84.8\n",
      "Iter 251: \taccuracy is 84.8\n",
      "Iter 252: \taccuracy is 84.8\n",
      "Iter 253: \taccuracy is 85.0\n",
      "Iter 254: \taccuracy is 85.3\n",
      "Iter 255: \taccuracy is 85.0\n",
      "Iter 256: \taccuracy is 85.2\n",
      "Iter 257: \taccuracy is 85.1\n",
      "Iter 258: \taccuracy is 85.1\n",
      "Iter 259: \taccuracy is 84.8\n",
      "Iter 260: \taccuracy is 85.0\n",
      "Iter 261: \taccuracy is 85.2\n",
      "Iter 262: \taccuracy is 85.3\n",
      "Iter 263: \taccuracy is 85.3\n",
      "Iter 264: \taccuracy is 84.9\n",
      "Iter 265: \taccuracy is 84.5\n",
      "Iter 266: \taccuracy is 84.7\n",
      "Iter 267: \taccuracy is 84.5\n",
      "Iter 268: \taccuracy is 84.4\n",
      "Iter 269: \taccuracy is 83.6\n",
      "Iter 270: \taccuracy is 84.0\n",
      "Iter 271: \taccuracy is 84.3\n",
      "Iter 272: \taccuracy is 84.8\n",
      "Iter 273: \taccuracy is 84.6\n",
      "Iter 274: \taccuracy is 85.4\n",
      "Iter 275: \taccuracy is 85.7\n",
      "Iter 276: \taccuracy is 85.3\n",
      "Iter 277: \taccuracy is 85.2\n",
      "Iter 278: \taccuracy is 84.8\n",
      "Iter 279: \taccuracy is 84.7\n",
      "Iter 280: \taccuracy is 84.9\n",
      "Iter 281: \taccuracy is 85.3\n",
      "Iter 282: \taccuracy is 85.4\n",
      "Iter 283: \taccuracy is 85.2\n",
      "Iter 284: \taccuracy is 85.1\n",
      "Iter 285: \taccuracy is 84.8\n",
      "Iter 286: \taccuracy is 84.5\n",
      "Iter 287: \taccuracy is 83.9\n",
      "Iter 288: \taccuracy is 83.9\n",
      "Iter 289: \taccuracy is 83.8\n",
      "Iter 290: \taccuracy is 83.9\n",
      "Iter 291: \taccuracy is 84.2\n",
      "Iter 292: \taccuracy is 84.8\n",
      "Iter 293: \taccuracy is 85.1\n",
      "Iter 294: \taccuracy is 85.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 295: \taccuracy is 85.9\n",
      "Iter 296: \taccuracy is 85.0\n",
      "Iter 297: \taccuracy is 85.2\n",
      "Iter 298: \taccuracy is 84.9\n",
      "Iter 299: \taccuracy is 84.0\n",
      "Iter 300: \taccuracy is 85.0\n",
      "Iter 301: \taccuracy is 85.3\n",
      "Iter 302: \taccuracy is 85.3\n",
      "Iter 303: \taccuracy is 85.9\n",
      "Iter 304: \taccuracy is 85.2\n",
      "Iter 305: \taccuracy is 84.6\n",
      "Iter 306: \taccuracy is 83.9\n",
      "Iter 307: \taccuracy is 83.3\n",
      "Iter 308: \taccuracy is 82.7\n",
      "Iter 309: \taccuracy is 82.1\n",
      "Iter 310: \taccuracy is 83.1\n",
      "Iter 311: \taccuracy is 83.6\n",
      "Iter 312: \taccuracy is 84.5\n",
      "Iter 313: \taccuracy is 85.1\n",
      "Iter 314: \taccuracy is 85.5\n",
      "Iter 315: \taccuracy is 85.2\n",
      "Iter 316: \taccuracy is 85.5\n",
      "Iter 317: \taccuracy is 84.4\n",
      "Iter 318: \taccuracy is 83.9\n",
      "Iter 319: \taccuracy is 83.2\n",
      "Iter 320: \taccuracy is 83.9\n",
      "Iter 321: \taccuracy is 84.5\n",
      "Iter 322: \taccuracy is 85.3\n",
      "Iter 323: \taccuracy is 85.7\n",
      "Iter 324: \taccuracy is 85.8\n",
      "Iter 325: \taccuracy is 84.5\n",
      "Iter 326: \taccuracy is 83.4\n",
      "Iter 327: \taccuracy is 82.2\n",
      "Iter 328: \taccuracy is 81.5\n",
      "Iter 329: \taccuracy is 80.0\n",
      "Iter 330: \taccuracy is 82.1\n",
      "Iter 331: \taccuracy is 83.2\n",
      "Iter 332: \taccuracy is 84.3\n",
      "Iter 333: \taccuracy is 85.0\n",
      "Iter 334: \taccuracy is 85.9\n",
      "Iter 335: \taccuracy is 85.8\n",
      "Iter 336: \taccuracy is 84.7\n",
      "Iter 337: \taccuracy is 84.3\n",
      "Iter 338: \taccuracy is 82.3\n",
      "Iter 339: \taccuracy is 80.0\n",
      "Iter 340: \taccuracy is 83.0\n",
      "Iter 341: \taccuracy is 83.9\n",
      "Iter 342: \taccuracy is 85.9\n",
      "Iter 343: \taccuracy is 86.4\n",
      "Iter 344: \taccuracy is 85.3\n",
      "Iter 345: \taccuracy is 84.1\n",
      "Iter 346: \taccuracy is 83.2\n",
      "Iter 347: \taccuracy is 81.2\n",
      "Iter 348: \taccuracy is 79.3\n",
      "Iter 349: \taccuracy is 77.5\n",
      "Iter 350: \taccuracy is 80.0\n",
      "Iter 351: \taccuracy is 82.1\n",
      "Iter 352: \taccuracy is 83.8\n",
      "Iter 353: \taccuracy is 85.5\n",
      "Iter 354: \taccuracy is 86.3\n",
      "Iter 355: \taccuracy is 85.8\n",
      "Iter 356: \taccuracy is 84.5\n",
      "Iter 357: \taccuracy is 82.3\n",
      "Iter 358: \taccuracy is 79.4\n",
      "Iter 359: \taccuracy is 74.3\n",
      "Iter 360: \taccuracy is 79.1\n",
      "Iter 361: \taccuracy is 82.4\n",
      "Iter 362: \taccuracy is 84.5\n",
      "Iter 363: \taccuracy is 85.9\n",
      "Iter 364: \taccuracy is 86.1\n",
      "Iter 365: \taccuracy is 84.6\n",
      "Iter 366: \taccuracy is 83.3\n",
      "Iter 367: \taccuracy is 81.8\n",
      "Iter 368: \taccuracy is 79.3\n",
      "Iter 369: \taccuracy is 76.6\n",
      "Iter 370: \taccuracy is 79.3\n",
      "Iter 371: \taccuracy is 81.8\n",
      "Iter 372: \taccuracy is 83.2\n",
      "Iter 373: \taccuracy is 84.5\n",
      "Iter 374: \taccuracy is 86.4\n",
      "Iter 375: \taccuracy is 86.2\n",
      "Iter 376: \taccuracy is 85.0\n",
      "Iter 377: \taccuracy is 83.2\n",
      "Iter 378: \taccuracy is 80.4\n",
      "Iter 379: \taccuracy is 75.1\n",
      "Iter 380: \taccuracy is 80.3\n",
      "Iter 381: \taccuracy is 82.7\n",
      "Iter 382: \taccuracy is 84.8\n",
      "Iter 383: \taccuracy is 85.9\n",
      "Iter 384: \taccuracy is 86.3\n",
      "Iter 385: \taccuracy is 85.5\n",
      "Iter 386: \taccuracy is 83.3\n",
      "Iter 387: \taccuracy is 82.2\n",
      "Iter 388: \taccuracy is 79.7\n",
      "Iter 389: \taccuracy is 77.3\n",
      "Iter 390: \taccuracy is 79.7\n",
      "Iter 391: \taccuracy is 82.3\n",
      "Iter 392: \taccuracy is 83.2\n",
      "Iter 393: \taccuracy is 85.2\n",
      "Iter 394: \taccuracy is 86.5\n",
      "Iter 395: \taccuracy is 86.8\n",
      "Iter 396: \taccuracy is 86.1\n",
      "Iter 397: \taccuracy is 84.4\n",
      "Iter 398: \taccuracy is 82.2\n",
      "Iter 399: \taccuracy is 79.5\n",
      "Iter 400: \taccuracy is 81.9\n",
      "Iter 401: \taccuracy is 83.9\n",
      "Iter 402: \taccuracy is 85.5\n",
      "Iter 403: \taccuracy is 86.4\n",
      "Iter 404: \taccuracy is 86.6\n",
      "Iter 405: \taccuracy is 86.4\n",
      "Iter 406: \taccuracy is 84.8\n",
      "Iter 407: \taccuracy is 83.5\n",
      "Iter 408: \taccuracy is 82.2\n",
      "Iter 409: \taccuracy is 80.1\n",
      "Iter 410: \taccuracy is 81.4\n",
      "Iter 411: \taccuracy is 82.8\n",
      "Iter 412: \taccuracy is 83.6\n",
      "Iter 413: \taccuracy is 85.3\n",
      "Iter 414: \taccuracy is 86.5\n",
      "Iter 415: \taccuracy is 86.9\n",
      "Iter 416: \taccuracy is 86.7\n",
      "Iter 417: \taccuracy is 86.1\n",
      "Iter 418: \taccuracy is 84.6\n",
      "Iter 419: \taccuracy is 83.0\n",
      "Iter 420: \taccuracy is 84.6\n",
      "Iter 421: \taccuracy is 85.7\n",
      "Iter 422: \taccuracy is 86.4\n",
      "Iter 423: \taccuracy is 87.1\n",
      "Iter 424: \taccuracy is 87.0\n",
      "Iter 425: \taccuracy is 86.4\n",
      "Iter 426: \taccuracy is 85.6\n",
      "Iter 427: \taccuracy is 84.3\n",
      "Iter 428: \taccuracy is 83.3\n",
      "Iter 429: \taccuracy is 82.8\n",
      "Iter 430: \taccuracy is 83.4\n",
      "Iter 431: \taccuracy is 84.1\n",
      "Iter 432: \taccuracy is 85.4\n",
      "Iter 433: \taccuracy is 85.7\n",
      "Iter 434: \taccuracy is 86.6\n",
      "Iter 435: \taccuracy is 87.2\n",
      "Iter 436: \taccuracy is 87.0\n",
      "Iter 437: \taccuracy is 87.0\n",
      "Iter 438: \taccuracy is 86.4\n",
      "Iter 439: \taccuracy is 85.4\n",
      "Iter 440: \taccuracy is 86.4\n",
      "Iter 441: \taccuracy is 87.0\n",
      "Iter 442: \taccuracy is 87.1\n",
      "Iter 443: \taccuracy is 87.0\n",
      "Iter 444: \taccuracy is 87.1\n",
      "Iter 445: \taccuracy is 86.6\n",
      "Iter 446: \taccuracy is 86.1\n",
      "Iter 447: \taccuracy is 85.6\n",
      "Iter 448: \taccuracy is 84.4\n",
      "Iter 449: \taccuracy is 84.4\n",
      "Iter 450: \taccuracy is 84.4\n",
      "Iter 451: \taccuracy is 85.7\n",
      "Iter 452: \taccuracy is 85.9\n",
      "Iter 453: \taccuracy is 86.6\n",
      "Iter 454: \taccuracy is 87.1\n",
      "Iter 455: \taccuracy is 87.1\n",
      "Iter 456: \taccuracy is 87.1\n",
      "Iter 457: \taccuracy is 87.2\n",
      "Iter 458: \taccuracy is 86.7\n",
      "Iter 459: \taccuracy is 86.7\n",
      "Iter 460: \taccuracy is 86.7\n",
      "Iter 461: \taccuracy is 87.5\n",
      "Iter 462: \taccuracy is 87.4\n",
      "Iter 463: \taccuracy is 87.1\n",
      "Iter 464: \taccuracy is 87.2\n",
      "Iter 465: \taccuracy is 86.7\n",
      "Iter 466: \taccuracy is 86.2\n",
      "Iter 467: \taccuracy is 85.8\n",
      "Iter 468: \taccuracy is 84.5\n",
      "Iter 469: \taccuracy is 84.5\n",
      "Iter 470: \taccuracy is 84.5\n",
      "Iter 471: \taccuracy is 85.7\n",
      "Iter 472: \taccuracy is 86.2\n",
      "Iter 473: \taccuracy is 86.5\n",
      "Iter 474: \taccuracy is 87.0\n",
      "Iter 475: \taccuracy is 87.3\n",
      "Iter 476: \taccuracy is 87.2\n",
      "Iter 477: \taccuracy is 87.5\n",
      "Iter 478: \taccuracy is 87.5\n",
      "Iter 479: \taccuracy is 86.9\n",
      "Iter 480: \taccuracy is 87.6\n",
      "Iter 481: \taccuracy is 87.6\n",
      "Iter 482: \taccuracy is 87.7\n",
      "Iter 483: \taccuracy is 87.3\n",
      "Iter 484: \taccuracy is 87.3\n",
      "Iter 485: \taccuracy is 86.7\n",
      "Iter 486: \taccuracy is 86.3\n",
      "Iter 487: \taccuracy is 85.6\n",
      "Iter 488: \taccuracy is 84.8\n",
      "Iter 489: \taccuracy is 84.4\n",
      "Iter 490: \taccuracy is 84.6\n",
      "Iter 491: \taccuracy is 85.9\n",
      "Iter 492: \taccuracy is 86.4\n",
      "Iter 493: \taccuracy is 86.6\n",
      "Iter 494: \taccuracy is 87.1\n",
      "Iter 495: \taccuracy is 87.3\n",
      "Iter 496: \taccuracy is 87.4\n",
      "Iter 497: \taccuracy is 87.8\n",
      "Iter 498: \taccuracy is 87.3\n",
      "Iter 499: \taccuracy is 87.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Running synchronous parameter server training.\")\n",
    "lr=0.1\n",
    "failure_iter=501\n",
    "failure_server=\"server4\"\n",
    "\n",
    "# we need to get a new keys order because we are not assuming a ordering in keys\n",
    "current_weights = []\n",
    "keys_order = []\n",
    "acc_vals = []\n",
    "\n",
    "for j in range(num_servers):\n",
    "    keys_order.extend(weight_assignments[\"server\" + str(j)])\n",
    "    current_weights.extend(ray.get(servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]))) \n",
    "curr_weights_ckpt = current_weights.copy()\n",
    "\n",
    "time_per_iteration = []\n",
    "for i in range(iterations):\n",
    " \n",
    "    #start = time()\n",
    "    \n",
    "    if i == failure_iter:\n",
    "        w0 = weight_assignments[\"server0\"]\n",
    "        #Define parameters that will need to be moved\n",
    "        failure_params = weight_assignments[failure_server]\n",
    "        #Delete server from hash ring and reassign params\n",
    "        hasher.delete_node_and_reassign_to_others(failure_server)\n",
    "        weight_assignments = hasher.get_keys_per_node()\n",
    "        #Update servers and workers\n",
    "        num_servers -= 1\n",
    "        server_ind = server_ids.index(failure_server)\n",
    "        server_ids = server_ids[0 : server_ind] + server_ids[server_ind + 1 : ]\n",
    "        servers = servers[0 : server_ind] + servers[server_ind + 1 : ]\n",
    "        workers = workers[0 : server_ind] + workers[server_ind + 1 : ]\n",
    "        #Add each relevant parameter to its new server\n",
    "        server_dict = {server_ids[x]:servers[x] for x in range(len(server_ids))}\n",
    "        for ind, param in enumerate(failure_params):\n",
    "            server_dict[hasher.get_key_to_node_map()[param]].add_weight.remote(param, curr_weights_ckpt[server_ind][ind])\n",
    "        #Update these parameters for each worker to make them trainable\n",
    "        [workers[j][idx].update_trainable.remote(weight_assignments[\"server\" + str(j)]) for  idx  in range(num_workers) for j in range(num_servers)]\n",
    "        [workers[j][idx].update_weights.remote(keys_order, *current_weights) for  idx  in range(num_workers) for j in range(num_servers)]\n",
    "        \n",
    "    \n",
    "    # sync all weights on workers\n",
    "    if i % 10 == 0:\n",
    "        curr_weights_ckpt = current_weights.copy()\n",
    "        # get weights from server\n",
    "        #current_weights = [servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]) for j in range(num_servers)] \n",
    "\n",
    "        # update weights on all workers\n",
    "        [workers[j][idx].update_weights.remote(keys_order, *current_weights) for  idx  in range(num_workers) for j in range(num_servers)]\n",
    "    \n",
    "        \n",
    "    # use local cache of weights and get gradients from workers\n",
    "    gradients = [[workers[j][idx].compute_gradients.remote() for  idx  in range(num_workers)] for j in range(num_servers)]\n",
    "\n",
    "#     start = time()\n",
    "    if i == failure_iter:\n",
    "        keys_order = []\n",
    "        for j in range(num_servers):\n",
    "            keys_order.extend(weight_assignments[\"server\" + str(j)])\n",
    "\n",
    "    # Updates gradients to specfic parameter servers\n",
    "    current_weights_t = [servers[j].apply_gradients.remote(weight_assignments[\"server\" + str(j)], lr, *gradients[j]) for j in range(num_servers)]\n",
    "    current_weights = ray.get(current_weights_t)\n",
    "    \n",
    "    end = time()\n",
    "    time_per_iteration.append(end-start)\n",
    "\n",
    "    if i % 1 == 0:\n",
    "        # Evaluate the current model.\n",
    "        # current_weights = [servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]) for j in range(num_servers)] \n",
    "      \n",
    "        # we are once again using the server to key mapping to set the weight back\n",
    "        model.set_weights(keys_order, current_weights)\n",
    "        accuracy = evaluate(model, test_loader)\n",
    "        acc_vals.append(accuracy)\n",
    "        print(\"Iter {}: \\taccuracy is {:.1f}\".format(i, accuracy))\n",
    "    #rint(\"\\n\")\n",
    "\n",
    "#print(\"Final accuracy is {:.1f}.\".format(accuracy))\n",
    "# Clean up Ray resources and processes before the next example.\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1470847402163641"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(time_per_iteration[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02577144327194029"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(time_per_iteration[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.588714361190796,\n",
       " 0.13730883598327637,\n",
       " 0.13313031196594238,\n",
       " 0.1397244930267334,\n",
       " 0.13124918937683105,\n",
       " 0.1424236297607422,\n",
       " 0.13758516311645508,\n",
       " 0.13960599899291992,\n",
       " 0.13950729370117188,\n",
       " 0.1336812973022461,\n",
       " 0.19435453414916992,\n",
       " 0.13529610633850098,\n",
       " 0.14109039306640625,\n",
       " 0.13781523704528809,\n",
       " 0.1302502155303955,\n",
       " 0.13529276847839355,\n",
       " 0.13292670249938965,\n",
       " 0.13463830947875977,\n",
       " 0.14171147346496582,\n",
       " 0.14144682884216309,\n",
       " 0.19423699378967285,\n",
       " 0.12644362449645996,\n",
       " 0.1280200481414795,\n",
       " 0.12221574783325195,\n",
       " 0.12069988250732422,\n",
       " 0.12054061889648438,\n",
       " 0.12222146987915039,\n",
       " 0.18278765678405762,\n",
       " 0.19629263877868652,\n",
       " 0.12352895736694336,\n",
       " 0.19074463844299316,\n",
       " 0.12006068229675293,\n",
       " 0.12671160697937012,\n",
       " 0.12433290481567383,\n",
       " 0.1338052749633789,\n",
       " 0.1232297420501709,\n",
       " 0.12189030647277832,\n",
       " 0.12937450408935547,\n",
       " 0.1230320930480957,\n",
       " 0.12760472297668457,\n",
       " 0.1854264736175537,\n",
       " 0.11983871459960938,\n",
       " 0.12497305870056152,\n",
       " 0.12382173538208008,\n",
       " 0.1202993392944336,\n",
       " 0.1263740062713623,\n",
       " 0.12149739265441895,\n",
       " 0.12420082092285156,\n",
       " 0.12353873252868652,\n",
       " 0.12755489349365234,\n",
       " 0.1820816993713379,\n",
       " 0.21605634689331055,\n",
       " 0.14264750480651855,\n",
       " 0.14026618003845215,\n",
       " 0.1417255401611328,\n",
       " 0.14444398880004883,\n",
       " 0.1253950595855713,\n",
       " 0.1298661231994629,\n",
       " 0.12682151794433594,\n",
       " 0.12833857536315918,\n",
       " 0.16559243202209473,\n",
       " 0.12835311889648438,\n",
       " 0.1300966739654541,\n",
       " 0.13056087493896484,\n",
       " 0.12795424461364746,\n",
       " 0.12892556190490723,\n",
       " 0.14424395561218262,\n",
       " 0.12783074378967285,\n",
       " 0.12282490730285645,\n",
       " 0.12252283096313477,\n",
       " 0.1841893196105957,\n",
       " 0.14269757270812988,\n",
       " 0.13878941535949707,\n",
       " 0.12727856636047363,\n",
       " 0.12466883659362793,\n",
       " 0.12364339828491211,\n",
       " 0.12157130241394043,\n",
       " 0.12425351142883301,\n",
       " 0.12210750579833984,\n",
       " 0.12828779220581055,\n",
       " 0.17039275169372559,\n",
       " 0.12045884132385254,\n",
       " 0.12500882148742676,\n",
       " 0.12307024002075195,\n",
       " 0.1196908950805664,\n",
       " 0.1222832202911377,\n",
       " 0.14913153648376465,\n",
       " 0.13460469245910645,\n",
       " 0.14117145538330078,\n",
       " 0.13710260391235352,\n",
       " 0.17919468879699707,\n",
       " 0.13693761825561523,\n",
       " 0.13512110710144043,\n",
       " 0.13065028190612793,\n",
       " 0.13193869590759277,\n",
       " 0.13787484169006348,\n",
       " 0.13448572158813477,\n",
       " 0.1340956687927246,\n",
       " 0.13578414916992188,\n",
       " 0.13019084930419922,\n",
       " 0.1794435977935791,\n",
       " 0.12315773963928223,\n",
       " 0.1231389045715332,\n",
       " 0.1212165355682373,\n",
       " 0.1252918243408203,\n",
       " 0.12147641181945801,\n",
       " 0.12157201766967773,\n",
       " 0.12213253974914551,\n",
       " 0.1202399730682373,\n",
       " 0.13007736206054688,\n",
       " 0.18225550651550293,\n",
       " 0.12760090827941895,\n",
       " 0.19522309303283691,\n",
       " 0.11833643913269043,\n",
       " 0.12111568450927734,\n",
       " 0.11885571479797363,\n",
       " 0.12067461013793945,\n",
       " 0.12244105339050293,\n",
       " 0.12076330184936523,\n",
       " 0.12005472183227539,\n",
       " 0.18515920639038086,\n",
       " 0.12366986274719238,\n",
       " 0.15457391738891602,\n",
       " 0.16038894653320312,\n",
       " 0.15955090522766113,\n",
       " 0.1561133861541748,\n",
       " 0.1503736972808838,\n",
       " 0.16120076179504395,\n",
       " 0.15075325965881348,\n",
       " 0.15771770477294922,\n",
       " 0.21598196029663086,\n",
       " 0.15779471397399902,\n",
       " 0.15866422653198242,\n",
       " 0.16682839393615723,\n",
       " 0.1515355110168457,\n",
       " 0.15619421005249023,\n",
       " 0.15932250022888184,\n",
       " 0.16324138641357422,\n",
       " 0.16686248779296875,\n",
       " 0.16022872924804688,\n",
       " 0.21947574615478516,\n",
       " 0.16245388984680176,\n",
       " 0.1614837646484375,\n",
       " 0.15810441970825195,\n",
       " 0.15517234802246094,\n",
       " 0.16928768157958984,\n",
       " 0.16161417961120605,\n",
       " 0.14965271949768066,\n",
       " 0.1609635353088379,\n",
       " 0.16119647026062012,\n",
       " 0.21338725090026855,\n",
       " 0.1564497947692871,\n",
       " 0.15535902976989746,\n",
       " 0.1545271873474121,\n",
       " 0.15725326538085938,\n",
       " 0.15796542167663574,\n",
       " 0.16361761093139648,\n",
       " 0.15983223915100098,\n",
       " 0.15895509719848633,\n",
       " 0.15792346000671387,\n",
       " 0.18034625053405762,\n",
       " 0.13035082817077637,\n",
       " 0.13068294525146484,\n",
       " 0.1308279037475586,\n",
       " 0.12936925888061523,\n",
       " 0.13068056106567383,\n",
       " 0.13065576553344727,\n",
       " 0.1311483383178711,\n",
       " 0.13117170333862305,\n",
       " 0.1307237148284912,\n",
       " 0.1846904754638672,\n",
       " 0.1312258243560791,\n",
       " 0.12980985641479492,\n",
       " 0.1765282154083252,\n",
       " 0.14496064186096191,\n",
       " 0.14835309982299805,\n",
       " 0.14578461647033691,\n",
       " 0.14438462257385254,\n",
       " 0.13753080368041992,\n",
       " 0.1454150676727295,\n",
       " 0.20393085479736328,\n",
       " 0.15209293365478516,\n",
       " 0.14257335662841797,\n",
       " 0.1346902847290039,\n",
       " 0.14958977699279785,\n",
       " 0.16284584999084473,\n",
       " 0.1621861457824707,\n",
       " 0.15857195854187012,\n",
       " 0.15723061561584473,\n",
       " 0.16019105911254883,\n",
       " 0.21585798263549805,\n",
       " 0.15961360931396484,\n",
       " 0.16126513481140137,\n",
       " 0.15205740928649902,\n",
       " 0.16143512725830078,\n",
       " 0.12444663047790527,\n",
       " 0.13853740692138672,\n",
       " 0.14239811897277832,\n",
       " 0.14174103736877441,\n",
       " 0.14020013809204102,\n",
       " 0.2006826400756836,\n",
       " 0.14292335510253906,\n",
       " 0.14998984336853027,\n",
       " 0.13971543312072754,\n",
       " 0.14864110946655273,\n",
       " 0.13617968559265137,\n",
       " 0.149977445602417,\n",
       " 0.13677597045898438,\n",
       " 0.14380526542663574,\n",
       " 0.14829087257385254,\n",
       " 0.20360279083251953,\n",
       " 0.14495372772216797,\n",
       " 0.1479339599609375,\n",
       " 0.14316558837890625,\n",
       " 0.1484973430633545,\n",
       " 0.14455366134643555,\n",
       " 0.14990901947021484,\n",
       " 0.1510298252105713,\n",
       " 0.148040771484375,\n",
       " 0.14684772491455078,\n",
       " 0.20139312744140625,\n",
       " 0.14557099342346191,\n",
       " 0.1496264934539795,\n",
       " 0.14600467681884766,\n",
       " 0.14110779762268066,\n",
       " 0.15066862106323242,\n",
       " 0.1429431438446045,\n",
       " 0.14523816108703613,\n",
       " 0.14439773559570312,\n",
       " 0.14445877075195312,\n",
       " 0.20244240760803223,\n",
       " 0.14615845680236816,\n",
       " 0.14536833763122559,\n",
       " 0.14378023147583008,\n",
       " 0.18847084045410156,\n",
       " 0.1435854434967041,\n",
       " 0.13408923149108887,\n",
       " 0.14010214805603027,\n",
       " 0.14461421966552734,\n",
       " 0.13532543182373047,\n",
       " 0.20049786567687988,\n",
       " 0.1433396339416504,\n",
       " 0.14646434783935547,\n",
       " 0.1421952247619629,\n",
       " 0.14671850204467773,\n",
       " 0.1420431137084961,\n",
       " 0.14083147048950195,\n",
       " 0.14219093322753906,\n",
       " 0.1452319622039795,\n",
       " 0.14455389976501465,\n",
       " 0.20417189598083496,\n",
       " 0.14422202110290527,\n",
       " 0.14219188690185547,\n",
       " 0.1451876163482666,\n",
       " 0.14583492279052734,\n",
       " 0.1478879451751709,\n",
       " 0.1319718360900879,\n",
       " 0.1298828125,\n",
       " 0.12665104866027832,\n",
       " 0.13114285469055176,\n",
       " 0.2024064064025879,\n",
       " 0.1334080696105957,\n",
       " 0.12706565856933594,\n",
       " 0.13207554817199707,\n",
       " 0.13097286224365234,\n",
       " 0.12812232971191406,\n",
       " 0.12932801246643066,\n",
       " 0.13187575340270996,\n",
       " 0.13041377067565918,\n",
       " 0.12995624542236328,\n",
       " 0.19485187530517578,\n",
       " 0.12957024574279785,\n",
       " 0.12966394424438477,\n",
       " 0.1280653476715088,\n",
       " 0.13150882720947266,\n",
       " 0.12711787223815918,\n",
       " 0.1256728172302246,\n",
       " 0.12952423095703125,\n",
       " 0.12906503677368164,\n",
       " 0.13178730010986328,\n",
       " 0.19459271430969238,\n",
       " 0.12001633644104004,\n",
       " 0.12906980514526367,\n",
       " 0.13058781623840332,\n",
       " 0.1280384063720703,\n",
       " 0.1311359405517578,\n",
       " 0.13184857368469238,\n",
       " 0.1294858455657959,\n",
       " 0.1324765682220459,\n",
       " 0.1293637752532959,\n",
       " 0.19247770309448242,\n",
       " 0.13024067878723145,\n",
       " 0.13043737411499023,\n",
       " 0.13093233108520508,\n",
       " 0.1313612461090088,\n",
       " 0.1897740364074707,\n",
       " 0.12133359909057617,\n",
       " 0.13621878623962402,\n",
       " 0.13880062103271484,\n",
       " 0.13218998908996582,\n",
       " 0.190901517868042,\n",
       " 0.13528800010681152,\n",
       " 0.1383047103881836,\n",
       " 0.13620829582214355,\n",
       " 0.13521313667297363,\n",
       " 0.13941001892089844,\n",
       " 0.1436910629272461,\n",
       " 0.14015936851501465,\n",
       " 0.1414051055908203,\n",
       " 0.14001989364624023,\n",
       " 0.19403719902038574,\n",
       " 0.14162921905517578,\n",
       " 0.12883782386779785,\n",
       " 0.12906503677368164,\n",
       " 0.12750887870788574,\n",
       " 0.11835598945617676,\n",
       " 0.12250375747680664,\n",
       " 0.12195825576782227,\n",
       " 0.12538504600524902,\n",
       " 0.12372612953186035,\n",
       " 0.19184327125549316,\n",
       " 0.1210176944732666,\n",
       " 0.12280535697937012,\n",
       " 0.12158679962158203,\n",
       " 0.12247943878173828,\n",
       " 0.11878514289855957,\n",
       " 0.11729693412780762,\n",
       " 0.12392091751098633,\n",
       " 0.11906170845031738,\n",
       " 0.1210336685180664,\n",
       " 0.18976616859436035,\n",
       " 0.12137007713317871,\n",
       " 0.13180255889892578,\n",
       " 0.12037134170532227,\n",
       " 0.11889863014221191,\n",
       " 0.11515307426452637,\n",
       " 0.12493085861206055,\n",
       " 0.12184381484985352,\n",
       " 0.11800479888916016,\n",
       " 0.11747336387634277,\n",
       " 0.19149374961853027,\n",
       " 0.12011051177978516,\n",
       " 0.12995171546936035,\n",
       " 0.1194465160369873,\n",
       " 0.13550424575805664,\n",
       " 0.12662386894226074,\n",
       " 0.13158750534057617,\n",
       " 0.1326158046722412,\n",
       " 0.1311337947845459,\n",
       " 0.11755800247192383,\n",
       " 0.19197916984558105,\n",
       " 0.12063288688659668,\n",
       " 0.11967897415161133,\n",
       " 0.11870408058166504,\n",
       " 0.13463473320007324,\n",
       " 0.13258099555969238,\n",
       " 0.19744157791137695,\n",
       " 0.13646674156188965,\n",
       " 0.1386868953704834,\n",
       " 0.13277912139892578,\n",
       " 0.19470572471618652,\n",
       " 0.13494038581848145,\n",
       " 0.1292872428894043,\n",
       " 0.12587285041809082,\n",
       " 0.13085055351257324,\n",
       " 0.1437394618988037,\n",
       " 0.1398007869720459,\n",
       " 0.13625359535217285,\n",
       " 0.14273619651794434,\n",
       " 0.1420753002166748,\n",
       " 0.19152164459228516,\n",
       " 0.1438918113708496,\n",
       " 0.13957953453063965,\n",
       " 0.14067554473876953,\n",
       " 0.1401810646057129,\n",
       " 0.18092036247253418,\n",
       " 0.20543980598449707,\n",
       " 0.19949889183044434,\n",
       " 0.13892483711242676,\n",
       " 0.13962125778198242,\n",
       " 0.19038081169128418,\n",
       " 0.1417844295501709,\n",
       " 0.1330561637878418,\n",
       " 0.13378667831420898,\n",
       " 0.13425302505493164,\n",
       " 0.1318073272705078,\n",
       " 0.1319413185119629,\n",
       " 0.13266730308532715,\n",
       " 0.13746023178100586,\n",
       " 0.13155341148376465,\n",
       " 0.18899917602539062,\n",
       " 0.1380906105041504,\n",
       " 0.13413524627685547,\n",
       " 0.13341283798217773,\n",
       " 0.13841962814331055,\n",
       " 0.13823866844177246,\n",
       " 0.13444113731384277,\n",
       " 0.13520288467407227,\n",
       " 0.13525795936584473,\n",
       " 0.14022135734558105,\n",
       " 0.19223904609680176,\n",
       " 0.1349339485168457,\n",
       " 0.13482975959777832,\n",
       " 0.14018607139587402,\n",
       " 0.1377394199371338,\n",
       " 0.13406896591186523,\n",
       " 0.13478803634643555,\n",
       " 0.1364588737487793,\n",
       " 0.13677501678466797,\n",
       " 0.1356792449951172,\n",
       " 0.18207573890686035,\n",
       " 0.1335923671722412,\n",
       " 0.13908863067626953,\n",
       " 0.13953089714050293,\n",
       " 0.1395125389099121,\n",
       " 0.13300299644470215,\n",
       " 0.13886523246765137,\n",
       " 0.18923306465148926,\n",
       " 0.1383810043334961,\n",
       " 0.13254857063293457,\n",
       " 0.17981958389282227,\n",
       " 0.13347530364990234,\n",
       " 0.13469600677490234,\n",
       " 0.13707637786865234,\n",
       " 0.13858485221862793,\n",
       " 0.13971543312072754,\n",
       " 0.13330340385437012,\n",
       " 0.14046359062194824,\n",
       " 0.13966703414916992,\n",
       " 0.13942646980285645,\n",
       " 0.18185186386108398,\n",
       " 0.13506174087524414,\n",
       " 0.13754820823669434,\n",
       " 0.13976621627807617,\n",
       " 0.1388871669769287,\n",
       " 0.14120221138000488,\n",
       " 0.13148736953735352,\n",
       " 0.13985133171081543,\n",
       " 0.13904356956481934,\n",
       " 0.1340954303741455,\n",
       " 0.1867210865020752,\n",
       " 0.1286945343017578,\n",
       " 0.13979601860046387,\n",
       " 0.13793349266052246,\n",
       " 0.1215674877166748,\n",
       " 0.12842774391174316,\n",
       " 0.12745261192321777,\n",
       " 0.13446450233459473,\n",
       " 0.12633776664733887,\n",
       " 0.1289050579071045,\n",
       " 0.18094515800476074,\n",
       " 0.1254866123199463,\n",
       " 0.13383960723876953,\n",
       " 0.12442159652709961,\n",
       " 0.12490558624267578,\n",
       " 0.12967419624328613,\n",
       " 0.13478803634643555,\n",
       " 0.12454032897949219,\n",
       " 0.13705658912658691,\n",
       " 0.12261581420898438,\n",
       " 0.1815023422241211,\n",
       " 0.1272437572479248,\n",
       " 0.11912941932678223,\n",
       " 0.12349438667297363,\n",
       " 0.1278378963470459,\n",
       " 0.13283276557922363,\n",
       " 0.12104296684265137,\n",
       " 0.13003969192504883,\n",
       " 0.12921571731567383,\n",
       " 0.6058883666992188,\n",
       " 0.17995500564575195,\n",
       " 0.13103461265563965,\n",
       " 0.1250143051147461,\n",
       " 0.13048195838928223,\n",
       " 0.13019704818725586,\n",
       " 0.13636398315429688,\n",
       " 0.1364281177520752,\n",
       " 0.13036036491394043,\n",
       " 0.17999649047851562,\n",
       " 0.13253211975097656,\n",
       " 0.17973923683166504,\n",
       " 0.14344573020935059,\n",
       " 0.14580035209655762,\n",
       " 0.14601588249206543,\n",
       " 0.14789438247680664,\n",
       " 0.14328360557556152,\n",
       " 0.14340853691101074,\n",
       " 0.1476285457611084,\n",
       " 0.1448345184326172,\n",
       " 0.1457052230834961,\n",
       " 0.17920398712158203,\n",
       " 0.1423816680908203,\n",
       " 0.14908885955810547,\n",
       " 0.1418910026550293,\n",
       " 0.14190459251403809,\n",
       " 0.14038610458374023,\n",
       " 0.14440083503723145,\n",
       " 0.1435098648071289,\n",
       " 0.14851784706115723,\n",
       " 0.14412832260131836]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_per_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['server0', 'server1', 'server2', 'server3']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'server4' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a58e50e12838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mserver_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"server4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: 'server4' is not in list"
     ]
    }
   ],
   "source": [
    "server_ids.index(\"server4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher.get_key_to_node_map()['fc_bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel=LinearNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc_bias': tensor([0.7625]),\n",
       " 'fc_weights.0': tensor([0.5567]),\n",
       " 'fc_weights.1': tensor([0.0267]),\n",
       " 'fc_weights.2': tensor([0.4424]),\n",
       " 'fc_weights.3': tensor([0.3825]),\n",
       " 'fc_weights.4': tensor([0.5311]),\n",
       " 'fc_weights.5': tensor([0.8576]),\n",
       " 'fc_weights.6': tensor([0.5620]),\n",
       " 'fc_weights.7': tensor([0.6615]),\n",
       " 'fc_weights.8': tensor([0.6206]),\n",
       " 'fc_weights.9': tensor([0.9785]),\n",
       " 'fc_weights.10': tensor([0.6421]),\n",
       " 'fc_weights.11': tensor([0.5424]),\n",
       " 'fc_weights.12': tensor([0.9489]),\n",
       " 'fc_weights.13': tensor([0.4326]),\n",
       " 'fc_weights.14': tensor([0.2355]),\n",
       " 'fc_weights.15': tensor([0.8898]),\n",
       " 'fc_weights.16': tensor([0.9719]),\n",
       " 'fc_weights.17': tensor([0.9767]),\n",
       " 'fc_weights.18': tensor([0.2998]),\n",
       " 'fc_weights.19': tensor([0.6725]),\n",
       " 'fc_weights.20': tensor([0.8768]),\n",
       " 'fc_weights.21': tensor([0.7303]),\n",
       " 'fc_weights.22': tensor([0.6391]),\n",
       " 'fc_weights.23': tensor([0.6964]),\n",
       " 'fc_weights.24': tensor([0.4810]),\n",
       " 'fc_weights.25': tensor([0.3722]),\n",
       " 'fc_weights.26': tensor([0.8999]),\n",
       " 'fc_weights.27': tensor([0.3884]),\n",
       " 'fc_weights.28': tensor([0.1685]),\n",
       " 'fc_weights.29': tensor([0.3936]),\n",
       " 'fc_weights.30': tensor([0.4552]),\n",
       " 'fc_weights.31': tensor([0.1880]),\n",
       " 'fc_weights.32': tensor([0.2471]),\n",
       " 'fc_weights.33': tensor([0.6051]),\n",
       " 'fc_weights.34': tensor([0.2964]),\n",
       " 'fc_weights.35': tensor([0.5948]),\n",
       " 'fc_weights.36': tensor([0.8974]),\n",
       " 'fc_weights.37': tensor([0.1101]),\n",
       " 'fc_weights.38': tensor([0.1972]),\n",
       " 'fc_weights.39': tensor([0.8189]),\n",
       " 'fc_weights.40': tensor([0.0715]),\n",
       " 'fc_weights.41': tensor([0.8899]),\n",
       " 'fc_weights.42': tensor([0.3332]),\n",
       " 'fc_weights.43': tensor([0.5072]),\n",
       " 'fc_weights.44': tensor([0.2837]),\n",
       " 'fc_weights.45': tensor([0.8767]),\n",
       " 'fc_weights.46': tensor([0.6117]),\n",
       " 'fc_weights.47': tensor([0.5177]),\n",
       " 'fc_weights.48': tensor([0.6978]),\n",
       " 'fc_weights.49': tensor([0.3316]),\n",
       " 'fc_weights.50': tensor([0.8643]),\n",
       " 'fc_weights.51': tensor([0.7003]),\n",
       " 'fc_weights.52': tensor([0.1233]),\n",
       " 'fc_weights.53': tensor([0.1730]),\n",
       " 'fc_weights.54': tensor([0.2694]),\n",
       " 'fc_weights.55': tensor([0.9803]),\n",
       " 'fc_weights.56': tensor([0.1872]),\n",
       " 'fc_weights.57': tensor([0.6635]),\n",
       " 'fc_weights.58': tensor([0.2268]),\n",
       " 'fc_weights.59': tensor([0.5473]),\n",
       " 'fc_weights.60': tensor([0.3946]),\n",
       " 'fc_weights.61': tensor([0.4909]),\n",
       " 'fc_weights.62': tensor([0.9389]),\n",
       " 'fc_weights.63': tensor([0.6069]),\n",
       " 'fc_weights.64': tensor([0.1353]),\n",
       " 'fc_weights.65': tensor([0.3428]),\n",
       " 'fc_weights.66': tensor([0.9811]),\n",
       " 'fc_weights.67': tensor([0.8428]),\n",
       " 'fc_weights.68': tensor([0.7773]),\n",
       " 'fc_weights.69': tensor([0.4340]),\n",
       " 'fc_weights.70': tensor([0.4733]),\n",
       " 'fc_weights.71': tensor([0.0277]),\n",
       " 'fc_weights.72': tensor([0.8987]),\n",
       " 'fc_weights.73': tensor([0.5039]),\n",
       " 'fc_weights.74': tensor([0.6467]),\n",
       " 'fc_weights.75': tensor([0.3526]),\n",
       " 'fc_weights.76': tensor([0.3281]),\n",
       " 'fc_weights.77': tensor([0.5660]),\n",
       " 'fc_weights.78': tensor([0.0026]),\n",
       " 'fc_weights.79': tensor([0.8693]),\n",
       " 'fc_weights.80': tensor([0.0604]),\n",
       " 'fc_weights.81': tensor([0.7813]),\n",
       " 'fc_weights.82': tensor([0.7656]),\n",
       " 'fc_weights.83': tensor([0.2427]),\n",
       " 'fc_weights.84': tensor([0.4718]),\n",
       " 'fc_weights.85': tensor([0.2625]),\n",
       " 'fc_weights.86': tensor([0.3076]),\n",
       " 'fc_weights.87': tensor([0.0130]),\n",
       " 'fc_weights.88': tensor([0.2374]),\n",
       " 'fc_weights.89': tensor([0.6674]),\n",
       " 'fc_weights.90': tensor([0.3649]),\n",
       " 'fc_weights.91': tensor([0.1961]),\n",
       " 'fc_weights.92': tensor([0.6558]),\n",
       " 'fc_weights.93': tensor([0.2807]),\n",
       " 'fc_weights.94': tensor([0.0606]),\n",
       " 'fc_weights.95': tensor([0.3978]),\n",
       " 'fc_weights.96': tensor([0.4284]),\n",
       " 'fc_weights.97': tensor([0.4936]),\n",
       " 'fc_weights.98': tensor([0.8378]),\n",
       " 'fc_weights.99': tensor([0.6511]),\n",
       " 'fc_weights.100': tensor([0.5372]),\n",
       " 'fc_weights.101': tensor([0.8678]),\n",
       " 'fc_weights.102': tensor([0.1119]),\n",
       " 'fc_weights.103': tensor([0.2242]),\n",
       " 'fc_weights.104': tensor([0.7202]),\n",
       " 'fc_weights.105': tensor([0.6494]),\n",
       " 'fc_weights.106': tensor([0.9867]),\n",
       " 'fc_weights.107': tensor([0.2684]),\n",
       " 'fc_weights.108': tensor([0.3641]),\n",
       " 'fc_weights.109': tensor([0.7815]),\n",
       " 'fc_weights.110': tensor([0.9672]),\n",
       " 'fc_weights.111': tensor([0.5665]),\n",
       " 'fc_weights.112': tensor([0.0204]),\n",
       " 'fc_weights.113': tensor([0.5874]),\n",
       " 'fc_weights.114': tensor([0.9232]),\n",
       " 'fc_weights.115': tensor([0.7837]),\n",
       " 'fc_weights.116': tensor([0.0687]),\n",
       " 'fc_weights.117': tensor([0.1729]),\n",
       " 'fc_weights.118': tensor([0.2021]),\n",
       " 'fc_weights.119': tensor([0.1733]),\n",
       " 'fc_weights.120': tensor([0.1134]),\n",
       " 'fc_weights.121': tensor([0.3406]),\n",
       " 'fc_weights.122': tensor([0.6298]),\n",
       " 'fc_weights.123': tensor([0.8149]),\n",
       " 'fc_weights.124': tensor([0.6542]),\n",
       " 'fc_weights.125': tensor([0.6125]),\n",
       " 'fc_weights.126': tensor([0.7592]),\n",
       " 'fc_weights.127': tensor([0.1453]),\n",
       " 'fc_weights.128': tensor([0.2906]),\n",
       " 'fc_weights.129': tensor([0.8972]),\n",
       " 'fc_weights.130': tensor([0.1801]),\n",
       " 'fc_weights.131': tensor([0.4057]),\n",
       " 'fc_weights.132': tensor([0.4967]),\n",
       " 'fc_weights.133': tensor([0.1841]),\n",
       " 'fc_weights.134': tensor([0.1208]),\n",
       " 'fc_weights.135': tensor([0.5800]),\n",
       " 'fc_weights.136': tensor([0.2158]),\n",
       " 'fc_weights.137': tensor([0.9913]),\n",
       " 'fc_weights.138': tensor([0.8290]),\n",
       " 'fc_weights.139': tensor([0.7372]),\n",
       " 'fc_weights.140': tensor([0.2003]),\n",
       " 'fc_weights.141': tensor([0.8894]),\n",
       " 'fc_weights.142': tensor([0.5395]),\n",
       " 'fc_weights.143': tensor([0.0833]),\n",
       " 'fc_weights.144': tensor([0.1923]),\n",
       " 'fc_weights.145': tensor([0.6103]),\n",
       " 'fc_weights.146': tensor([0.9093]),\n",
       " 'fc_weights.147': tensor([0.4833]),\n",
       " 'fc_weights.148': tensor([0.3949]),\n",
       " 'fc_weights.149': tensor([0.4463]),\n",
       " 'fc_weights.150': tensor([0.0400]),\n",
       " 'fc_weights.151': tensor([0.7101]),\n",
       " 'fc_weights.152': tensor([0.7806]),\n",
       " 'fc_weights.153': tensor([0.3071]),\n",
       " 'fc_weights.154': tensor([0.4453]),\n",
       " 'fc_weights.155': tensor([0.6243]),\n",
       " 'fc_weights.156': tensor([0.6997]),\n",
       " 'fc_weights.157': tensor([0.6692]),\n",
       " 'fc_weights.158': tensor([0.4867]),\n",
       " 'fc_weights.159': tensor([0.9764]),\n",
       " 'fc_weights.160': tensor([0.1412]),\n",
       " 'fc_weights.161': tensor([0.6923]),\n",
       " 'fc_weights.162': tensor([0.7441]),\n",
       " 'fc_weights.163': tensor([0.2915]),\n",
       " 'fc_weights.164': tensor([0.5224]),\n",
       " 'fc_weights.165': tensor([0.5726]),\n",
       " 'fc_weights.166': tensor([0.1113]),\n",
       " 'fc_weights.167': tensor([0.4965]),\n",
       " 'fc_weights.168': tensor([0.7196]),\n",
       " 'fc_weights.169': tensor([0.2720]),\n",
       " 'fc_weights.170': tensor([0.4908]),\n",
       " 'fc_weights.171': tensor([0.6682]),\n",
       " 'fc_weights.172': tensor([0.1519]),\n",
       " 'fc_weights.173': tensor([0.8194]),\n",
       " 'fc_weights.174': tensor([0.1834]),\n",
       " 'fc_weights.175': tensor([0.6993]),\n",
       " 'fc_weights.176': tensor([0.1394]),\n",
       " 'fc_weights.177': tensor([0.5423]),\n",
       " 'fc_weights.178': tensor([0.7026]),\n",
       " 'fc_weights.179': tensor([0.8395]),\n",
       " 'fc_weights.180': tensor([0.7426]),\n",
       " 'fc_weights.181': tensor([0.4872]),\n",
       " 'fc_weights.182': tensor([0.3051]),\n",
       " 'fc_weights.183': tensor([0.5298]),\n",
       " 'fc_weights.184': tensor([0.1542]),\n",
       " 'fc_weights.185': tensor([0.1390]),\n",
       " 'fc_weights.186': tensor([0.5410]),\n",
       " 'fc_weights.187': tensor([0.4380]),\n",
       " 'fc_weights.188': tensor([0.1274]),\n",
       " 'fc_weights.189': tensor([0.6118]),\n",
       " 'fc_weights.190': tensor([0.9207]),\n",
       " 'fc_weights.191': tensor([0.3969]),\n",
       " 'fc_weights.192': tensor([0.5280]),\n",
       " 'fc_weights.193': tensor([0.9284]),\n",
       " 'fc_weights.194': tensor([0.8555]),\n",
       " 'fc_weights.195': tensor([0.7195]),\n",
       " 'fc_weights.196': tensor([0.7330]),\n",
       " 'fc_weights.197': tensor([0.8522]),\n",
       " 'fc_weights.198': tensor([0.4537]),\n",
       " 'fc_weights.199': tensor([0.9926]),\n",
       " 'fc_weights.200': tensor([0.2211]),\n",
       " 'fc_weights.201': tensor([0.6367]),\n",
       " 'fc_weights.202': tensor([0.1351]),\n",
       " 'fc_weights.203': tensor([0.6210]),\n",
       " 'fc_weights.204': tensor([0.7355]),\n",
       " 'fc_weights.205': tensor([0.8311]),\n",
       " 'fc_weights.206': tensor([0.7399]),\n",
       " 'fc_weights.207': tensor([0.2414]),\n",
       " 'fc_weights.208': tensor([0.5199]),\n",
       " 'fc_weights.209': tensor([0.0205]),\n",
       " 'fc_weights.210': tensor([0.2514]),\n",
       " 'fc_weights.211': tensor([0.7458]),\n",
       " 'fc_weights.212': tensor([0.1263]),\n",
       " 'fc_weights.213': tensor([0.0783]),\n",
       " 'fc_weights.214': tensor([0.6535]),\n",
       " 'fc_weights.215': tensor([0.2574]),\n",
       " 'fc_weights.216': tensor([0.8095]),\n",
       " 'fc_weights.217': tensor([0.2486]),\n",
       " 'fc_weights.218': tensor([0.3367]),\n",
       " 'fc_weights.219': tensor([0.0844]),\n",
       " 'fc_weights.220': tensor([0.7873]),\n",
       " 'fc_weights.221': tensor([0.9258]),\n",
       " 'fc_weights.222': tensor([0.9372]),\n",
       " 'fc_weights.223': tensor([0.1561]),\n",
       " 'fc_weights.224': tensor([0.7876]),\n",
       " 'fc_weights.225': tensor([0.8100]),\n",
       " 'fc_weights.226': tensor([0.3686]),\n",
       " 'fc_weights.227': tensor([0.9206]),\n",
       " 'fc_weights.228': tensor([0.5207]),\n",
       " 'fc_weights.229': tensor([0.8997]),\n",
       " 'fc_weights.230': tensor([0.9022]),\n",
       " 'fc_weights.231': tensor([0.1905]),\n",
       " 'fc_weights.232': tensor([0.8638]),\n",
       " 'fc_weights.233': tensor([0.4816]),\n",
       " 'fc_weights.234': tensor([0.4610]),\n",
       " 'fc_weights.235': tensor([0.9430]),\n",
       " 'fc_weights.236': tensor([0.3083]),\n",
       " 'fc_weights.237': tensor([0.5289]),\n",
       " 'fc_weights.238': tensor([0.6719]),\n",
       " 'fc_weights.239': tensor([0.0213]),\n",
       " 'fc_weights.240': tensor([0.3253]),\n",
       " 'fc_weights.241': tensor([0.8446]),\n",
       " 'fc_weights.242': tensor([0.1870]),\n",
       " 'fc_weights.243': tensor([0.1618]),\n",
       " 'fc_weights.244': tensor([0.5481]),\n",
       " 'fc_weights.245': tensor([0.4226]),\n",
       " 'fc_weights.246': tensor([0.2411]),\n",
       " 'fc_weights.247': tensor([0.0518]),\n",
       " 'fc_weights.248': tensor([0.5404]),\n",
       " 'fc_weights.249': tensor([0.9196]),\n",
       " 'fc_weights.250': tensor([0.8260]),\n",
       " 'fc_weights.251': tensor([0.6679]),\n",
       " 'fc_weights.252': tensor([0.0833]),\n",
       " 'fc_weights.253': tensor([0.1937]),\n",
       " 'fc_weights.254': tensor([0.7598]),\n",
       " 'fc_weights.255': tensor([0.8274]),\n",
       " 'fc_weights.256': tensor([0.2132]),\n",
       " 'fc_weights.257': tensor([0.3669]),\n",
       " 'fc_weights.258': tensor([0.8428]),\n",
       " 'fc_weights.259': tensor([0.8586]),\n",
       " 'fc_weights.260': tensor([0.3030]),\n",
       " 'fc_weights.261': tensor([0.5176]),\n",
       " 'fc_weights.262': tensor([0.0623]),\n",
       " 'fc_weights.263': tensor([0.1966]),\n",
       " 'fc_weights.264': tensor([0.9233]),\n",
       " 'fc_weights.265': tensor([0.4177]),\n",
       " 'fc_weights.266': tensor([0.4501]),\n",
       " 'fc_weights.267': tensor([0.6788]),\n",
       " 'fc_weights.268': tensor([0.8516]),\n",
       " 'fc_weights.269': tensor([0.9824]),\n",
       " 'fc_weights.270': tensor([0.7776]),\n",
       " 'fc_weights.271': tensor([0.3584]),\n",
       " 'fc_weights.272': tensor([0.7839]),\n",
       " 'fc_weights.273': tensor([0.4955]),\n",
       " 'fc_weights.274': tensor([0.7328]),\n",
       " 'fc_weights.275': tensor([0.9950]),\n",
       " 'fc_weights.276': tensor([0.0858]),\n",
       " 'fc_weights.277': tensor([0.5690]),\n",
       " 'fc_weights.278': tensor([0.6279]),\n",
       " 'fc_weights.279': tensor([0.1951]),\n",
       " 'fc_weights.280': tensor([0.1237]),\n",
       " 'fc_weights.281': tensor([0.9587]),\n",
       " 'fc_weights.282': tensor([0.4556]),\n",
       " 'fc_weights.283': tensor([0.6684]),\n",
       " 'fc_weights.284': tensor([0.6251]),\n",
       " 'fc_weights.285': tensor([0.6041]),\n",
       " 'fc_weights.286': tensor([0.6741]),\n",
       " 'fc_weights.287': tensor([0.1346]),\n",
       " 'fc_weights.288': tensor([0.6569]),\n",
       " 'fc_weights.289': tensor([0.5533]),\n",
       " 'fc_weights.290': tensor([0.3399]),\n",
       " 'fc_weights.291': tensor([0.7719]),\n",
       " 'fc_weights.292': tensor([0.8539]),\n",
       " 'fc_weights.293': tensor([0.0230]),\n",
       " 'fc_weights.294': tensor([0.0752]),\n",
       " 'fc_weights.295': tensor([0.0036]),\n",
       " 'fc_weights.296': tensor([0.2037]),\n",
       " 'fc_weights.297': tensor([0.8923]),\n",
       " 'fc_weights.298': tensor([0.1159]),\n",
       " 'fc_weights.299': tensor([0.7056]),\n",
       " 'fc_weights.300': tensor([0.6748]),\n",
       " 'fc_weights.301': tensor([0.3363]),\n",
       " 'fc_weights.302': tensor([0.9991]),\n",
       " 'fc_weights.303': tensor([0.7621]),\n",
       " 'fc_weights.304': tensor([0.9812]),\n",
       " 'fc_weights.305': tensor([0.7233]),\n",
       " 'fc_weights.306': tensor([0.4696]),\n",
       " 'fc_weights.307': tensor([0.3138]),\n",
       " 'fc_weights.308': tensor([0.3329]),\n",
       " 'fc_weights.309': tensor([0.4649]),\n",
       " 'fc_weights.310': tensor([0.6256]),\n",
       " 'fc_weights.311': tensor([0.2406]),\n",
       " 'fc_weights.312': tensor([0.6590]),\n",
       " 'fc_weights.313': tensor([0.5891]),\n",
       " 'fc_weights.314': tensor([0.5727]),\n",
       " 'fc_weights.315': tensor([0.9017]),\n",
       " 'fc_weights.316': tensor([0.7307]),\n",
       " 'fc_weights.317': tensor([0.8789]),\n",
       " 'fc_weights.318': tensor([0.9673]),\n",
       " 'fc_weights.319': tensor([0.4310]),\n",
       " 'fc_weights.320': tensor([0.4886]),\n",
       " 'fc_weights.321': tensor([0.5743]),\n",
       " 'fc_weights.322': tensor([0.7229]),\n",
       " 'fc_weights.323': tensor([0.1103]),\n",
       " 'fc_weights.324': tensor([0.2025]),\n",
       " 'fc_weights.325': tensor([0.8711]),\n",
       " 'fc_weights.326': tensor([0.1192]),\n",
       " 'fc_weights.327': tensor([0.6133]),\n",
       " 'fc_weights.328': tensor([0.8475]),\n",
       " 'fc_weights.329': tensor([0.0143]),\n",
       " 'fc_weights.330': tensor([0.0615]),\n",
       " 'fc_weights.331': tensor([0.4685]),\n",
       " 'fc_weights.332': tensor([0.9549]),\n",
       " 'fc_weights.333': tensor([0.9710]),\n",
       " 'fc_weights.334': tensor([0.9362]),\n",
       " 'fc_weights.335': tensor([0.6918]),\n",
       " 'fc_weights.336': tensor([0.0450]),\n",
       " 'fc_weights.337': tensor([0.4208]),\n",
       " 'fc_weights.338': tensor([0.9861]),\n",
       " 'fc_weights.339': tensor([0.7076]),\n",
       " 'fc_weights.340': tensor([0.2278]),\n",
       " 'fc_weights.341': tensor([0.6735]),\n",
       " 'fc_weights.342': tensor([0.4650]),\n",
       " 'fc_weights.343': tensor([0.5346]),\n",
       " 'fc_weights.344': tensor([0.6681]),\n",
       " 'fc_weights.345': tensor([0.0045]),\n",
       " 'fc_weights.346': tensor([0.3208]),\n",
       " 'fc_weights.347': tensor([0.6250]),\n",
       " 'fc_weights.348': tensor([0.5843]),\n",
       " 'fc_weights.349': tensor([0.4561]),\n",
       " 'fc_weights.350': tensor([0.5521]),\n",
       " 'fc_weights.351': tensor([0.4113]),\n",
       " 'fc_weights.352': tensor([0.9333]),\n",
       " 'fc_weights.353': tensor([0.7285]),\n",
       " 'fc_weights.354': tensor([0.9195]),\n",
       " 'fc_weights.355': tensor([0.8361]),\n",
       " 'fc_weights.356': tensor([0.0717]),\n",
       " 'fc_weights.357': tensor([0.2217]),\n",
       " 'fc_weights.358': tensor([0.1219]),\n",
       " 'fc_weights.359': tensor([0.1402]),\n",
       " 'fc_weights.360': tensor([0.7816]),\n",
       " 'fc_weights.361': tensor([0.6121]),\n",
       " 'fc_weights.362': tensor([0.3482]),\n",
       " 'fc_weights.363': tensor([0.6504]),\n",
       " 'fc_weights.364': tensor([0.2803]),\n",
       " 'fc_weights.365': tensor([0.9507]),\n",
       " 'fc_weights.366': tensor([0.5290]),\n",
       " 'fc_weights.367': tensor([0.7220]),\n",
       " 'fc_weights.368': tensor([0.6440]),\n",
       " 'fc_weights.369': tensor([0.2346]),\n",
       " 'fc_weights.370': tensor([0.9901]),\n",
       " 'fc_weights.371': tensor([0.7146]),\n",
       " 'fc_weights.372': tensor([0.0770]),\n",
       " 'fc_weights.373': tensor([0.9077]),\n",
       " 'fc_weights.374': tensor([0.1698]),\n",
       " 'fc_weights.375': tensor([0.8739]),\n",
       " 'fc_weights.376': tensor([0.4723]),\n",
       " 'fc_weights.377': tensor([0.2339]),\n",
       " 'fc_weights.378': tensor([0.7975]),\n",
       " 'fc_weights.379': tensor([0.6585]),\n",
       " 'fc_weights.380': tensor([0.3807]),\n",
       " 'fc_weights.381': tensor([0.3239]),\n",
       " 'fc_weights.382': tensor([0.0189]),\n",
       " 'fc_weights.383': tensor([0.2437]),\n",
       " 'fc_weights.384': tensor([0.8505]),\n",
       " 'fc_weights.385': tensor([0.4888]),\n",
       " 'fc_weights.386': tensor([0.5200]),\n",
       " 'fc_weights.387': tensor([0.6393]),\n",
       " 'fc_weights.388': tensor([0.0826]),\n",
       " 'fc_weights.389': tensor([0.2117]),\n",
       " 'fc_weights.390': tensor([0.5443]),\n",
       " 'fc_weights.391': tensor([0.9049]),\n",
       " 'fc_weights.392': tensor([0.4385]),\n",
       " 'fc_weights.393': tensor([0.0601]),\n",
       " 'fc_weights.394': tensor([0.0029]),\n",
       " 'fc_weights.395': tensor([0.6535]),\n",
       " 'fc_weights.396': tensor([0.9199]),\n",
       " 'fc_weights.397': tensor([0.5837]),\n",
       " 'fc_weights.398': tensor([0.0586]),\n",
       " 'fc_weights.399': tensor([0.6461]),\n",
       " 'fc_weights.400': tensor([0.7872]),\n",
       " 'fc_weights.401': tensor([0.1815]),\n",
       " 'fc_weights.402': tensor([0.1229]),\n",
       " 'fc_weights.403': tensor([0.8780]),\n",
       " 'fc_weights.404': tensor([0.7861]),\n",
       " 'fc_weights.405': tensor([0.6214]),\n",
       " 'fc_weights.406': tensor([0.6655]),\n",
       " 'fc_weights.407': tensor([0.0268]),\n",
       " 'fc_weights.408': tensor([0.5651]),\n",
       " 'fc_weights.409': tensor([0.7611]),\n",
       " 'fc_weights.410': tensor([0.3256]),\n",
       " 'fc_weights.411': tensor([0.2275]),\n",
       " 'fc_weights.412': tensor([0.2304]),\n",
       " 'fc_weights.413': tensor([0.6075]),\n",
       " 'fc_weights.414': tensor([0.9440]),\n",
       " 'fc_weights.415': tensor([0.8373]),\n",
       " 'fc_weights.416': tensor([0.3902]),\n",
       " 'fc_weights.417': tensor([0.1725]),\n",
       " 'fc_weights.418': tensor([0.6470]),\n",
       " 'fc_weights.419': tensor([0.3482]),\n",
       " 'fc_weights.420': tensor([0.8530]),\n",
       " 'fc_weights.421': tensor([0.9557]),\n",
       " 'fc_weights.422': tensor([0.5696]),\n",
       " 'fc_weights.423': tensor([0.3027]),\n",
       " 'fc_weights.424': tensor([0.2709]),\n",
       " 'fc_weights.425': tensor([0.1711]),\n",
       " 'fc_weights.426': tensor([0.0726]),\n",
       " 'fc_weights.427': tensor([0.9476]),\n",
       " 'fc_weights.428': tensor([0.2040]),\n",
       " 'fc_weights.429': tensor([0.7306]),\n",
       " 'fc_weights.430': tensor([0.9221]),\n",
       " 'fc_weights.431': tensor([0.1667]),\n",
       " 'fc_weights.432': tensor([0.2709]),\n",
       " 'fc_weights.433': tensor([0.5720]),\n",
       " 'fc_weights.434': tensor([0.7101]),\n",
       " 'fc_weights.435': tensor([0.8398]),\n",
       " 'fc_weights.436': tensor([0.2065]),\n",
       " 'fc_weights.437': tensor([0.7501]),\n",
       " 'fc_weights.438': tensor([0.2288]),\n",
       " 'fc_weights.439': tensor([0.0080]),\n",
       " 'fc_weights.440': tensor([0.3613]),\n",
       " 'fc_weights.441': tensor([0.1574]),\n",
       " 'fc_weights.442': tensor([0.1616]),\n",
       " 'fc_weights.443': tensor([0.1784]),\n",
       " 'fc_weights.444': tensor([0.0600]),\n",
       " 'fc_weights.445': tensor([0.0782]),\n",
       " 'fc_weights.446': tensor([0.5548]),\n",
       " 'fc_weights.447': tensor([0.3134]),\n",
       " 'fc_weights.448': tensor([0.6594]),\n",
       " 'fc_weights.449': tensor([0.0428]),\n",
       " 'fc_weights.450': tensor([0.5967]),\n",
       " 'fc_weights.451': tensor([0.9557]),\n",
       " 'fc_weights.452': tensor([0.1050]),\n",
       " 'fc_weights.453': tensor([0.8661]),\n",
       " 'fc_weights.454': tensor([0.4976]),\n",
       " 'fc_weights.455': tensor([0.3576]),\n",
       " 'fc_weights.456': tensor([0.5573]),\n",
       " 'fc_weights.457': tensor([0.2397]),\n",
       " 'fc_weights.458': tensor([0.8565]),\n",
       " 'fc_weights.459': tensor([0.2550]),\n",
       " 'fc_weights.460': tensor([0.1528]),\n",
       " 'fc_weights.461': tensor([0.3285]),\n",
       " 'fc_weights.462': tensor([0.0467]),\n",
       " 'fc_weights.463': tensor([0.5050]),\n",
       " 'fc_weights.464': tensor([0.8533]),\n",
       " 'fc_weights.465': tensor([0.5879]),\n",
       " 'fc_weights.466': tensor([0.8979]),\n",
       " 'fc_weights.467': tensor([0.2039]),\n",
       " 'fc_weights.468': tensor([0.2274]),\n",
       " 'fc_weights.469': tensor([0.7326]),\n",
       " 'fc_weights.470': tensor([0.8664]),\n",
       " 'fc_weights.471': tensor([0.8818]),\n",
       " 'fc_weights.472': tensor([0.9897]),\n",
       " 'fc_weights.473': tensor([0.1989]),\n",
       " 'fc_weights.474': tensor([0.6717]),\n",
       " 'fc_weights.475': tensor([0.6322]),\n",
       " 'fc_weights.476': tensor([0.8413]),\n",
       " 'fc_weights.477': tensor([0.3907]),\n",
       " 'fc_weights.478': tensor([0.4359]),\n",
       " 'fc_weights.479': tensor([0.9175]),\n",
       " 'fc_weights.480': tensor([0.0398]),\n",
       " 'fc_weights.481': tensor([0.7389]),\n",
       " 'fc_weights.482': tensor([0.7645]),\n",
       " 'fc_weights.483': tensor([0.2450]),\n",
       " 'fc_weights.484': tensor([0.2236]),\n",
       " 'fc_weights.485': tensor([0.7556]),\n",
       " 'fc_weights.486': tensor([0.0895]),\n",
       " 'fc_weights.487': tensor([0.5619]),\n",
       " 'fc_weights.488': tensor([0.4674]),\n",
       " 'fc_weights.489': tensor([0.2467]),\n",
       " 'fc_weights.490': tensor([0.3900]),\n",
       " 'fc_weights.491': tensor([0.9551]),\n",
       " 'fc_weights.492': tensor([0.4503]),\n",
       " 'fc_weights.493': tensor([0.3018]),\n",
       " 'fc_weights.494': tensor([0.1677]),\n",
       " 'fc_weights.495': tensor([0.7035]),\n",
       " 'fc_weights.496': tensor([0.0631]),\n",
       " 'fc_weights.497': tensor([0.3558]),\n",
       " 'fc_weights.498': tensor([0.7726]),\n",
       " 'fc_weights.499': tensor([0.8322]),\n",
       " 'fc_weights.500': tensor([0.1154]),\n",
       " 'fc_weights.501': tensor([0.3440]),\n",
       " 'fc_weights.502': tensor([0.6093]),\n",
       " 'fc_weights.503': tensor([0.1467]),\n",
       " 'fc_weights.504': tensor([0.1662]),\n",
       " 'fc_weights.505': tensor([0.2079]),\n",
       " 'fc_weights.506': tensor([0.5103]),\n",
       " 'fc_weights.507': tensor([0.2939]),\n",
       " 'fc_weights.508': tensor([0.7913]),\n",
       " 'fc_weights.509': tensor([0.8855]),\n",
       " 'fc_weights.510': tensor([0.6723]),\n",
       " 'fc_weights.511': tensor([0.8634]),\n",
       " 'fc_weights.512': tensor([0.3973]),\n",
       " 'fc_weights.513': tensor([0.6982]),\n",
       " 'fc_weights.514': tensor([0.0234]),\n",
       " 'fc_weights.515': tensor([0.2785]),\n",
       " 'fc_weights.516': tensor([0.2489]),\n",
       " 'fc_weights.517': tensor([0.0064]),\n",
       " 'fc_weights.518': tensor([0.3801]),\n",
       " 'fc_weights.519': tensor([0.3370]),\n",
       " 'fc_weights.520': tensor([0.8467]),\n",
       " 'fc_weights.521': tensor([0.3352]),\n",
       " 'fc_weights.522': tensor([0.8924]),\n",
       " 'fc_weights.523': tensor([0.4074]),\n",
       " 'fc_weights.524': tensor([0.2309]),\n",
       " 'fc_weights.525': tensor([0.3560]),\n",
       " 'fc_weights.526': tensor([0.4443]),\n",
       " 'fc_weights.527': tensor([0.6790]),\n",
       " 'fc_weights.528': tensor([0.2471]),\n",
       " 'fc_weights.529': tensor([0.2572]),\n",
       " 'fc_weights.530': tensor([0.4837]),\n",
       " 'fc_weights.531': tensor([0.6287]),\n",
       " 'fc_weights.532': tensor([0.6658]),\n",
       " 'fc_weights.533': tensor([0.5282]),\n",
       " 'fc_weights.534': tensor([0.7684]),\n",
       " 'fc_weights.535': tensor([0.1598]),\n",
       " 'fc_weights.536': tensor([0.7683]),\n",
       " 'fc_weights.537': tensor([0.9911]),\n",
       " 'fc_weights.538': tensor([0.8561]),\n",
       " 'fc_weights.539': tensor([0.2831]),\n",
       " 'fc_weights.540': tensor([0.8860]),\n",
       " 'fc_weights.541': tensor([0.3768]),\n",
       " 'fc_weights.542': tensor([0.4888]),\n",
       " 'fc_weights.543': tensor([0.1207]),\n",
       " 'fc_weights.544': tensor([0.1600]),\n",
       " 'fc_weights.545': tensor([0.6687]),\n",
       " 'fc_weights.546': tensor([0.5498]),\n",
       " 'fc_weights.547': tensor([0.2282]),\n",
       " 'fc_weights.548': tensor([0.2844]),\n",
       " 'fc_weights.549': tensor([0.1310]),\n",
       " 'fc_weights.550': tensor([0.3642]),\n",
       " 'fc_weights.551': tensor([0.8770]),\n",
       " 'fc_weights.552': tensor([0.8471]),\n",
       " 'fc_weights.553': tensor([0.7820]),\n",
       " 'fc_weights.554': tensor([0.9578]),\n",
       " 'fc_weights.555': tensor([0.5248]),\n",
       " 'fc_weights.556': tensor([0.2494]),\n",
       " 'fc_weights.557': tensor([0.5794]),\n",
       " 'fc_weights.558': tensor([0.8896]),\n",
       " 'fc_weights.559': tensor([0.7642]),\n",
       " 'fc_weights.560': tensor([0.8780]),\n",
       " 'fc_weights.561': tensor([0.6299]),\n",
       " 'fc_weights.562': tensor([0.9672]),\n",
       " 'fc_weights.563': tensor([0.6397]),\n",
       " 'fc_weights.564': tensor([0.7420]),\n",
       " 'fc_weights.565': tensor([0.6347]),\n",
       " 'fc_weights.566': tensor([0.0500]),\n",
       " 'fc_weights.567': tensor([0.7330]),\n",
       " 'fc_weights.568': tensor([0.3319]),\n",
       " 'fc_weights.569': tensor([0.7163]),\n",
       " 'fc_weights.570': tensor([0.4752]),\n",
       " 'fc_weights.571': tensor([0.5772]),\n",
       " 'fc_weights.572': tensor([0.1026]),\n",
       " 'fc_weights.573': tensor([0.0553]),\n",
       " 'fc_weights.574': tensor([0.0736]),\n",
       " 'fc_weights.575': tensor([0.8005]),\n",
       " 'fc_weights.576': tensor([0.1275]),\n",
       " 'fc_weights.577': tensor([0.3111]),\n",
       " 'fc_weights.578': tensor([0.8820]),\n",
       " 'fc_weights.579': tensor([0.5926]),\n",
       " 'fc_weights.580': tensor([0.7868]),\n",
       " 'fc_weights.581': tensor([0.1855]),\n",
       " 'fc_weights.582': tensor([0.2678]),\n",
       " 'fc_weights.583': tensor([0.1216]),\n",
       " 'fc_weights.584': tensor([0.4754]),\n",
       " 'fc_weights.585': tensor([0.4203]),\n",
       " 'fc_weights.586': tensor([0.1131]),\n",
       " 'fc_weights.587': tensor([0.3086]),\n",
       " 'fc_weights.588': tensor([0.4064]),\n",
       " 'fc_weights.589': tensor([0.2162]),\n",
       " 'fc_weights.590': tensor([0.2701]),\n",
       " 'fc_weights.591': tensor([0.0056]),\n",
       " 'fc_weights.592': tensor([0.3150]),\n",
       " 'fc_weights.593': tensor([0.3511]),\n",
       " 'fc_weights.594': tensor([0.8519]),\n",
       " 'fc_weights.595': tensor([0.1338]),\n",
       " 'fc_weights.596': tensor([0.8400]),\n",
       " 'fc_weights.597': tensor([0.0077]),\n",
       " 'fc_weights.598': tensor([0.3578]),\n",
       " 'fc_weights.599': tensor([0.9120]),\n",
       " 'fc_weights.600': tensor([0.9130]),\n",
       " 'fc_weights.601': tensor([0.8631]),\n",
       " 'fc_weights.602': tensor([0.2471]),\n",
       " 'fc_weights.603': tensor([0.4306]),\n",
       " 'fc_weights.604': tensor([0.3207]),\n",
       " 'fc_weights.605': tensor([0.7904]),\n",
       " 'fc_weights.606': tensor([0.0885]),\n",
       " 'fc_weights.607': tensor([0.9312]),\n",
       " 'fc_weights.608': tensor([0.9925]),\n",
       " 'fc_weights.609': tensor([0.9711]),\n",
       " 'fc_weights.610': tensor([0.6080]),\n",
       " 'fc_weights.611': tensor([0.8758]),\n",
       " 'fc_weights.612': tensor([0.8867]),\n",
       " 'fc_weights.613': tensor([0.5778]),\n",
       " 'fc_weights.614': tensor([0.7596]),\n",
       " 'fc_weights.615': tensor([0.5265]),\n",
       " 'fc_weights.616': tensor([0.2661]),\n",
       " 'fc_weights.617': tensor([0.1774]),\n",
       " 'fc_weights.618': tensor([0.9236]),\n",
       " 'fc_weights.619': tensor([0.8819]),\n",
       " 'fc_weights.620': tensor([0.8168]),\n",
       " 'fc_weights.621': tensor([0.9207]),\n",
       " 'fc_weights.622': tensor([0.9289]),\n",
       " 'fc_weights.623': tensor([0.0714]),\n",
       " 'fc_weights.624': tensor([0.6081]),\n",
       " 'fc_weights.625': tensor([0.8485]),\n",
       " 'fc_weights.626': tensor([0.2819]),\n",
       " 'fc_weights.627': tensor([0.5874]),\n",
       " 'fc_weights.628': tensor([0.4672]),\n",
       " 'fc_weights.629': tensor([0.0665]),\n",
       " 'fc_weights.630': tensor([0.3806]),\n",
       " 'fc_weights.631': tensor([0.2647]),\n",
       " 'fc_weights.632': tensor([0.0823]),\n",
       " 'fc_weights.633': tensor([0.9066]),\n",
       " 'fc_weights.634': tensor([0.1315]),\n",
       " 'fc_weights.635': tensor([0.8551]),\n",
       " 'fc_weights.636': tensor([0.3051]),\n",
       " 'fc_weights.637': tensor([0.6440]),\n",
       " 'fc_weights.638': tensor([0.9074]),\n",
       " 'fc_weights.639': tensor([0.7919]),\n",
       " 'fc_weights.640': tensor([0.3692]),\n",
       " 'fc_weights.641': tensor([0.2489]),\n",
       " 'fc_weights.642': tensor([0.6011]),\n",
       " 'fc_weights.643': tensor([0.2534]),\n",
       " 'fc_weights.644': tensor([0.1858]),\n",
       " 'fc_weights.645': tensor([0.8550]),\n",
       " 'fc_weights.646': tensor([0.3536]),\n",
       " 'fc_weights.647': tensor([0.7138]),\n",
       " 'fc_weights.648': tensor([0.6154]),\n",
       " 'fc_weights.649': tensor([0.2099]),\n",
       " 'fc_weights.650': tensor([0.9975]),\n",
       " 'fc_weights.651': tensor([0.6819]),\n",
       " 'fc_weights.652': tensor([0.2469]),\n",
       " 'fc_weights.653': tensor([0.7277]),\n",
       " 'fc_weights.654': tensor([0.9778]),\n",
       " 'fc_weights.655': tensor([0.9167]),\n",
       " 'fc_weights.656': tensor([0.9959]),\n",
       " 'fc_weights.657': tensor([0.5132]),\n",
       " 'fc_weights.658': tensor([0.0771]),\n",
       " 'fc_weights.659': tensor([0.0618]),\n",
       " 'fc_weights.660': tensor([0.2892]),\n",
       " 'fc_weights.661': tensor([0.3903]),\n",
       " 'fc_weights.662': tensor([0.0595]),\n",
       " 'fc_weights.663': tensor([0.5793]),\n",
       " 'fc_weights.664': tensor([0.6411]),\n",
       " 'fc_weights.665': tensor([0.6597]),\n",
       " 'fc_weights.666': tensor([0.6888]),\n",
       " 'fc_weights.667': tensor([0.1010]),\n",
       " 'fc_weights.668': tensor([0.0786]),\n",
       " 'fc_weights.669': tensor([0.6745]),\n",
       " 'fc_weights.670': tensor([0.3146]),\n",
       " 'fc_weights.671': tensor([0.5480]),\n",
       " 'fc_weights.672': tensor([0.8201]),\n",
       " 'fc_weights.673': tensor([0.4633]),\n",
       " 'fc_weights.674': tensor([0.7091]),\n",
       " 'fc_weights.675': tensor([0.9946]),\n",
       " 'fc_weights.676': tensor([0.5373]),\n",
       " 'fc_weights.677': tensor([0.6324]),\n",
       " 'fc_weights.678': tensor([0.3732]),\n",
       " 'fc_weights.679': tensor([0.9691]),\n",
       " 'fc_weights.680': tensor([0.6322]),\n",
       " 'fc_weights.681': tensor([0.7498]),\n",
       " 'fc_weights.682': tensor([0.6701]),\n",
       " 'fc_weights.683': tensor([0.6551]),\n",
       " 'fc_weights.684': tensor([0.1816]),\n",
       " 'fc_weights.685': tensor([0.3969]),\n",
       " 'fc_weights.686': tensor([0.6722]),\n",
       " 'fc_weights.687': tensor([0.0051]),\n",
       " 'fc_weights.688': tensor([0.1791]),\n",
       " 'fc_weights.689': tensor([0.5287]),\n",
       " 'fc_weights.690': tensor([0.4960]),\n",
       " 'fc_weights.691': tensor([0.6812]),\n",
       " 'fc_weights.692': tensor([0.5508]),\n",
       " 'fc_weights.693': tensor([0.6460]),\n",
       " 'fc_weights.694': tensor([0.3578]),\n",
       " 'fc_weights.695': tensor([0.0082]),\n",
       " 'fc_weights.696': tensor([0.4276]),\n",
       " 'fc_weights.697': tensor([0.1977]),\n",
       " 'fc_weights.698': tensor([0.3648]),\n",
       " 'fc_weights.699': tensor([0.6015]),\n",
       " 'fc_weights.700': tensor([0.1512]),\n",
       " 'fc_weights.701': tensor([0.5620]),\n",
       " 'fc_weights.702': tensor([0.4886]),\n",
       " 'fc_weights.703': tensor([0.3733]),\n",
       " 'fc_weights.704': tensor([0.7528]),\n",
       " 'fc_weights.705': tensor([0.4631]),\n",
       " 'fc_weights.706': tensor([0.2164]),\n",
       " 'fc_weights.707': tensor([0.7096]),\n",
       " 'fc_weights.708': tensor([0.2844]),\n",
       " 'fc_weights.709': tensor([0.8805]),\n",
       " 'fc_weights.710': tensor([0.6141]),\n",
       " 'fc_weights.711': tensor([0.4621]),\n",
       " 'fc_weights.712': tensor([0.6171]),\n",
       " 'fc_weights.713': tensor([0.0505]),\n",
       " 'fc_weights.714': tensor([0.5244]),\n",
       " 'fc_weights.715': tensor([0.6539]),\n",
       " 'fc_weights.716': tensor([0.5997]),\n",
       " 'fc_weights.717': tensor([0.1261]),\n",
       " 'fc_weights.718': tensor([0.9037]),\n",
       " 'fc_weights.719': tensor([0.9317]),\n",
       " 'fc_weights.720': tensor([0.9927]),\n",
       " 'fc_weights.721': tensor([0.9289]),\n",
       " 'fc_weights.722': tensor([0.3419]),\n",
       " 'fc_weights.723': tensor([0.4781]),\n",
       " 'fc_weights.724': tensor([0.9177]),\n",
       " 'fc_weights.725': tensor([0.7975]),\n",
       " 'fc_weights.726': tensor([0.0261]),\n",
       " 'fc_weights.727': tensor([0.9603]),\n",
       " 'fc_weights.728': tensor([0.7458]),\n",
       " 'fc_weights.729': tensor([0.2070]),\n",
       " 'fc_weights.730': tensor([0.6496]),\n",
       " 'fc_weights.731': tensor([0.0183]),\n",
       " 'fc_weights.732': tensor([0.4439]),\n",
       " 'fc_weights.733': tensor([0.2163]),\n",
       " 'fc_weights.734': tensor([0.7598]),\n",
       " 'fc_weights.735': tensor([0.4471]),\n",
       " 'fc_weights.736': tensor([0.4724]),\n",
       " 'fc_weights.737': tensor([0.3902]),\n",
       " 'fc_weights.738': tensor([0.3351]),\n",
       " 'fc_weights.739': tensor([0.4056]),\n",
       " 'fc_weights.740': tensor([0.5142]),\n",
       " 'fc_weights.741': tensor([0.5281]),\n",
       " 'fc_weights.742': tensor([0.5527]),\n",
       " 'fc_weights.743': tensor([0.5954]),\n",
       " 'fc_weights.744': tensor([0.9328]),\n",
       " 'fc_weights.745': tensor([0.0856]),\n",
       " 'fc_weights.746': tensor([0.6105]),\n",
       " 'fc_weights.747': tensor([0.9170]),\n",
       " 'fc_weights.748': tensor([0.2149]),\n",
       " 'fc_weights.749': tensor([0.2371]),\n",
       " 'fc_weights.750': tensor([0.0787]),\n",
       " 'fc_weights.751': tensor([0.8912]),\n",
       " 'fc_weights.752': tensor([0.4434]),\n",
       " 'fc_weights.753': tensor([0.1828]),\n",
       " 'fc_weights.754': tensor([0.2419]),\n",
       " 'fc_weights.755': tensor([0.4014]),\n",
       " 'fc_weights.756': tensor([0.9063]),\n",
       " 'fc_weights.757': tensor([0.2846]),\n",
       " 'fc_weights.758': tensor([0.3928]),\n",
       " 'fc_weights.759': tensor([0.7510]),\n",
       " 'fc_weights.760': tensor([0.1939]),\n",
       " 'fc_weights.761': tensor([0.2029]),\n",
       " 'fc_weights.762': tensor([0.2135]),\n",
       " 'fc_weights.763': tensor([0.6513]),\n",
       " 'fc_weights.764': tensor([0.2182]),\n",
       " 'fc_weights.765': tensor([0.9014]),\n",
       " 'fc_weights.766': tensor([0.5389]),\n",
       " 'fc_weights.767': tensor([0.0655]),\n",
       " 'fc_weights.768': tensor([0.8326]),\n",
       " 'fc_weights.769': tensor([0.4620]),\n",
       " 'fc_weights.770': tensor([0.6291]),\n",
       " 'fc_weights.771': tensor([0.3477]),\n",
       " 'fc_weights.772': tensor([0.0022]),\n",
       " 'fc_weights.773': tensor([0.0499]),\n",
       " 'fc_weights.774': tensor([0.7804]),\n",
       " 'fc_weights.775': tensor([0.0233]),\n",
       " 'fc_weights.776': tensor([0.8129]),\n",
       " 'fc_weights.777': tensor([0.5339]),\n",
       " 'fc_weights.778': tensor([0.8210]),\n",
       " 'fc_weights.779': tensor([0.9763]),\n",
       " 'fc_weights.780': tensor([0.2680]),\n",
       " 'fc_weights.781': tensor([0.3246]),\n",
       " 'fc_weights.782': tensor([0.9250]),\n",
       " 'fc_weights.783': tensor([0.2904])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(testmodel.state_dict().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(current_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([184, 198, 191, 212])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa93bea27d0>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(acc_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ParamServer",
   "language": "python",
   "name": "paramserver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

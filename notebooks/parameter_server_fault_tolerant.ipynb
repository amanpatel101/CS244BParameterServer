{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from filelock import FileLock\n",
    "import numpy as np\n",
    "import ray\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from consistent_hashing import ConsistentHash\n",
    "import math \n",
    "from time import time\n",
    "def get_data_loader():\n",
    "    \n",
    "    \"\"\"Safely downloads data. Returns training/validation set dataloader.\"\"\"\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    \n",
    "    class MNISTEvenOddDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, ready_data):\n",
    "            self.img_data = ready_data.data\n",
    "            self.labels = ready_data.targets % 2\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "        def __getitem__(self, ind):\n",
    "            return torch.true_divide(self.img_data[ind].view(-1, 28 * 28).squeeze(), 255), torch.tensor([self.labels[ind]])\n",
    "\n",
    "\n",
    "    \n",
    "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
    "        \n",
    "        train_dataset = datasets.MNIST(\n",
    "                \"~/data\", train=True, download=True, transform=mnist_transforms\n",
    "            )\n",
    "        \n",
    "        test_dataset = datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            MNISTEvenOddDataset(train_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "             MNISTEvenOddDataset(test_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Evaluates the accuracy of the model on a validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # This is only set to finish evaluation faster.\n",
    "            if batch_idx * len(data) > 1024:\n",
    "                break\n",
    "            outputs = nn.Sigmoid()(model(data))\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = outputs > 0.5\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    \"\"\"Small Linear Network for MNIST.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc_weights = nn.ParameterList([nn.Parameter(torch.empty(1)) for weight in range(784)])\n",
    "        init_fc = [nn.init.uniform_(x) for x in self.fc_weights]\n",
    "        \n",
    "        self.fc_bias = nn.Parameter(torch.empty(1))\n",
    "        nn.init.uniform_(self.fc_bias)\n",
    "        \n",
    "    #def __init__(self):\n",
    "    #    super(LinearNet, self).__init__()\n",
    "    #    self.fc = nn.Linear(28*28, 1)\n",
    "    #    nn.init.normal(self.fc.weight)\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    x = self.fc(x)\n",
    "    #    return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #fc_layer = torch.cat(tuple(self.fc_weights)).unsqueeze(0)\n",
    "        #x = x @ fc_layer.T + self.fc_bias\n",
    "        for i, param in enumerate(self.fc_weights):\n",
    "            if i==0:\n",
    "                p=x[:,i]*param\n",
    "            else:\n",
    "                p += x[:,i]*param\n",
    "        x = p.unsqueeze(1) + self.fc_bias\n",
    "        return x\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "\n",
    "    def set_weights(self, keys, weights): \n",
    "        flatten_weights =  [item for sublist in weights for item in sublist]\n",
    "        self.load_state_dict({keys[i]:flatten_weights[i] for i in range(len(keys))})\n",
    "        \n",
    "    def get_gradients(self, keys):\n",
    "        grads = {}\n",
    "\n",
    "        for name, p in self.named_parameters():\n",
    "            if name in keys:\n",
    "                grad = None if p.grad is None else p.grad.data.cpu().numpy()\n",
    "                grads[name] = grad\n",
    "\n",
    "        return [grads[key] for key in keys]\n",
    "\n",
    "    def set_gradients(self, gradients):\n",
    "        for g, p in zip(gradients, self.parameters()):\n",
    "            if g is not None:\n",
    "                p.grad = torch.from_numpy(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote  \n",
    "class ParameterServer(object):\n",
    "    def __init__(self, keys, values):\n",
    "        self.weights = dict(zip(keys, values))\n",
    "\n",
    "    def apply_gradients(self, keys, lr, *values):\n",
    "        summed_gradients = [\n",
    "            np.stack(gradient_zip).sum(axis=0) for gradient_zip in zip(*values)\n",
    "        ]\n",
    "    \n",
    "        idx = 0\n",
    "        for key, value in zip(keys, summed_gradients):\n",
    "            self.weights[key] -= lr * torch.from_numpy(summed_gradients[idx])\n",
    "            idx+=1\n",
    "\n",
    "        return [self.weights[key] for key in keys]\n",
    "    \n",
    "    def add_weight(self, key, value):\n",
    "        self.weights[key] = value\n",
    "    \n",
    "    def get_weights(self, keys):\n",
    "        return [self.weights[key] for key in keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class DataWorker(object):\n",
    "    def __init__(self, keys):\n",
    "        self.model = LinearNet()\n",
    "        self.data_iterator = iter(get_data_loader()[0])\n",
    "        self.keys = keys\n",
    "        self.key_set = set(self.keys)\n",
    "        for key, value in dict(self.model.named_parameters()).items():\n",
    "            if key not in self.key_set:\n",
    "                value.requires_grad=False\n",
    "\n",
    "        \n",
    "    def update_weights(self, keys, *weights):\n",
    "        self.model.set_weights(keys, weights)\n",
    "        \n",
    "    def update_trainable(self, keys):\n",
    "        self.keys = keys\n",
    "        self.key_set = set(self.keys)\n",
    "        for key, value in dict(self.model.named_parameters()).items():\n",
    "            if key in self.key_set:\n",
    "                value.requires_grad = True\n",
    "            else:\n",
    "                value.requires_grad = False\n",
    "       \n",
    "\n",
    "    def compute_gradients(self):\n",
    "        #self.model.set_weights(keys, weights)\n",
    "        try:\n",
    "            data, target = next(self.data_iterator)\n",
    "        except StopIteration:  # When the epoch ends, start a new epoch.\n",
    "            self.data_iterator = iter(get_data_loader()[0])\n",
    "            data, target = next(self.data_iterator)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        output = self.model(data)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, target.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        return self.model.get_gradients(self.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 23:31:14,627\tINFO worker.py:963 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "iterations = 500\n",
    "num_workers = 1 # number of workers per server\n",
    "num_servers = 5 # number of servers\n",
    "hashes_per_server = 100\n",
    "\n",
    "def Scheduler(num_servers, hashes_per_server=50):\n",
    "    \n",
    "    model = LinearNet()\n",
    "    key_values = model.get_weights()\n",
    "    keys = np.array(list(key_values.keys()))\n",
    "    #print(keys)\n",
    "    #print(key_values) z\n",
    "    values = [key_values[key] for key in keys]\n",
    "    #values = [key_values[key] for key in keys]\n",
    "    \n",
    "    key_indices = {key: x for x, key in enumerate(keys)}\n",
    "   \n",
    "    # distributing weights across servers - do this using consistency hashing\n",
    "    server_ids = [\"server\" + str(ind) for ind in range(num_servers)]\n",
    "    hasher = ConsistentHash(keys, server_ids, hashes_per_server)\n",
    "    servers = [ParameterServer.remote(keys[[key_indices[key] for key in hasher.get_keys_per_node()[serv]]], \n",
    "                                      [values[key_indices[key]] for key in hasher.get_keys_per_node()[serv]]) for serv in server_ids]\n",
    "    # servers = [ParameterServer.remote(keys[0:1], values[0:1]), ParameterServer.remote(keys[1:2], values[1:2])]\n",
    "    \n",
    "    return hasher, servers, keys, model, hasher.get_keys_per_node(), server_ids.copy()\n",
    "\n",
    "hasher, servers, keys, model, weight_assignments, server_ids =  Scheduler(num_servers, hashes_per_server)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# creating equal workers per server\n",
    "\n",
    "workers = [[DataWorker.remote(weight_assignments[\"server\" + str(j)]) for i in range(num_workers)] for j in range(num_servers)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight_assignments[\"server0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_data_loader()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running synchronous parameter server training.\n",
      "Iter 0: \taccuracy is 51.9\n",
      "Iter 5: \taccuracy is 51.9\n",
      "Iter 10: \taccuracy is 51.9\n",
      "Iter 15: \taccuracy is 51.9\n",
      "Iter 20: \taccuracy is 52.0\n",
      "Iter 25: \taccuracy is 46.5\n",
      "Iter 30: \taccuracy is 47.2\n",
      "Iter 35: \taccuracy is 47.8\n",
      "Iter 40: \taccuracy is 47.7\n",
      "Iter 45: \taccuracy is 48.1\n",
      "Iter 50: \taccuracy is 51.3\n",
      "Iter 55: \taccuracy is 68.0\n",
      "Iter 60: \taccuracy is 61.2\n",
      "Iter 65: \taccuracy is 49.9\n",
      "Iter 70: \taccuracy is 48.1\n",
      "Iter 75: \taccuracy is 48.1\n",
      "Iter 80: \taccuracy is 48.1\n",
      "Iter 85: \taccuracy is 48.1\n",
      "Iter 90: \taccuracy is 48.5\n",
      "Iter 95: \taccuracy is 56.9\n",
      "Iter 100: \taccuracy is 75.7\n",
      "Iter 105: \taccuracy is 78.2\n",
      "Iter 110: \taccuracy is 76.6\n",
      "Iter 115: \taccuracy is 72.2\n",
      "Iter 120: \taccuracy is 70.1\n",
      "Iter 125: \taccuracy is 78.6\n",
      "Iter 130: \taccuracy is 75.2\n",
      "Iter 135: \taccuracy is 59.6\n",
      "Iter 140: \taccuracy is 54.3\n",
      "Iter 145: \taccuracy is 74.2\n",
      "Iter 150: \taccuracy is 80.6\n",
      "Iter 155: \taccuracy is 70.9\n",
      "Iter 160: \taccuracy is 63.5\n",
      "Iter 165: \taccuracy is 77.2\n",
      "Iter 170: \taccuracy is 82.0\n",
      "Iter 175: \taccuracy is 73.6\n",
      "Iter 180: \taccuracy is 63.2\n",
      "Iter 185: \taccuracy is 78.7\n",
      "Iter 190: \taccuracy is 82.3\n",
      "Iter 195: \taccuracy is 78.6\n",
      "Iter 200: \taccuracy is 72.7\n",
      "Iter 205: \taccuracy is 79.4\n",
      "Iter 210: \taccuracy is 81.7\n",
      "Iter 215: \taccuracy is 82.3\n",
      "Iter 220: \taccuracy is 79.2\n",
      "Iter 225: \taccuracy is 83.0\n",
      "Iter 230: \taccuracy is 82.2\n",
      "Iter 235: \taccuracy is 80.8\n",
      "Iter 240: \taccuracy is 78.6\n",
      "Iter 245: \taccuracy is 82.2\n",
      "Iter 250: \taccuracy is 82.8\n",
      "Iter 255: \taccuracy is 83.2\n",
      "Iter 260: \taccuracy is 81.8\n",
      "Iter 265: \taccuracy is 83.9\n",
      "Iter 270: \taccuracy is 83.0\n",
      "Iter 275: \taccuracy is 83.1\n",
      "Iter 280: \taccuracy is 81.5\n",
      "Iter 285: \taccuracy is 83.6\n",
      "Iter 290: \taccuracy is 83.6\n",
      "Iter 295: \taccuracy is 83.9\n",
      "Iter 300: \taccuracy is 83.2\n",
      "Iter 305: \taccuracy is 84.5\n",
      "Iter 310: \taccuracy is 83.4\n",
      "Iter 315: \taccuracy is 83.1\n",
      "Iter 320: \taccuracy is 82.3\n",
      "Iter 325: \taccuracy is 83.9\n",
      "Iter 330: \taccuracy is 84.4\n",
      "Iter 335: \taccuracy is 84.3\n",
      "Iter 340: \taccuracy is 83.5\n",
      "Iter 345: \taccuracy is 85.0\n",
      "Iter 350: \taccuracy is 84.4\n",
      "Iter 355: \taccuracy is 83.8\n",
      "Iter 360: \taccuracy is 82.6\n",
      "Iter 365: \taccuracy is 84.1\n",
      "Iter 370: \taccuracy is 84.7\n",
      "Iter 375: \taccuracy is 85.0\n",
      "Iter 380: \taccuracy is 83.9\n",
      "Iter 385: \taccuracy is 85.2\n",
      "Iter 390: \taccuracy is 85.2\n",
      "Iter 395: \taccuracy is 83.5\n",
      "Iter 400: \taccuracy is 82.9\n",
      "Iter 405: \taccuracy is 84.5\n",
      "Iter 410: \taccuracy is 85.4\n",
      "Iter 415: \taccuracy is 84.6\n",
      "Iter 420: \taccuracy is 83.0\n",
      "Iter 425: \taccuracy is 85.9\n",
      "Iter 430: \taccuracy is 85.7\n",
      "Iter 435: \taccuracy is 83.4\n",
      "Iter 440: \taccuracy is 82.2\n",
      "Iter 445: \taccuracy is 84.4\n",
      "Iter 450: \taccuracy is 85.6\n",
      "Iter 455: \taccuracy is 84.0\n",
      "Iter 460: \taccuracy is 82.4\n",
      "Iter 465: \taccuracy is 86.0\n",
      "Iter 470: \taccuracy is 85.2\n",
      "Iter 475: \taccuracy is 82.6\n",
      "Iter 480: \taccuracy is 80.1\n",
      "Iter 485: \taccuracy is 84.1\n",
      "Iter 490: \taccuracy is 85.9\n",
      "Iter 495: \taccuracy is 82.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Running synchronous parameter server training.\")\n",
    "lr=0.1\n",
    "failure_iter=60\n",
    "failure_server=\"server4\"\n",
    "\n",
    "# we need to get a new keys order because we are not assuming a ordering in keys\n",
    "current_weights = []\n",
    "keys_order = []\n",
    "\n",
    "for j in range(num_servers):\n",
    "    keys_order.extend(weight_assignments[\"server\" + str(j)])\n",
    "    current_weights.extend(ray.get(servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]))) \n",
    "curr_weights_ckpt = current_weights.copy()\n",
    "\n",
    "time_per_iteration = []\n",
    "for i in range(iterations):\n",
    " \n",
    "    #start = time()\n",
    "    \n",
    "    if i == failure_iter:\n",
    "        #Define parameters that will need to be moved\n",
    "        failure_params = weight_assignments[failure_server]\n",
    "        #Delete server from hash ring and reassign params\n",
    "        hasher.delete_node_and_reassign_to_others(failure_server)\n",
    "        weight_assignments = hasher.get_keys_per_node()\n",
    "        #Update servers and workers\n",
    "        num_servers -= 1\n",
    "        server_ind = server_ids.index(failure_server)\n",
    "        server_ids = server_ids[0 : server_ind] + server_ids[server_ind + 1 : ]\n",
    "        servers = servers[0 : server_ind] + servers[server_ind + 1 : ]\n",
    "        workers = workers[0 : server_ind] + workers[server_ind + 1 : ]\n",
    "        #Add each relevant parameter to its new server\n",
    "        server_dict = {server_ids[x]:servers[x] for x in range(len(server_ids))}\n",
    "        for ind, param in enumerate(failure_params):\n",
    "            server_dict[hasher.get_key_to_node_map()[param]].add_weight.remote(param, curr_weights_ckpt[server_ind][ind])\n",
    "        #Update these parameters for each worker to make them trainable\n",
    "        [workers[j][idx].update_trainable.remote(weight_assignments[\"server\" + str(j)]) for  idx  in range(num_workers) for j in range(num_servers)]\n",
    "        keys_order = []\n",
    "        for j in range(num_servers):\n",
    "            keys_order.extend(weight_assignments[\"server\" + str(j)])\n",
    "\n",
    "    \n",
    "    # sync all weights on workers\n",
    "    if i % 20 == 0:\n",
    "        curr_weights_ckpt = current_weights.copy()\n",
    "        # get weights from server\n",
    "        #current_weights = [servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]) for j in range(num_servers)] \n",
    "\n",
    "        # update weights on all workers\n",
    "        [workers[j][idx].update_weights.remote(keys_order, *current_weights) for  idx  in range(num_workers) for j in range(num_servers)]\n",
    "    \n",
    "        \n",
    "    # use local cache of weights and get gradients from workers\n",
    "    gradients = [[workers[j][idx].compute_gradients.remote() for  idx  in range(num_workers)] for j in range(num_servers)]\n",
    "\n",
    "    start = time()\n",
    "    # Updates gradients to specfic parameter servers\n",
    "    current_weights_t = [servers[j].apply_gradients.remote(weight_assignments[\"server\" + str(j)], lr, *gradients[j]) for j in range(num_servers)]\n",
    "    current_weights = ray.get(current_weights_t)\n",
    "    \n",
    "    end = time()\n",
    "    time_per_iteration.append(end-start)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        # Evaluate the current model.\n",
    "        # current_weights = [servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]) for j in range(num_servers)] \n",
    "      \n",
    "        # we are once again using the server to key mapping to set the weight back\n",
    "        model.set_weights(keys_order, current_weights)\n",
    "        accuracy = evaluate(model, test_loader)\n",
    "        print(\"Iter {}: \\taccuracy is {:.1f}\".format(i, accuracy))\n",
    "    #rint(\"\\n\")\n",
    "\n",
    "#print(\"Final accuracy is {:.1f}.\".format(accuracy))\n",
    "# Clean up Ray resources and processes before the next example.\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1470847402163641"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(time_per_iteration[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02577144327194029"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(time_per_iteration[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.588714361190796,\n",
       " 0.13730883598327637,\n",
       " 0.13313031196594238,\n",
       " 0.1397244930267334,\n",
       " 0.13124918937683105,\n",
       " 0.1424236297607422,\n",
       " 0.13758516311645508,\n",
       " 0.13960599899291992,\n",
       " 0.13950729370117188,\n",
       " 0.1336812973022461,\n",
       " 0.19435453414916992,\n",
       " 0.13529610633850098,\n",
       " 0.14109039306640625,\n",
       " 0.13781523704528809,\n",
       " 0.1302502155303955,\n",
       " 0.13529276847839355,\n",
       " 0.13292670249938965,\n",
       " 0.13463830947875977,\n",
       " 0.14171147346496582,\n",
       " 0.14144682884216309,\n",
       " 0.19423699378967285,\n",
       " 0.12644362449645996,\n",
       " 0.1280200481414795,\n",
       " 0.12221574783325195,\n",
       " 0.12069988250732422,\n",
       " 0.12054061889648438,\n",
       " 0.12222146987915039,\n",
       " 0.18278765678405762,\n",
       " 0.19629263877868652,\n",
       " 0.12352895736694336,\n",
       " 0.19074463844299316,\n",
       " 0.12006068229675293,\n",
       " 0.12671160697937012,\n",
       " 0.12433290481567383,\n",
       " 0.1338052749633789,\n",
       " 0.1232297420501709,\n",
       " 0.12189030647277832,\n",
       " 0.12937450408935547,\n",
       " 0.1230320930480957,\n",
       " 0.12760472297668457,\n",
       " 0.1854264736175537,\n",
       " 0.11983871459960938,\n",
       " 0.12497305870056152,\n",
       " 0.12382173538208008,\n",
       " 0.1202993392944336,\n",
       " 0.1263740062713623,\n",
       " 0.12149739265441895,\n",
       " 0.12420082092285156,\n",
       " 0.12353873252868652,\n",
       " 0.12755489349365234,\n",
       " 0.1820816993713379,\n",
       " 0.21605634689331055,\n",
       " 0.14264750480651855,\n",
       " 0.14026618003845215,\n",
       " 0.1417255401611328,\n",
       " 0.14444398880004883,\n",
       " 0.1253950595855713,\n",
       " 0.1298661231994629,\n",
       " 0.12682151794433594,\n",
       " 0.12833857536315918,\n",
       " 0.16559243202209473,\n",
       " 0.12835311889648438,\n",
       " 0.1300966739654541,\n",
       " 0.13056087493896484,\n",
       " 0.12795424461364746,\n",
       " 0.12892556190490723,\n",
       " 0.14424395561218262,\n",
       " 0.12783074378967285,\n",
       " 0.12282490730285645,\n",
       " 0.12252283096313477,\n",
       " 0.1841893196105957,\n",
       " 0.14269757270812988,\n",
       " 0.13878941535949707,\n",
       " 0.12727856636047363,\n",
       " 0.12466883659362793,\n",
       " 0.12364339828491211,\n",
       " 0.12157130241394043,\n",
       " 0.12425351142883301,\n",
       " 0.12210750579833984,\n",
       " 0.12828779220581055,\n",
       " 0.17039275169372559,\n",
       " 0.12045884132385254,\n",
       " 0.12500882148742676,\n",
       " 0.12307024002075195,\n",
       " 0.1196908950805664,\n",
       " 0.1222832202911377,\n",
       " 0.14913153648376465,\n",
       " 0.13460469245910645,\n",
       " 0.14117145538330078,\n",
       " 0.13710260391235352,\n",
       " 0.17919468879699707,\n",
       " 0.13693761825561523,\n",
       " 0.13512110710144043,\n",
       " 0.13065028190612793,\n",
       " 0.13193869590759277,\n",
       " 0.13787484169006348,\n",
       " 0.13448572158813477,\n",
       " 0.1340956687927246,\n",
       " 0.13578414916992188,\n",
       " 0.13019084930419922,\n",
       " 0.1794435977935791,\n",
       " 0.12315773963928223,\n",
       " 0.1231389045715332,\n",
       " 0.1212165355682373,\n",
       " 0.1252918243408203,\n",
       " 0.12147641181945801,\n",
       " 0.12157201766967773,\n",
       " 0.12213253974914551,\n",
       " 0.1202399730682373,\n",
       " 0.13007736206054688,\n",
       " 0.18225550651550293,\n",
       " 0.12760090827941895,\n",
       " 0.19522309303283691,\n",
       " 0.11833643913269043,\n",
       " 0.12111568450927734,\n",
       " 0.11885571479797363,\n",
       " 0.12067461013793945,\n",
       " 0.12244105339050293,\n",
       " 0.12076330184936523,\n",
       " 0.12005472183227539,\n",
       " 0.18515920639038086,\n",
       " 0.12366986274719238,\n",
       " 0.15457391738891602,\n",
       " 0.16038894653320312,\n",
       " 0.15955090522766113,\n",
       " 0.1561133861541748,\n",
       " 0.1503736972808838,\n",
       " 0.16120076179504395,\n",
       " 0.15075325965881348,\n",
       " 0.15771770477294922,\n",
       " 0.21598196029663086,\n",
       " 0.15779471397399902,\n",
       " 0.15866422653198242,\n",
       " 0.16682839393615723,\n",
       " 0.1515355110168457,\n",
       " 0.15619421005249023,\n",
       " 0.15932250022888184,\n",
       " 0.16324138641357422,\n",
       " 0.16686248779296875,\n",
       " 0.16022872924804688,\n",
       " 0.21947574615478516,\n",
       " 0.16245388984680176,\n",
       " 0.1614837646484375,\n",
       " 0.15810441970825195,\n",
       " 0.15517234802246094,\n",
       " 0.16928768157958984,\n",
       " 0.16161417961120605,\n",
       " 0.14965271949768066,\n",
       " 0.1609635353088379,\n",
       " 0.16119647026062012,\n",
       " 0.21338725090026855,\n",
       " 0.1564497947692871,\n",
       " 0.15535902976989746,\n",
       " 0.1545271873474121,\n",
       " 0.15725326538085938,\n",
       " 0.15796542167663574,\n",
       " 0.16361761093139648,\n",
       " 0.15983223915100098,\n",
       " 0.15895509719848633,\n",
       " 0.15792346000671387,\n",
       " 0.18034625053405762,\n",
       " 0.13035082817077637,\n",
       " 0.13068294525146484,\n",
       " 0.1308279037475586,\n",
       " 0.12936925888061523,\n",
       " 0.13068056106567383,\n",
       " 0.13065576553344727,\n",
       " 0.1311483383178711,\n",
       " 0.13117170333862305,\n",
       " 0.1307237148284912,\n",
       " 0.1846904754638672,\n",
       " 0.1312258243560791,\n",
       " 0.12980985641479492,\n",
       " 0.1765282154083252,\n",
       " 0.14496064186096191,\n",
       " 0.14835309982299805,\n",
       " 0.14578461647033691,\n",
       " 0.14438462257385254,\n",
       " 0.13753080368041992,\n",
       " 0.1454150676727295,\n",
       " 0.20393085479736328,\n",
       " 0.15209293365478516,\n",
       " 0.14257335662841797,\n",
       " 0.1346902847290039,\n",
       " 0.14958977699279785,\n",
       " 0.16284584999084473,\n",
       " 0.1621861457824707,\n",
       " 0.15857195854187012,\n",
       " 0.15723061561584473,\n",
       " 0.16019105911254883,\n",
       " 0.21585798263549805,\n",
       " 0.15961360931396484,\n",
       " 0.16126513481140137,\n",
       " 0.15205740928649902,\n",
       " 0.16143512725830078,\n",
       " 0.12444663047790527,\n",
       " 0.13853740692138672,\n",
       " 0.14239811897277832,\n",
       " 0.14174103736877441,\n",
       " 0.14020013809204102,\n",
       " 0.2006826400756836,\n",
       " 0.14292335510253906,\n",
       " 0.14998984336853027,\n",
       " 0.13971543312072754,\n",
       " 0.14864110946655273,\n",
       " 0.13617968559265137,\n",
       " 0.149977445602417,\n",
       " 0.13677597045898438,\n",
       " 0.14380526542663574,\n",
       " 0.14829087257385254,\n",
       " 0.20360279083251953,\n",
       " 0.14495372772216797,\n",
       " 0.1479339599609375,\n",
       " 0.14316558837890625,\n",
       " 0.1484973430633545,\n",
       " 0.14455366134643555,\n",
       " 0.14990901947021484,\n",
       " 0.1510298252105713,\n",
       " 0.148040771484375,\n",
       " 0.14684772491455078,\n",
       " 0.20139312744140625,\n",
       " 0.14557099342346191,\n",
       " 0.1496264934539795,\n",
       " 0.14600467681884766,\n",
       " 0.14110779762268066,\n",
       " 0.15066862106323242,\n",
       " 0.1429431438446045,\n",
       " 0.14523816108703613,\n",
       " 0.14439773559570312,\n",
       " 0.14445877075195312,\n",
       " 0.20244240760803223,\n",
       " 0.14615845680236816,\n",
       " 0.14536833763122559,\n",
       " 0.14378023147583008,\n",
       " 0.18847084045410156,\n",
       " 0.1435854434967041,\n",
       " 0.13408923149108887,\n",
       " 0.14010214805603027,\n",
       " 0.14461421966552734,\n",
       " 0.13532543182373047,\n",
       " 0.20049786567687988,\n",
       " 0.1433396339416504,\n",
       " 0.14646434783935547,\n",
       " 0.1421952247619629,\n",
       " 0.14671850204467773,\n",
       " 0.1420431137084961,\n",
       " 0.14083147048950195,\n",
       " 0.14219093322753906,\n",
       " 0.1452319622039795,\n",
       " 0.14455389976501465,\n",
       " 0.20417189598083496,\n",
       " 0.14422202110290527,\n",
       " 0.14219188690185547,\n",
       " 0.1451876163482666,\n",
       " 0.14583492279052734,\n",
       " 0.1478879451751709,\n",
       " 0.1319718360900879,\n",
       " 0.1298828125,\n",
       " 0.12665104866027832,\n",
       " 0.13114285469055176,\n",
       " 0.2024064064025879,\n",
       " 0.1334080696105957,\n",
       " 0.12706565856933594,\n",
       " 0.13207554817199707,\n",
       " 0.13097286224365234,\n",
       " 0.12812232971191406,\n",
       " 0.12932801246643066,\n",
       " 0.13187575340270996,\n",
       " 0.13041377067565918,\n",
       " 0.12995624542236328,\n",
       " 0.19485187530517578,\n",
       " 0.12957024574279785,\n",
       " 0.12966394424438477,\n",
       " 0.1280653476715088,\n",
       " 0.13150882720947266,\n",
       " 0.12711787223815918,\n",
       " 0.1256728172302246,\n",
       " 0.12952423095703125,\n",
       " 0.12906503677368164,\n",
       " 0.13178730010986328,\n",
       " 0.19459271430969238,\n",
       " 0.12001633644104004,\n",
       " 0.12906980514526367,\n",
       " 0.13058781623840332,\n",
       " 0.1280384063720703,\n",
       " 0.1311359405517578,\n",
       " 0.13184857368469238,\n",
       " 0.1294858455657959,\n",
       " 0.1324765682220459,\n",
       " 0.1293637752532959,\n",
       " 0.19247770309448242,\n",
       " 0.13024067878723145,\n",
       " 0.13043737411499023,\n",
       " 0.13093233108520508,\n",
       " 0.1313612461090088,\n",
       " 0.1897740364074707,\n",
       " 0.12133359909057617,\n",
       " 0.13621878623962402,\n",
       " 0.13880062103271484,\n",
       " 0.13218998908996582,\n",
       " 0.190901517868042,\n",
       " 0.13528800010681152,\n",
       " 0.1383047103881836,\n",
       " 0.13620829582214355,\n",
       " 0.13521313667297363,\n",
       " 0.13941001892089844,\n",
       " 0.1436910629272461,\n",
       " 0.14015936851501465,\n",
       " 0.1414051055908203,\n",
       " 0.14001989364624023,\n",
       " 0.19403719902038574,\n",
       " 0.14162921905517578,\n",
       " 0.12883782386779785,\n",
       " 0.12906503677368164,\n",
       " 0.12750887870788574,\n",
       " 0.11835598945617676,\n",
       " 0.12250375747680664,\n",
       " 0.12195825576782227,\n",
       " 0.12538504600524902,\n",
       " 0.12372612953186035,\n",
       " 0.19184327125549316,\n",
       " 0.1210176944732666,\n",
       " 0.12280535697937012,\n",
       " 0.12158679962158203,\n",
       " 0.12247943878173828,\n",
       " 0.11878514289855957,\n",
       " 0.11729693412780762,\n",
       " 0.12392091751098633,\n",
       " 0.11906170845031738,\n",
       " 0.1210336685180664,\n",
       " 0.18976616859436035,\n",
       " 0.12137007713317871,\n",
       " 0.13180255889892578,\n",
       " 0.12037134170532227,\n",
       " 0.11889863014221191,\n",
       " 0.11515307426452637,\n",
       " 0.12493085861206055,\n",
       " 0.12184381484985352,\n",
       " 0.11800479888916016,\n",
       " 0.11747336387634277,\n",
       " 0.19149374961853027,\n",
       " 0.12011051177978516,\n",
       " 0.12995171546936035,\n",
       " 0.1194465160369873,\n",
       " 0.13550424575805664,\n",
       " 0.12662386894226074,\n",
       " 0.13158750534057617,\n",
       " 0.1326158046722412,\n",
       " 0.1311337947845459,\n",
       " 0.11755800247192383,\n",
       " 0.19197916984558105,\n",
       " 0.12063288688659668,\n",
       " 0.11967897415161133,\n",
       " 0.11870408058166504,\n",
       " 0.13463473320007324,\n",
       " 0.13258099555969238,\n",
       " 0.19744157791137695,\n",
       " 0.13646674156188965,\n",
       " 0.1386868953704834,\n",
       " 0.13277912139892578,\n",
       " 0.19470572471618652,\n",
       " 0.13494038581848145,\n",
       " 0.1292872428894043,\n",
       " 0.12587285041809082,\n",
       " 0.13085055351257324,\n",
       " 0.1437394618988037,\n",
       " 0.1398007869720459,\n",
       " 0.13625359535217285,\n",
       " 0.14273619651794434,\n",
       " 0.1420753002166748,\n",
       " 0.19152164459228516,\n",
       " 0.1438918113708496,\n",
       " 0.13957953453063965,\n",
       " 0.14067554473876953,\n",
       " 0.1401810646057129,\n",
       " 0.18092036247253418,\n",
       " 0.20543980598449707,\n",
       " 0.19949889183044434,\n",
       " 0.13892483711242676,\n",
       " 0.13962125778198242,\n",
       " 0.19038081169128418,\n",
       " 0.1417844295501709,\n",
       " 0.1330561637878418,\n",
       " 0.13378667831420898,\n",
       " 0.13425302505493164,\n",
       " 0.1318073272705078,\n",
       " 0.1319413185119629,\n",
       " 0.13266730308532715,\n",
       " 0.13746023178100586,\n",
       " 0.13155341148376465,\n",
       " 0.18899917602539062,\n",
       " 0.1380906105041504,\n",
       " 0.13413524627685547,\n",
       " 0.13341283798217773,\n",
       " 0.13841962814331055,\n",
       " 0.13823866844177246,\n",
       " 0.13444113731384277,\n",
       " 0.13520288467407227,\n",
       " 0.13525795936584473,\n",
       " 0.14022135734558105,\n",
       " 0.19223904609680176,\n",
       " 0.1349339485168457,\n",
       " 0.13482975959777832,\n",
       " 0.14018607139587402,\n",
       " 0.1377394199371338,\n",
       " 0.13406896591186523,\n",
       " 0.13478803634643555,\n",
       " 0.1364588737487793,\n",
       " 0.13677501678466797,\n",
       " 0.1356792449951172,\n",
       " 0.18207573890686035,\n",
       " 0.1335923671722412,\n",
       " 0.13908863067626953,\n",
       " 0.13953089714050293,\n",
       " 0.1395125389099121,\n",
       " 0.13300299644470215,\n",
       " 0.13886523246765137,\n",
       " 0.18923306465148926,\n",
       " 0.1383810043334961,\n",
       " 0.13254857063293457,\n",
       " 0.17981958389282227,\n",
       " 0.13347530364990234,\n",
       " 0.13469600677490234,\n",
       " 0.13707637786865234,\n",
       " 0.13858485221862793,\n",
       " 0.13971543312072754,\n",
       " 0.13330340385437012,\n",
       " 0.14046359062194824,\n",
       " 0.13966703414916992,\n",
       " 0.13942646980285645,\n",
       " 0.18185186386108398,\n",
       " 0.13506174087524414,\n",
       " 0.13754820823669434,\n",
       " 0.13976621627807617,\n",
       " 0.1388871669769287,\n",
       " 0.14120221138000488,\n",
       " 0.13148736953735352,\n",
       " 0.13985133171081543,\n",
       " 0.13904356956481934,\n",
       " 0.1340954303741455,\n",
       " 0.1867210865020752,\n",
       " 0.1286945343017578,\n",
       " 0.13979601860046387,\n",
       " 0.13793349266052246,\n",
       " 0.1215674877166748,\n",
       " 0.12842774391174316,\n",
       " 0.12745261192321777,\n",
       " 0.13446450233459473,\n",
       " 0.12633776664733887,\n",
       " 0.1289050579071045,\n",
       " 0.18094515800476074,\n",
       " 0.1254866123199463,\n",
       " 0.13383960723876953,\n",
       " 0.12442159652709961,\n",
       " 0.12490558624267578,\n",
       " 0.12967419624328613,\n",
       " 0.13478803634643555,\n",
       " 0.12454032897949219,\n",
       " 0.13705658912658691,\n",
       " 0.12261581420898438,\n",
       " 0.1815023422241211,\n",
       " 0.1272437572479248,\n",
       " 0.11912941932678223,\n",
       " 0.12349438667297363,\n",
       " 0.1278378963470459,\n",
       " 0.13283276557922363,\n",
       " 0.12104296684265137,\n",
       " 0.13003969192504883,\n",
       " 0.12921571731567383,\n",
       " 0.6058883666992188,\n",
       " 0.17995500564575195,\n",
       " 0.13103461265563965,\n",
       " 0.1250143051147461,\n",
       " 0.13048195838928223,\n",
       " 0.13019704818725586,\n",
       " 0.13636398315429688,\n",
       " 0.1364281177520752,\n",
       " 0.13036036491394043,\n",
       " 0.17999649047851562,\n",
       " 0.13253211975097656,\n",
       " 0.17973923683166504,\n",
       " 0.14344573020935059,\n",
       " 0.14580035209655762,\n",
       " 0.14601588249206543,\n",
       " 0.14789438247680664,\n",
       " 0.14328360557556152,\n",
       " 0.14340853691101074,\n",
       " 0.1476285457611084,\n",
       " 0.1448345184326172,\n",
       " 0.1457052230834961,\n",
       " 0.17920398712158203,\n",
       " 0.1423816680908203,\n",
       " 0.14908885955810547,\n",
       " 0.1418910026550293,\n",
       " 0.14190459251403809,\n",
       " 0.14038610458374023,\n",
       " 0.14440083503723145,\n",
       " 0.1435098648071289,\n",
       " 0.14851784706115723,\n",
       " 0.14412832260131836]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_per_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['server0', 'server1', 'server2', 'server3']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'server4' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a58e50e12838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mserver_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"server4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: 'server4' is not in list"
     ]
    }
   ],
   "source": [
    "server_ids.index(\"server4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher.get_key_to_node_map()['fc_bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel=LinearNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(current_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([184, 198, 191, 212])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

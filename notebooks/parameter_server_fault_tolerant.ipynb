{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from filelock import FileLock\n",
    "import numpy as np\n",
    "import ray\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from consistent_hashing import ConsistentHash\n",
    "import math \n",
    "from time import time\n",
    "def get_data_loader():\n",
    "    \n",
    "    \"\"\"Safely downloads data. Returns training/validation set dataloader.\"\"\"\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    \n",
    "    class MNISTEvenOddDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, ready_data):\n",
    "            self.img_data = ready_data.data\n",
    "            self.labels = ready_data.targets % 2\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "        def __getitem__(self, ind):\n",
    "            return torch.true_divide(self.img_data[ind].view(-1, 28 * 28).squeeze(), 255), torch.tensor([self.labels[ind]])\n",
    "\n",
    "\n",
    "    \n",
    "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
    "        \n",
    "        train_dataset = datasets.MNIST(\n",
    "                \"~/data\", train=True, download=True, transform=mnist_transforms\n",
    "            )\n",
    "        \n",
    "        test_dataset = datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            MNISTEvenOddDataset(train_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "             MNISTEvenOddDataset(test_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Evaluates the accuracy of the model on a validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # This is only set to finish evaluation faster.\n",
    "            if batch_idx * len(data) > 1024:\n",
    "                break\n",
    "            outputs = nn.Sigmoid()(model(data))\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = outputs > 0.5\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    \"\"\"Small Linear Network for MNIST.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc_weights = nn.ParameterList([nn.Parameter(torch.empty(1)) for weight in range(784)])\n",
    "        init_fc = [nn.init.uniform_(x) for x in self.fc_weights]\n",
    "        \n",
    "        self.fc_bias = nn.Parameter(torch.empty(1))\n",
    "        nn.init.uniform_(self.fc_bias)\n",
    "        \n",
    "    #def __init__(self):\n",
    "    #    super(LinearNet, self).__init__()\n",
    "    #    self.fc = nn.Linear(28*28, 1)\n",
    "    #    nn.init.normal(self.fc.weight)\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    x = self.fc(x)\n",
    "    #    return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #fc_layer = torch.cat(tuple(self.fc_weights)).unsqueeze(0)\n",
    "        #x = x @ fc_layer.T + self.fc_bias\n",
    "        for i, param in enumerate(self.fc_weights):\n",
    "            if i==0:\n",
    "                p=x[:,i]*param\n",
    "            else:\n",
    "                p += x[:,i]*param\n",
    "        x = p.unsqueeze(1) + self.fc_bias\n",
    "        return x\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "\n",
    "    def set_weights(self, keys, weights): \n",
    "        flatten_weights =  [item for sublist in weights for item in sublist]\n",
    "        self.load_state_dict({keys[i]:flatten_weights[i] for i in range(len(keys))})\n",
    "        \n",
    "    def get_gradients(self, keys):\n",
    "        grads = {}\n",
    "\n",
    "        for name, p in self.named_parameters():\n",
    "            if name in keys:\n",
    "                grad = None if p.grad is None else p.grad.data.cpu().numpy()\n",
    "                grads[name] = grad\n",
    "\n",
    "        return [grads[key] for key in keys]\n",
    "\n",
    "    def set_gradients(self, gradients):\n",
    "        for g, p in zip(gradients, self.parameters()):\n",
    "            if g is not None:\n",
    "                p.grad = torch.from_numpy(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote  \n",
    "class ParameterServer(object):\n",
    "    def __init__(self, keys, values):\n",
    "        self.weights = dict(zip(keys, values))\n",
    "\n",
    "    def apply_gradients(self, keys, lr, *values):\n",
    "        summed_gradients = [\n",
    "            np.stack(gradient_zip).sum(axis=0) for gradient_zip in zip(*values)\n",
    "        ]\n",
    "    \n",
    "        idx = 0\n",
    "        for key, value in zip(keys, summed_gradients):\n",
    "            self.weights[key] -= lr * torch.from_numpy(summed_gradients[idx])\n",
    "            idx+=1\n",
    "\n",
    "        return [self.weights[key] for key in keys]\n",
    "    \n",
    "    def add_weight(self, key, value):\n",
    "        self.weights[key] = value\n",
    "    \n",
    "    def get_weights(self, keys):\n",
    "        return [self.weights[key] for key in keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class DataWorker(object):\n",
    "    def __init__(self, keys):\n",
    "        self.model = LinearNet()\n",
    "        self.data_iterator = iter(get_data_loader()[0])\n",
    "        self.keys = keys\n",
    "        self.key_set = set(self.keys)\n",
    "        for key, value in dict(self.model.named_parameters()).items():\n",
    "            if key not in self.key_set:\n",
    "                value.requires_grad=False\n",
    "\n",
    "        \n",
    "    def update_weights(self, keys, *weights):\n",
    "        self.model.set_weights(keys, weights)\n",
    "        \n",
    "#     def update_weights_selected(self, keys, *weights):\n",
    "#         curr_state_dict = dict(self.model.state_dict().items())\n",
    "#         flatten_weights =  [item for sublist in weights for item in sublist]\n",
    "#         for i, key in enumerate(keys):\n",
    "#             curr_state_dict[keys] = flatten_weights[i]\n",
    "#         self.model.load_state_dict(curr_state_dict)\n",
    "        \n",
    "    def update_trainable(self, keys):\n",
    "        self.keys = keys\n",
    "        self.key_set = set(self.keys)\n",
    "        for key, value in dict(self.model.named_parameters()).items():\n",
    "            if key in self.key_set:\n",
    "                value.requires_grad = True\n",
    "            else:\n",
    "                value.requires_grad = False\n",
    "       \n",
    "\n",
    "    def compute_gradients(self):\n",
    "        #self.model.set_weights(keys, weights)\n",
    "        try:\n",
    "            data, target = next(self.data_iterator)\n",
    "        except StopIteration:  # When the epoch ends, start a new epoch.\n",
    "            self.data_iterator = iter(get_data_loader()[0])\n",
    "            data, target = next(self.data_iterator)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        output = self.model(data)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, target.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        return self.model.get_gradients(self.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 14:20:33,890\tINFO worker.py:963 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "iterations = 500\n",
    "num_workers = 1 # number of workers per server\n",
    "num_servers = 5 # number of servers\n",
    "hashes_per_server = 100\n",
    "\n",
    "def Scheduler(num_servers, hashes_per_server=50):\n",
    "    \n",
    "    model = LinearNet()\n",
    "    key_values = model.get_weights()\n",
    "    keys = np.array(list(key_values.keys()))\n",
    "    #print(keys)\n",
    "    #print(key_values) z\n",
    "    values = [key_values[key] for key in keys]\n",
    "    #values = [key_values[key] for key in keys]\n",
    "    \n",
    "    key_indices = {key: x for x, key in enumerate(keys)}\n",
    "   \n",
    "    # distributing weights across servers - do this using consistency hashing\n",
    "    server_ids = [\"server\" + str(ind) for ind in range(num_servers)]\n",
    "    hasher = ConsistentHash(keys, server_ids, hashes_per_server)\n",
    "    servers = [ParameterServer.remote(keys[[key_indices[key] for key in hasher.get_keys_per_node()[serv]]], \n",
    "                                      [values[key_indices[key]] for key in hasher.get_keys_per_node()[serv]]) for serv in server_ids]\n",
    "    # servers = [ParameterServer.remote(keys[0:1], values[0:1]), ParameterServer.remote(keys[1:2], values[1:2])]\n",
    "    \n",
    "    return hasher, servers, keys, model, hasher.get_keys_per_node(), server_ids.copy()\n",
    "\n",
    "hasher, servers, keys, model, weight_assignments, server_ids =  Scheduler(num_servers, hashes_per_server)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# creating equal workers per server\n",
    "\n",
    "workers = [[DataWorker.remote(weight_assignments[\"server\" + str(j)]) for i in range(num_workers)] for j in range(num_servers)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight_assignments[\"server0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_data_loader()[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running synchronous parameter server training.\n",
      "Iter 0: \taccuracy is 51.9\n",
      "Iter 5: \taccuracy is 51.9\n",
      "Iter 10: \taccuracy is 51.9\n",
      "Iter 15: \taccuracy is 51.9\n",
      "Iter 20: \taccuracy is 51.9\n",
      "Iter 25: \taccuracy is 54.0\n",
      "Iter 30: \taccuracy is 49.9\n",
      "Iter 35: \taccuracy is 47.7\n",
      "Iter 40: \taccuracy is 47.9\n",
      "Iter 45: \taccuracy is 49.0\n",
      "Iter 50: \taccuracy is 56.0\n",
      "Iter 55: \taccuracy is 71.4\n",
      "Iter 60: \taccuracy is 69.8\n",
      "Iter 65: \taccuracy is 52.9\n",
      "Iter 70: \taccuracy is 48.4\n",
      "Iter 75: \taccuracy is 48.1\n",
      "Iter 80: \taccuracy is 48.1\n",
      "Iter 85: \taccuracy is 48.2\n",
      "Iter 90: \taccuracy is 50.0\n",
      "Iter 95: \taccuracy is 62.5\n",
      "Iter 100: \taccuracy is 79.7\n",
      "Iter 105: \taccuracy is 78.9\n",
      "Iter 110: \taccuracy is 79.0\n",
      "Iter 115: \taccuracy is 76.9\n",
      "Iter 120: \taccuracy is 77.3\n",
      "Iter 125: \taccuracy is 80.7\n",
      "Iter 130: \taccuracy is 80.6\n",
      "Iter 135: \taccuracy is 70.6\n",
      "Iter 140: \taccuracy is 65.6\n",
      "Iter 145: \taccuracy is 82.1\n",
      "Iter 150: \taccuracy is 78.6\n",
      "Iter 155: \taccuracy is 68.4\n",
      "Iter 160: \taccuracy is 62.2\n",
      "Iter 165: \taccuracy is 77.3\n",
      "Iter 170: \taccuracy is 82.6\n",
      "Iter 175: \taccuracy is 72.1\n",
      "Iter 180: \taccuracy is 59.6\n",
      "Iter 185: \taccuracy is 78.7\n",
      "Iter 190: \taccuracy is 82.8\n",
      "Iter 195: \taccuracy is 78.5\n",
      "Iter 200: \taccuracy is 71.6\n",
      "Iter 205: \taccuracy is 79.8\n",
      "Iter 210: \taccuracy is 82.8\n",
      "Iter 215: \taccuracy is 82.3\n",
      "Iter 220: \taccuracy is 77.3\n",
      "Iter 225: \taccuracy is 83.0\n",
      "Iter 230: \taccuracy is 83.2\n",
      "Iter 235: \taccuracy is 80.2\n",
      "Iter 240: \taccuracy is 78.8\n",
      "Iter 245: \taccuracy is 82.0\n",
      "Iter 250: \taccuracy is 83.8\n",
      "Iter 255: \taccuracy is 83.5\n",
      "Iter 260: \taccuracy is 82.4\n",
      "Iter 265: \taccuracy is 84.1\n",
      "Iter 270: \taccuracy is 83.8\n",
      "Iter 275: \taccuracy is 82.5\n",
      "Iter 280: \taccuracy is 80.6\n",
      "Iter 285: \taccuracy is 82.7\n",
      "Iter 290: \taccuracy is 84.5\n",
      "Iter 295: \taccuracy is 84.3\n",
      "Iter 300: \taccuracy is 83.3\n",
      "Iter 305: \taccuracy is 84.9\n",
      "Iter 310: \taccuracy is 84.4\n",
      "Iter 315: \taccuracy is 83.4\n",
      "Iter 320: \taccuracy is 81.8\n",
      "Iter 325: \taccuracy is 83.2\n",
      "Iter 330: \taccuracy is 84.9\n",
      "Iter 335: \taccuracy is 85.1\n",
      "Iter 340: \taccuracy is 84.4\n",
      "Iter 345: \taccuracy is 85.5\n",
      "Iter 350: \taccuracy is 85.2\n",
      "Iter 355: \taccuracy is 83.5\n",
      "Iter 360: \taccuracy is 82.9\n",
      "Iter 365: \taccuracy is 84.8\n",
      "Iter 370: \taccuracy is 85.8\n",
      "Iter 375: \taccuracy is 85.9\n",
      "Iter 380: \taccuracy is 84.6\n",
      "Iter 385: \taccuracy is 86.0\n",
      "Iter 390: \taccuracy is 85.7\n",
      "Iter 395: \taccuracy is 84.0\n",
      "Iter 400: \taccuracy is 82.9\n",
      "Iter 405: \taccuracy is 85.8\n",
      "Iter 410: \taccuracy is 86.3\n",
      "Iter 415: \taccuracy is 86.0\n",
      "Iter 420: \taccuracy is 84.6\n",
      "Iter 425: \taccuracy is 86.5\n",
      "Iter 430: \taccuracy is 86.0\n",
      "Iter 435: \taccuracy is 83.9\n",
      "Iter 440: \taccuracy is 83.1\n",
      "Iter 445: \taccuracy is 85.9\n",
      "Iter 450: \taccuracy is 86.5\n",
      "Iter 455: \taccuracy is 85.4\n",
      "Iter 460: \taccuracy is 83.7\n",
      "Iter 465: \taccuracy is 86.3\n",
      "Iter 470: \taccuracy is 86.9\n",
      "Iter 475: \taccuracy is 83.9\n",
      "Iter 480: \taccuracy is 83.0\n",
      "Iter 485: \taccuracy is 85.8\n",
      "Iter 490: \taccuracy is 87.1\n",
      "Iter 495: \taccuracy is 85.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Running synchronous parameter server training.\")\n",
    "lr=0.1\n",
    "failure_iter=60\n",
    "failure_server=\"server4\"\n",
    "\n",
    "# we need to get a new keys order because we are not assuming a ordering in keys\n",
    "current_weights = []\n",
    "keys_order = []\n",
    "\n",
    "for j in range(num_servers):\n",
    "    keys_order.extend(weight_assignments[\"server\" + str(j)])\n",
    "    current_weights.extend(ray.get(servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]))) \n",
    "curr_weights_ckpt = current_weights.copy()\n",
    "\n",
    "time_per_iteration = []\n",
    "for i in range(iterations):\n",
    " \n",
    "    #start = time()\n",
    "    \n",
    "    if i == failure_iter:\n",
    "        #Define parameters that will need to be moved\n",
    "        failure_params = weight_assignments[failure_server]\n",
    "        #Delete server from hash ring and reassign params\n",
    "        hasher.delete_node_and_reassign_to_others(failure_server)\n",
    "        weight_assignments = hasher.get_keys_per_node()\n",
    "        #Update servers and workers\n",
    "        num_servers -= 1\n",
    "        server_ind = server_ids.index(failure_server)\n",
    "        server_ids = server_ids[0 : server_ind] + server_ids[server_ind + 1 : ]\n",
    "        servers = servers[0 : server_ind] + servers[server_ind + 1 : ]\n",
    "        workers = workers[0 : server_ind] + workers[server_ind + 1 : ]\n",
    "        #Add each relevant parameter to its new server\n",
    "        server_dict = {server_ids[x]:servers[x] for x in range(len(server_ids))}\n",
    "        for ind, param in enumerate(failure_params):\n",
    "            server_dict[hasher.get_key_to_node_map()[param]].add_weight.remote(param, curr_weights_ckpt[server_ind][ind])\n",
    "        #Update these parameters for each worker to make them trainable\n",
    "        [workers[j][idx].update_trainable.remote(weight_assignments[\"server\" + str(j)]) for  idx  in range(num_workers) for j in range(num_servers)]\n",
    "        keys_order = []\n",
    "        for j in range(num_servers):\n",
    "            keys_order.extend(weight_assignments[\"server\" + str(j)])\n",
    "        [workers[j][idx].update_weights.remote(keys_order, *current_weights) for  idx  in range(num_workers) for j in range(num_servers)]\n",
    "        \n",
    "    \n",
    "    # sync all weights on workers\n",
    "    if i % 20 == 0:\n",
    "        curr_weights_ckpt = current_weights.copy()\n",
    "        # get weights from server\n",
    "        #current_weights = [servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]) for j in range(num_servers)] \n",
    "\n",
    "        # update weights on all workers\n",
    "        [workers[j][idx].update_weights.remote(keys_order, *current_weights) for  idx  in range(num_workers) for j in range(num_servers)]\n",
    "    \n",
    "        \n",
    "    # use local cache of weights and get gradients from workers\n",
    "    gradients = [[workers[j][idx].compute_gradients.remote() for  idx  in range(num_workers)] for j in range(num_servers)]\n",
    "\n",
    "    start = time()\n",
    "    # Updates gradients to specfic parameter servers\n",
    "    current_weights_t = [servers[j].apply_gradients.remote(weight_assignments[\"server\" + str(j)], lr, *gradients[j]) for j in range(num_servers)]\n",
    "    current_weights = ray.get(current_weights_t)\n",
    "    \n",
    "    end = time()\n",
    "    time_per_iteration.append(end-start)\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        # Evaluate the current model.\n",
    "        # current_weights = [servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]) for j in range(num_servers)] \n",
    "      \n",
    "        # we are once again using the server to key mapping to set the weight back\n",
    "        model.set_weights(keys_order, current_weights)\n",
    "        accuracy = evaluate(model, test_loader)\n",
    "        print(\"Iter {}: \\taccuracy is {:.1f}\".format(i, accuracy))\n",
    "    #rint(\"\\n\")\n",
    "\n",
    "#print(\"Final accuracy is {:.1f}.\".format(accuracy))\n",
    "# Clean up Ray resources and processes before the next example.\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1470847402163641"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(time_per_iteration[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02577144327194029"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(time_per_iteration[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.588714361190796,\n",
       " 0.13730883598327637,\n",
       " 0.13313031196594238,\n",
       " 0.1397244930267334,\n",
       " 0.13124918937683105,\n",
       " 0.1424236297607422,\n",
       " 0.13758516311645508,\n",
       " 0.13960599899291992,\n",
       " 0.13950729370117188,\n",
       " 0.1336812973022461,\n",
       " 0.19435453414916992,\n",
       " 0.13529610633850098,\n",
       " 0.14109039306640625,\n",
       " 0.13781523704528809,\n",
       " 0.1302502155303955,\n",
       " 0.13529276847839355,\n",
       " 0.13292670249938965,\n",
       " 0.13463830947875977,\n",
       " 0.14171147346496582,\n",
       " 0.14144682884216309,\n",
       " 0.19423699378967285,\n",
       " 0.12644362449645996,\n",
       " 0.1280200481414795,\n",
       " 0.12221574783325195,\n",
       " 0.12069988250732422,\n",
       " 0.12054061889648438,\n",
       " 0.12222146987915039,\n",
       " 0.18278765678405762,\n",
       " 0.19629263877868652,\n",
       " 0.12352895736694336,\n",
       " 0.19074463844299316,\n",
       " 0.12006068229675293,\n",
       " 0.12671160697937012,\n",
       " 0.12433290481567383,\n",
       " 0.1338052749633789,\n",
       " 0.1232297420501709,\n",
       " 0.12189030647277832,\n",
       " 0.12937450408935547,\n",
       " 0.1230320930480957,\n",
       " 0.12760472297668457,\n",
       " 0.1854264736175537,\n",
       " 0.11983871459960938,\n",
       " 0.12497305870056152,\n",
       " 0.12382173538208008,\n",
       " 0.1202993392944336,\n",
       " 0.1263740062713623,\n",
       " 0.12149739265441895,\n",
       " 0.12420082092285156,\n",
       " 0.12353873252868652,\n",
       " 0.12755489349365234,\n",
       " 0.1820816993713379,\n",
       " 0.21605634689331055,\n",
       " 0.14264750480651855,\n",
       " 0.14026618003845215,\n",
       " 0.1417255401611328,\n",
       " 0.14444398880004883,\n",
       " 0.1253950595855713,\n",
       " 0.1298661231994629,\n",
       " 0.12682151794433594,\n",
       " 0.12833857536315918,\n",
       " 0.16559243202209473,\n",
       " 0.12835311889648438,\n",
       " 0.1300966739654541,\n",
       " 0.13056087493896484,\n",
       " 0.12795424461364746,\n",
       " 0.12892556190490723,\n",
       " 0.14424395561218262,\n",
       " 0.12783074378967285,\n",
       " 0.12282490730285645,\n",
       " 0.12252283096313477,\n",
       " 0.1841893196105957,\n",
       " 0.14269757270812988,\n",
       " 0.13878941535949707,\n",
       " 0.12727856636047363,\n",
       " 0.12466883659362793,\n",
       " 0.12364339828491211,\n",
       " 0.12157130241394043,\n",
       " 0.12425351142883301,\n",
       " 0.12210750579833984,\n",
       " 0.12828779220581055,\n",
       " 0.17039275169372559,\n",
       " 0.12045884132385254,\n",
       " 0.12500882148742676,\n",
       " 0.12307024002075195,\n",
       " 0.1196908950805664,\n",
       " 0.1222832202911377,\n",
       " 0.14913153648376465,\n",
       " 0.13460469245910645,\n",
       " 0.14117145538330078,\n",
       " 0.13710260391235352,\n",
       " 0.17919468879699707,\n",
       " 0.13693761825561523,\n",
       " 0.13512110710144043,\n",
       " 0.13065028190612793,\n",
       " 0.13193869590759277,\n",
       " 0.13787484169006348,\n",
       " 0.13448572158813477,\n",
       " 0.1340956687927246,\n",
       " 0.13578414916992188,\n",
       " 0.13019084930419922,\n",
       " 0.1794435977935791,\n",
       " 0.12315773963928223,\n",
       " 0.1231389045715332,\n",
       " 0.1212165355682373,\n",
       " 0.1252918243408203,\n",
       " 0.12147641181945801,\n",
       " 0.12157201766967773,\n",
       " 0.12213253974914551,\n",
       " 0.1202399730682373,\n",
       " 0.13007736206054688,\n",
       " 0.18225550651550293,\n",
       " 0.12760090827941895,\n",
       " 0.19522309303283691,\n",
       " 0.11833643913269043,\n",
       " 0.12111568450927734,\n",
       " 0.11885571479797363,\n",
       " 0.12067461013793945,\n",
       " 0.12244105339050293,\n",
       " 0.12076330184936523,\n",
       " 0.12005472183227539,\n",
       " 0.18515920639038086,\n",
       " 0.12366986274719238,\n",
       " 0.15457391738891602,\n",
       " 0.16038894653320312,\n",
       " 0.15955090522766113,\n",
       " 0.1561133861541748,\n",
       " 0.1503736972808838,\n",
       " 0.16120076179504395,\n",
       " 0.15075325965881348,\n",
       " 0.15771770477294922,\n",
       " 0.21598196029663086,\n",
       " 0.15779471397399902,\n",
       " 0.15866422653198242,\n",
       " 0.16682839393615723,\n",
       " 0.1515355110168457,\n",
       " 0.15619421005249023,\n",
       " 0.15932250022888184,\n",
       " 0.16324138641357422,\n",
       " 0.16686248779296875,\n",
       " 0.16022872924804688,\n",
       " 0.21947574615478516,\n",
       " 0.16245388984680176,\n",
       " 0.1614837646484375,\n",
       " 0.15810441970825195,\n",
       " 0.15517234802246094,\n",
       " 0.16928768157958984,\n",
       " 0.16161417961120605,\n",
       " 0.14965271949768066,\n",
       " 0.1609635353088379,\n",
       " 0.16119647026062012,\n",
       " 0.21338725090026855,\n",
       " 0.1564497947692871,\n",
       " 0.15535902976989746,\n",
       " 0.1545271873474121,\n",
       " 0.15725326538085938,\n",
       " 0.15796542167663574,\n",
       " 0.16361761093139648,\n",
       " 0.15983223915100098,\n",
       " 0.15895509719848633,\n",
       " 0.15792346000671387,\n",
       " 0.18034625053405762,\n",
       " 0.13035082817077637,\n",
       " 0.13068294525146484,\n",
       " 0.1308279037475586,\n",
       " 0.12936925888061523,\n",
       " 0.13068056106567383,\n",
       " 0.13065576553344727,\n",
       " 0.1311483383178711,\n",
       " 0.13117170333862305,\n",
       " 0.1307237148284912,\n",
       " 0.1846904754638672,\n",
       " 0.1312258243560791,\n",
       " 0.12980985641479492,\n",
       " 0.1765282154083252,\n",
       " 0.14496064186096191,\n",
       " 0.14835309982299805,\n",
       " 0.14578461647033691,\n",
       " 0.14438462257385254,\n",
       " 0.13753080368041992,\n",
       " 0.1454150676727295,\n",
       " 0.20393085479736328,\n",
       " 0.15209293365478516,\n",
       " 0.14257335662841797,\n",
       " 0.1346902847290039,\n",
       " 0.14958977699279785,\n",
       " 0.16284584999084473,\n",
       " 0.1621861457824707,\n",
       " 0.15857195854187012,\n",
       " 0.15723061561584473,\n",
       " 0.16019105911254883,\n",
       " 0.21585798263549805,\n",
       " 0.15961360931396484,\n",
       " 0.16126513481140137,\n",
       " 0.15205740928649902,\n",
       " 0.16143512725830078,\n",
       " 0.12444663047790527,\n",
       " 0.13853740692138672,\n",
       " 0.14239811897277832,\n",
       " 0.14174103736877441,\n",
       " 0.14020013809204102,\n",
       " 0.2006826400756836,\n",
       " 0.14292335510253906,\n",
       " 0.14998984336853027,\n",
       " 0.13971543312072754,\n",
       " 0.14864110946655273,\n",
       " 0.13617968559265137,\n",
       " 0.149977445602417,\n",
       " 0.13677597045898438,\n",
       " 0.14380526542663574,\n",
       " 0.14829087257385254,\n",
       " 0.20360279083251953,\n",
       " 0.14495372772216797,\n",
       " 0.1479339599609375,\n",
       " 0.14316558837890625,\n",
       " 0.1484973430633545,\n",
       " 0.14455366134643555,\n",
       " 0.14990901947021484,\n",
       " 0.1510298252105713,\n",
       " 0.148040771484375,\n",
       " 0.14684772491455078,\n",
       " 0.20139312744140625,\n",
       " 0.14557099342346191,\n",
       " 0.1496264934539795,\n",
       " 0.14600467681884766,\n",
       " 0.14110779762268066,\n",
       " 0.15066862106323242,\n",
       " 0.1429431438446045,\n",
       " 0.14523816108703613,\n",
       " 0.14439773559570312,\n",
       " 0.14445877075195312,\n",
       " 0.20244240760803223,\n",
       " 0.14615845680236816,\n",
       " 0.14536833763122559,\n",
       " 0.14378023147583008,\n",
       " 0.18847084045410156,\n",
       " 0.1435854434967041,\n",
       " 0.13408923149108887,\n",
       " 0.14010214805603027,\n",
       " 0.14461421966552734,\n",
       " 0.13532543182373047,\n",
       " 0.20049786567687988,\n",
       " 0.1433396339416504,\n",
       " 0.14646434783935547,\n",
       " 0.1421952247619629,\n",
       " 0.14671850204467773,\n",
       " 0.1420431137084961,\n",
       " 0.14083147048950195,\n",
       " 0.14219093322753906,\n",
       " 0.1452319622039795,\n",
       " 0.14455389976501465,\n",
       " 0.20417189598083496,\n",
       " 0.14422202110290527,\n",
       " 0.14219188690185547,\n",
       " 0.1451876163482666,\n",
       " 0.14583492279052734,\n",
       " 0.1478879451751709,\n",
       " 0.1319718360900879,\n",
       " 0.1298828125,\n",
       " 0.12665104866027832,\n",
       " 0.13114285469055176,\n",
       " 0.2024064064025879,\n",
       " 0.1334080696105957,\n",
       " 0.12706565856933594,\n",
       " 0.13207554817199707,\n",
       " 0.13097286224365234,\n",
       " 0.12812232971191406,\n",
       " 0.12932801246643066,\n",
       " 0.13187575340270996,\n",
       " 0.13041377067565918,\n",
       " 0.12995624542236328,\n",
       " 0.19485187530517578,\n",
       " 0.12957024574279785,\n",
       " 0.12966394424438477,\n",
       " 0.1280653476715088,\n",
       " 0.13150882720947266,\n",
       " 0.12711787223815918,\n",
       " 0.1256728172302246,\n",
       " 0.12952423095703125,\n",
       " 0.12906503677368164,\n",
       " 0.13178730010986328,\n",
       " 0.19459271430969238,\n",
       " 0.12001633644104004,\n",
       " 0.12906980514526367,\n",
       " 0.13058781623840332,\n",
       " 0.1280384063720703,\n",
       " 0.1311359405517578,\n",
       " 0.13184857368469238,\n",
       " 0.1294858455657959,\n",
       " 0.1324765682220459,\n",
       " 0.1293637752532959,\n",
       " 0.19247770309448242,\n",
       " 0.13024067878723145,\n",
       " 0.13043737411499023,\n",
       " 0.13093233108520508,\n",
       " 0.1313612461090088,\n",
       " 0.1897740364074707,\n",
       " 0.12133359909057617,\n",
       " 0.13621878623962402,\n",
       " 0.13880062103271484,\n",
       " 0.13218998908996582,\n",
       " 0.190901517868042,\n",
       " 0.13528800010681152,\n",
       " 0.1383047103881836,\n",
       " 0.13620829582214355,\n",
       " 0.13521313667297363,\n",
       " 0.13941001892089844,\n",
       " 0.1436910629272461,\n",
       " 0.14015936851501465,\n",
       " 0.1414051055908203,\n",
       " 0.14001989364624023,\n",
       " 0.19403719902038574,\n",
       " 0.14162921905517578,\n",
       " 0.12883782386779785,\n",
       " 0.12906503677368164,\n",
       " 0.12750887870788574,\n",
       " 0.11835598945617676,\n",
       " 0.12250375747680664,\n",
       " 0.12195825576782227,\n",
       " 0.12538504600524902,\n",
       " 0.12372612953186035,\n",
       " 0.19184327125549316,\n",
       " 0.1210176944732666,\n",
       " 0.12280535697937012,\n",
       " 0.12158679962158203,\n",
       " 0.12247943878173828,\n",
       " 0.11878514289855957,\n",
       " 0.11729693412780762,\n",
       " 0.12392091751098633,\n",
       " 0.11906170845031738,\n",
       " 0.1210336685180664,\n",
       " 0.18976616859436035,\n",
       " 0.12137007713317871,\n",
       " 0.13180255889892578,\n",
       " 0.12037134170532227,\n",
       " 0.11889863014221191,\n",
       " 0.11515307426452637,\n",
       " 0.12493085861206055,\n",
       " 0.12184381484985352,\n",
       " 0.11800479888916016,\n",
       " 0.11747336387634277,\n",
       " 0.19149374961853027,\n",
       " 0.12011051177978516,\n",
       " 0.12995171546936035,\n",
       " 0.1194465160369873,\n",
       " 0.13550424575805664,\n",
       " 0.12662386894226074,\n",
       " 0.13158750534057617,\n",
       " 0.1326158046722412,\n",
       " 0.1311337947845459,\n",
       " 0.11755800247192383,\n",
       " 0.19197916984558105,\n",
       " 0.12063288688659668,\n",
       " 0.11967897415161133,\n",
       " 0.11870408058166504,\n",
       " 0.13463473320007324,\n",
       " 0.13258099555969238,\n",
       " 0.19744157791137695,\n",
       " 0.13646674156188965,\n",
       " 0.1386868953704834,\n",
       " 0.13277912139892578,\n",
       " 0.19470572471618652,\n",
       " 0.13494038581848145,\n",
       " 0.1292872428894043,\n",
       " 0.12587285041809082,\n",
       " 0.13085055351257324,\n",
       " 0.1437394618988037,\n",
       " 0.1398007869720459,\n",
       " 0.13625359535217285,\n",
       " 0.14273619651794434,\n",
       " 0.1420753002166748,\n",
       " 0.19152164459228516,\n",
       " 0.1438918113708496,\n",
       " 0.13957953453063965,\n",
       " 0.14067554473876953,\n",
       " 0.1401810646057129,\n",
       " 0.18092036247253418,\n",
       " 0.20543980598449707,\n",
       " 0.19949889183044434,\n",
       " 0.13892483711242676,\n",
       " 0.13962125778198242,\n",
       " 0.19038081169128418,\n",
       " 0.1417844295501709,\n",
       " 0.1330561637878418,\n",
       " 0.13378667831420898,\n",
       " 0.13425302505493164,\n",
       " 0.1318073272705078,\n",
       " 0.1319413185119629,\n",
       " 0.13266730308532715,\n",
       " 0.13746023178100586,\n",
       " 0.13155341148376465,\n",
       " 0.18899917602539062,\n",
       " 0.1380906105041504,\n",
       " 0.13413524627685547,\n",
       " 0.13341283798217773,\n",
       " 0.13841962814331055,\n",
       " 0.13823866844177246,\n",
       " 0.13444113731384277,\n",
       " 0.13520288467407227,\n",
       " 0.13525795936584473,\n",
       " 0.14022135734558105,\n",
       " 0.19223904609680176,\n",
       " 0.1349339485168457,\n",
       " 0.13482975959777832,\n",
       " 0.14018607139587402,\n",
       " 0.1377394199371338,\n",
       " 0.13406896591186523,\n",
       " 0.13478803634643555,\n",
       " 0.1364588737487793,\n",
       " 0.13677501678466797,\n",
       " 0.1356792449951172,\n",
       " 0.18207573890686035,\n",
       " 0.1335923671722412,\n",
       " 0.13908863067626953,\n",
       " 0.13953089714050293,\n",
       " 0.1395125389099121,\n",
       " 0.13300299644470215,\n",
       " 0.13886523246765137,\n",
       " 0.18923306465148926,\n",
       " 0.1383810043334961,\n",
       " 0.13254857063293457,\n",
       " 0.17981958389282227,\n",
       " 0.13347530364990234,\n",
       " 0.13469600677490234,\n",
       " 0.13707637786865234,\n",
       " 0.13858485221862793,\n",
       " 0.13971543312072754,\n",
       " 0.13330340385437012,\n",
       " 0.14046359062194824,\n",
       " 0.13966703414916992,\n",
       " 0.13942646980285645,\n",
       " 0.18185186386108398,\n",
       " 0.13506174087524414,\n",
       " 0.13754820823669434,\n",
       " 0.13976621627807617,\n",
       " 0.1388871669769287,\n",
       " 0.14120221138000488,\n",
       " 0.13148736953735352,\n",
       " 0.13985133171081543,\n",
       " 0.13904356956481934,\n",
       " 0.1340954303741455,\n",
       " 0.1867210865020752,\n",
       " 0.1286945343017578,\n",
       " 0.13979601860046387,\n",
       " 0.13793349266052246,\n",
       " 0.1215674877166748,\n",
       " 0.12842774391174316,\n",
       " 0.12745261192321777,\n",
       " 0.13446450233459473,\n",
       " 0.12633776664733887,\n",
       " 0.1289050579071045,\n",
       " 0.18094515800476074,\n",
       " 0.1254866123199463,\n",
       " 0.13383960723876953,\n",
       " 0.12442159652709961,\n",
       " 0.12490558624267578,\n",
       " 0.12967419624328613,\n",
       " 0.13478803634643555,\n",
       " 0.12454032897949219,\n",
       " 0.13705658912658691,\n",
       " 0.12261581420898438,\n",
       " 0.1815023422241211,\n",
       " 0.1272437572479248,\n",
       " 0.11912941932678223,\n",
       " 0.12349438667297363,\n",
       " 0.1278378963470459,\n",
       " 0.13283276557922363,\n",
       " 0.12104296684265137,\n",
       " 0.13003969192504883,\n",
       " 0.12921571731567383,\n",
       " 0.6058883666992188,\n",
       " 0.17995500564575195,\n",
       " 0.13103461265563965,\n",
       " 0.1250143051147461,\n",
       " 0.13048195838928223,\n",
       " 0.13019704818725586,\n",
       " 0.13636398315429688,\n",
       " 0.1364281177520752,\n",
       " 0.13036036491394043,\n",
       " 0.17999649047851562,\n",
       " 0.13253211975097656,\n",
       " 0.17973923683166504,\n",
       " 0.14344573020935059,\n",
       " 0.14580035209655762,\n",
       " 0.14601588249206543,\n",
       " 0.14789438247680664,\n",
       " 0.14328360557556152,\n",
       " 0.14340853691101074,\n",
       " 0.1476285457611084,\n",
       " 0.1448345184326172,\n",
       " 0.1457052230834961,\n",
       " 0.17920398712158203,\n",
       " 0.1423816680908203,\n",
       " 0.14908885955810547,\n",
       " 0.1418910026550293,\n",
       " 0.14190459251403809,\n",
       " 0.14038610458374023,\n",
       " 0.14440083503723145,\n",
       " 0.1435098648071289,\n",
       " 0.14851784706115723,\n",
       " 0.14412832260131836]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_per_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['server0', 'server1', 'server2', 'server3']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'server4' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a58e50e12838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mserver_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"server4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: 'server4' is not in list"
     ]
    }
   ],
   "source": [
    "server_ids.index(\"server4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher.get_key_to_node_map()['fc_bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel=LinearNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc_bias': tensor([0.7625]),\n",
       " 'fc_weights.0': tensor([0.5567]),\n",
       " 'fc_weights.1': tensor([0.0267]),\n",
       " 'fc_weights.2': tensor([0.4424]),\n",
       " 'fc_weights.3': tensor([0.3825]),\n",
       " 'fc_weights.4': tensor([0.5311]),\n",
       " 'fc_weights.5': tensor([0.8576]),\n",
       " 'fc_weights.6': tensor([0.5620]),\n",
       " 'fc_weights.7': tensor([0.6615]),\n",
       " 'fc_weights.8': tensor([0.6206]),\n",
       " 'fc_weights.9': tensor([0.9785]),\n",
       " 'fc_weights.10': tensor([0.6421]),\n",
       " 'fc_weights.11': tensor([0.5424]),\n",
       " 'fc_weights.12': tensor([0.9489]),\n",
       " 'fc_weights.13': tensor([0.4326]),\n",
       " 'fc_weights.14': tensor([0.2355]),\n",
       " 'fc_weights.15': tensor([0.8898]),\n",
       " 'fc_weights.16': tensor([0.9719]),\n",
       " 'fc_weights.17': tensor([0.9767]),\n",
       " 'fc_weights.18': tensor([0.2998]),\n",
       " 'fc_weights.19': tensor([0.6725]),\n",
       " 'fc_weights.20': tensor([0.8768]),\n",
       " 'fc_weights.21': tensor([0.7303]),\n",
       " 'fc_weights.22': tensor([0.6391]),\n",
       " 'fc_weights.23': tensor([0.6964]),\n",
       " 'fc_weights.24': tensor([0.4810]),\n",
       " 'fc_weights.25': tensor([0.3722]),\n",
       " 'fc_weights.26': tensor([0.8999]),\n",
       " 'fc_weights.27': tensor([0.3884]),\n",
       " 'fc_weights.28': tensor([0.1685]),\n",
       " 'fc_weights.29': tensor([0.3936]),\n",
       " 'fc_weights.30': tensor([0.4552]),\n",
       " 'fc_weights.31': tensor([0.1880]),\n",
       " 'fc_weights.32': tensor([0.2471]),\n",
       " 'fc_weights.33': tensor([0.6051]),\n",
       " 'fc_weights.34': tensor([0.2964]),\n",
       " 'fc_weights.35': tensor([0.5948]),\n",
       " 'fc_weights.36': tensor([0.8974]),\n",
       " 'fc_weights.37': tensor([0.1101]),\n",
       " 'fc_weights.38': tensor([0.1972]),\n",
       " 'fc_weights.39': tensor([0.8189]),\n",
       " 'fc_weights.40': tensor([0.0715]),\n",
       " 'fc_weights.41': tensor([0.8899]),\n",
       " 'fc_weights.42': tensor([0.3332]),\n",
       " 'fc_weights.43': tensor([0.5072]),\n",
       " 'fc_weights.44': tensor([0.2837]),\n",
       " 'fc_weights.45': tensor([0.8767]),\n",
       " 'fc_weights.46': tensor([0.6117]),\n",
       " 'fc_weights.47': tensor([0.5177]),\n",
       " 'fc_weights.48': tensor([0.6978]),\n",
       " 'fc_weights.49': tensor([0.3316]),\n",
       " 'fc_weights.50': tensor([0.8643]),\n",
       " 'fc_weights.51': tensor([0.7003]),\n",
       " 'fc_weights.52': tensor([0.1233]),\n",
       " 'fc_weights.53': tensor([0.1730]),\n",
       " 'fc_weights.54': tensor([0.2694]),\n",
       " 'fc_weights.55': tensor([0.9803]),\n",
       " 'fc_weights.56': tensor([0.1872]),\n",
       " 'fc_weights.57': tensor([0.6635]),\n",
       " 'fc_weights.58': tensor([0.2268]),\n",
       " 'fc_weights.59': tensor([0.5473]),\n",
       " 'fc_weights.60': tensor([0.3946]),\n",
       " 'fc_weights.61': tensor([0.4909]),\n",
       " 'fc_weights.62': tensor([0.9389]),\n",
       " 'fc_weights.63': tensor([0.6069]),\n",
       " 'fc_weights.64': tensor([0.1353]),\n",
       " 'fc_weights.65': tensor([0.3428]),\n",
       " 'fc_weights.66': tensor([0.9811]),\n",
       " 'fc_weights.67': tensor([0.8428]),\n",
       " 'fc_weights.68': tensor([0.7773]),\n",
       " 'fc_weights.69': tensor([0.4340]),\n",
       " 'fc_weights.70': tensor([0.4733]),\n",
       " 'fc_weights.71': tensor([0.0277]),\n",
       " 'fc_weights.72': tensor([0.8987]),\n",
       " 'fc_weights.73': tensor([0.5039]),\n",
       " 'fc_weights.74': tensor([0.6467]),\n",
       " 'fc_weights.75': tensor([0.3526]),\n",
       " 'fc_weights.76': tensor([0.3281]),\n",
       " 'fc_weights.77': tensor([0.5660]),\n",
       " 'fc_weights.78': tensor([0.0026]),\n",
       " 'fc_weights.79': tensor([0.8693]),\n",
       " 'fc_weights.80': tensor([0.0604]),\n",
       " 'fc_weights.81': tensor([0.7813]),\n",
       " 'fc_weights.82': tensor([0.7656]),\n",
       " 'fc_weights.83': tensor([0.2427]),\n",
       " 'fc_weights.84': tensor([0.4718]),\n",
       " 'fc_weights.85': tensor([0.2625]),\n",
       " 'fc_weights.86': tensor([0.3076]),\n",
       " 'fc_weights.87': tensor([0.0130]),\n",
       " 'fc_weights.88': tensor([0.2374]),\n",
       " 'fc_weights.89': tensor([0.6674]),\n",
       " 'fc_weights.90': tensor([0.3649]),\n",
       " 'fc_weights.91': tensor([0.1961]),\n",
       " 'fc_weights.92': tensor([0.6558]),\n",
       " 'fc_weights.93': tensor([0.2807]),\n",
       " 'fc_weights.94': tensor([0.0606]),\n",
       " 'fc_weights.95': tensor([0.3978]),\n",
       " 'fc_weights.96': tensor([0.4284]),\n",
       " 'fc_weights.97': tensor([0.4936]),\n",
       " 'fc_weights.98': tensor([0.8378]),\n",
       " 'fc_weights.99': tensor([0.6511]),\n",
       " 'fc_weights.100': tensor([0.5372]),\n",
       " 'fc_weights.101': tensor([0.8678]),\n",
       " 'fc_weights.102': tensor([0.1119]),\n",
       " 'fc_weights.103': tensor([0.2242]),\n",
       " 'fc_weights.104': tensor([0.7202]),\n",
       " 'fc_weights.105': tensor([0.6494]),\n",
       " 'fc_weights.106': tensor([0.9867]),\n",
       " 'fc_weights.107': tensor([0.2684]),\n",
       " 'fc_weights.108': tensor([0.3641]),\n",
       " 'fc_weights.109': tensor([0.7815]),\n",
       " 'fc_weights.110': tensor([0.9672]),\n",
       " 'fc_weights.111': tensor([0.5665]),\n",
       " 'fc_weights.112': tensor([0.0204]),\n",
       " 'fc_weights.113': tensor([0.5874]),\n",
       " 'fc_weights.114': tensor([0.9232]),\n",
       " 'fc_weights.115': tensor([0.7837]),\n",
       " 'fc_weights.116': tensor([0.0687]),\n",
       " 'fc_weights.117': tensor([0.1729]),\n",
       " 'fc_weights.118': tensor([0.2021]),\n",
       " 'fc_weights.119': tensor([0.1733]),\n",
       " 'fc_weights.120': tensor([0.1134]),\n",
       " 'fc_weights.121': tensor([0.3406]),\n",
       " 'fc_weights.122': tensor([0.6298]),\n",
       " 'fc_weights.123': tensor([0.8149]),\n",
       " 'fc_weights.124': tensor([0.6542]),\n",
       " 'fc_weights.125': tensor([0.6125]),\n",
       " 'fc_weights.126': tensor([0.7592]),\n",
       " 'fc_weights.127': tensor([0.1453]),\n",
       " 'fc_weights.128': tensor([0.2906]),\n",
       " 'fc_weights.129': tensor([0.8972]),\n",
       " 'fc_weights.130': tensor([0.1801]),\n",
       " 'fc_weights.131': tensor([0.4057]),\n",
       " 'fc_weights.132': tensor([0.4967]),\n",
       " 'fc_weights.133': tensor([0.1841]),\n",
       " 'fc_weights.134': tensor([0.1208]),\n",
       " 'fc_weights.135': tensor([0.5800]),\n",
       " 'fc_weights.136': tensor([0.2158]),\n",
       " 'fc_weights.137': tensor([0.9913]),\n",
       " 'fc_weights.138': tensor([0.8290]),\n",
       " 'fc_weights.139': tensor([0.7372]),\n",
       " 'fc_weights.140': tensor([0.2003]),\n",
       " 'fc_weights.141': tensor([0.8894]),\n",
       " 'fc_weights.142': tensor([0.5395]),\n",
       " 'fc_weights.143': tensor([0.0833]),\n",
       " 'fc_weights.144': tensor([0.1923]),\n",
       " 'fc_weights.145': tensor([0.6103]),\n",
       " 'fc_weights.146': tensor([0.9093]),\n",
       " 'fc_weights.147': tensor([0.4833]),\n",
       " 'fc_weights.148': tensor([0.3949]),\n",
       " 'fc_weights.149': tensor([0.4463]),\n",
       " 'fc_weights.150': tensor([0.0400]),\n",
       " 'fc_weights.151': tensor([0.7101]),\n",
       " 'fc_weights.152': tensor([0.7806]),\n",
       " 'fc_weights.153': tensor([0.3071]),\n",
       " 'fc_weights.154': tensor([0.4453]),\n",
       " 'fc_weights.155': tensor([0.6243]),\n",
       " 'fc_weights.156': tensor([0.6997]),\n",
       " 'fc_weights.157': tensor([0.6692]),\n",
       " 'fc_weights.158': tensor([0.4867]),\n",
       " 'fc_weights.159': tensor([0.9764]),\n",
       " 'fc_weights.160': tensor([0.1412]),\n",
       " 'fc_weights.161': tensor([0.6923]),\n",
       " 'fc_weights.162': tensor([0.7441]),\n",
       " 'fc_weights.163': tensor([0.2915]),\n",
       " 'fc_weights.164': tensor([0.5224]),\n",
       " 'fc_weights.165': tensor([0.5726]),\n",
       " 'fc_weights.166': tensor([0.1113]),\n",
       " 'fc_weights.167': tensor([0.4965]),\n",
       " 'fc_weights.168': tensor([0.7196]),\n",
       " 'fc_weights.169': tensor([0.2720]),\n",
       " 'fc_weights.170': tensor([0.4908]),\n",
       " 'fc_weights.171': tensor([0.6682]),\n",
       " 'fc_weights.172': tensor([0.1519]),\n",
       " 'fc_weights.173': tensor([0.8194]),\n",
       " 'fc_weights.174': tensor([0.1834]),\n",
       " 'fc_weights.175': tensor([0.6993]),\n",
       " 'fc_weights.176': tensor([0.1394]),\n",
       " 'fc_weights.177': tensor([0.5423]),\n",
       " 'fc_weights.178': tensor([0.7026]),\n",
       " 'fc_weights.179': tensor([0.8395]),\n",
       " 'fc_weights.180': tensor([0.7426]),\n",
       " 'fc_weights.181': tensor([0.4872]),\n",
       " 'fc_weights.182': tensor([0.3051]),\n",
       " 'fc_weights.183': tensor([0.5298]),\n",
       " 'fc_weights.184': tensor([0.1542]),\n",
       " 'fc_weights.185': tensor([0.1390]),\n",
       " 'fc_weights.186': tensor([0.5410]),\n",
       " 'fc_weights.187': tensor([0.4380]),\n",
       " 'fc_weights.188': tensor([0.1274]),\n",
       " 'fc_weights.189': tensor([0.6118]),\n",
       " 'fc_weights.190': tensor([0.9207]),\n",
       " 'fc_weights.191': tensor([0.3969]),\n",
       " 'fc_weights.192': tensor([0.5280]),\n",
       " 'fc_weights.193': tensor([0.9284]),\n",
       " 'fc_weights.194': tensor([0.8555]),\n",
       " 'fc_weights.195': tensor([0.7195]),\n",
       " 'fc_weights.196': tensor([0.7330]),\n",
       " 'fc_weights.197': tensor([0.8522]),\n",
       " 'fc_weights.198': tensor([0.4537]),\n",
       " 'fc_weights.199': tensor([0.9926]),\n",
       " 'fc_weights.200': tensor([0.2211]),\n",
       " 'fc_weights.201': tensor([0.6367]),\n",
       " 'fc_weights.202': tensor([0.1351]),\n",
       " 'fc_weights.203': tensor([0.6210]),\n",
       " 'fc_weights.204': tensor([0.7355]),\n",
       " 'fc_weights.205': tensor([0.8311]),\n",
       " 'fc_weights.206': tensor([0.7399]),\n",
       " 'fc_weights.207': tensor([0.2414]),\n",
       " 'fc_weights.208': tensor([0.5199]),\n",
       " 'fc_weights.209': tensor([0.0205]),\n",
       " 'fc_weights.210': tensor([0.2514]),\n",
       " 'fc_weights.211': tensor([0.7458]),\n",
       " 'fc_weights.212': tensor([0.1263]),\n",
       " 'fc_weights.213': tensor([0.0783]),\n",
       " 'fc_weights.214': tensor([0.6535]),\n",
       " 'fc_weights.215': tensor([0.2574]),\n",
       " 'fc_weights.216': tensor([0.8095]),\n",
       " 'fc_weights.217': tensor([0.2486]),\n",
       " 'fc_weights.218': tensor([0.3367]),\n",
       " 'fc_weights.219': tensor([0.0844]),\n",
       " 'fc_weights.220': tensor([0.7873]),\n",
       " 'fc_weights.221': tensor([0.9258]),\n",
       " 'fc_weights.222': tensor([0.9372]),\n",
       " 'fc_weights.223': tensor([0.1561]),\n",
       " 'fc_weights.224': tensor([0.7876]),\n",
       " 'fc_weights.225': tensor([0.8100]),\n",
       " 'fc_weights.226': tensor([0.3686]),\n",
       " 'fc_weights.227': tensor([0.9206]),\n",
       " 'fc_weights.228': tensor([0.5207]),\n",
       " 'fc_weights.229': tensor([0.8997]),\n",
       " 'fc_weights.230': tensor([0.9022]),\n",
       " 'fc_weights.231': tensor([0.1905]),\n",
       " 'fc_weights.232': tensor([0.8638]),\n",
       " 'fc_weights.233': tensor([0.4816]),\n",
       " 'fc_weights.234': tensor([0.4610]),\n",
       " 'fc_weights.235': tensor([0.9430]),\n",
       " 'fc_weights.236': tensor([0.3083]),\n",
       " 'fc_weights.237': tensor([0.5289]),\n",
       " 'fc_weights.238': tensor([0.6719]),\n",
       " 'fc_weights.239': tensor([0.0213]),\n",
       " 'fc_weights.240': tensor([0.3253]),\n",
       " 'fc_weights.241': tensor([0.8446]),\n",
       " 'fc_weights.242': tensor([0.1870]),\n",
       " 'fc_weights.243': tensor([0.1618]),\n",
       " 'fc_weights.244': tensor([0.5481]),\n",
       " 'fc_weights.245': tensor([0.4226]),\n",
       " 'fc_weights.246': tensor([0.2411]),\n",
       " 'fc_weights.247': tensor([0.0518]),\n",
       " 'fc_weights.248': tensor([0.5404]),\n",
       " 'fc_weights.249': tensor([0.9196]),\n",
       " 'fc_weights.250': tensor([0.8260]),\n",
       " 'fc_weights.251': tensor([0.6679]),\n",
       " 'fc_weights.252': tensor([0.0833]),\n",
       " 'fc_weights.253': tensor([0.1937]),\n",
       " 'fc_weights.254': tensor([0.7598]),\n",
       " 'fc_weights.255': tensor([0.8274]),\n",
       " 'fc_weights.256': tensor([0.2132]),\n",
       " 'fc_weights.257': tensor([0.3669]),\n",
       " 'fc_weights.258': tensor([0.8428]),\n",
       " 'fc_weights.259': tensor([0.8586]),\n",
       " 'fc_weights.260': tensor([0.3030]),\n",
       " 'fc_weights.261': tensor([0.5176]),\n",
       " 'fc_weights.262': tensor([0.0623]),\n",
       " 'fc_weights.263': tensor([0.1966]),\n",
       " 'fc_weights.264': tensor([0.9233]),\n",
       " 'fc_weights.265': tensor([0.4177]),\n",
       " 'fc_weights.266': tensor([0.4501]),\n",
       " 'fc_weights.267': tensor([0.6788]),\n",
       " 'fc_weights.268': tensor([0.8516]),\n",
       " 'fc_weights.269': tensor([0.9824]),\n",
       " 'fc_weights.270': tensor([0.7776]),\n",
       " 'fc_weights.271': tensor([0.3584]),\n",
       " 'fc_weights.272': tensor([0.7839]),\n",
       " 'fc_weights.273': tensor([0.4955]),\n",
       " 'fc_weights.274': tensor([0.7328]),\n",
       " 'fc_weights.275': tensor([0.9950]),\n",
       " 'fc_weights.276': tensor([0.0858]),\n",
       " 'fc_weights.277': tensor([0.5690]),\n",
       " 'fc_weights.278': tensor([0.6279]),\n",
       " 'fc_weights.279': tensor([0.1951]),\n",
       " 'fc_weights.280': tensor([0.1237]),\n",
       " 'fc_weights.281': tensor([0.9587]),\n",
       " 'fc_weights.282': tensor([0.4556]),\n",
       " 'fc_weights.283': tensor([0.6684]),\n",
       " 'fc_weights.284': tensor([0.6251]),\n",
       " 'fc_weights.285': tensor([0.6041]),\n",
       " 'fc_weights.286': tensor([0.6741]),\n",
       " 'fc_weights.287': tensor([0.1346]),\n",
       " 'fc_weights.288': tensor([0.6569]),\n",
       " 'fc_weights.289': tensor([0.5533]),\n",
       " 'fc_weights.290': tensor([0.3399]),\n",
       " 'fc_weights.291': tensor([0.7719]),\n",
       " 'fc_weights.292': tensor([0.8539]),\n",
       " 'fc_weights.293': tensor([0.0230]),\n",
       " 'fc_weights.294': tensor([0.0752]),\n",
       " 'fc_weights.295': tensor([0.0036]),\n",
       " 'fc_weights.296': tensor([0.2037]),\n",
       " 'fc_weights.297': tensor([0.8923]),\n",
       " 'fc_weights.298': tensor([0.1159]),\n",
       " 'fc_weights.299': tensor([0.7056]),\n",
       " 'fc_weights.300': tensor([0.6748]),\n",
       " 'fc_weights.301': tensor([0.3363]),\n",
       " 'fc_weights.302': tensor([0.9991]),\n",
       " 'fc_weights.303': tensor([0.7621]),\n",
       " 'fc_weights.304': tensor([0.9812]),\n",
       " 'fc_weights.305': tensor([0.7233]),\n",
       " 'fc_weights.306': tensor([0.4696]),\n",
       " 'fc_weights.307': tensor([0.3138]),\n",
       " 'fc_weights.308': tensor([0.3329]),\n",
       " 'fc_weights.309': tensor([0.4649]),\n",
       " 'fc_weights.310': tensor([0.6256]),\n",
       " 'fc_weights.311': tensor([0.2406]),\n",
       " 'fc_weights.312': tensor([0.6590]),\n",
       " 'fc_weights.313': tensor([0.5891]),\n",
       " 'fc_weights.314': tensor([0.5727]),\n",
       " 'fc_weights.315': tensor([0.9017]),\n",
       " 'fc_weights.316': tensor([0.7307]),\n",
       " 'fc_weights.317': tensor([0.8789]),\n",
       " 'fc_weights.318': tensor([0.9673]),\n",
       " 'fc_weights.319': tensor([0.4310]),\n",
       " 'fc_weights.320': tensor([0.4886]),\n",
       " 'fc_weights.321': tensor([0.5743]),\n",
       " 'fc_weights.322': tensor([0.7229]),\n",
       " 'fc_weights.323': tensor([0.1103]),\n",
       " 'fc_weights.324': tensor([0.2025]),\n",
       " 'fc_weights.325': tensor([0.8711]),\n",
       " 'fc_weights.326': tensor([0.1192]),\n",
       " 'fc_weights.327': tensor([0.6133]),\n",
       " 'fc_weights.328': tensor([0.8475]),\n",
       " 'fc_weights.329': tensor([0.0143]),\n",
       " 'fc_weights.330': tensor([0.0615]),\n",
       " 'fc_weights.331': tensor([0.4685]),\n",
       " 'fc_weights.332': tensor([0.9549]),\n",
       " 'fc_weights.333': tensor([0.9710]),\n",
       " 'fc_weights.334': tensor([0.9362]),\n",
       " 'fc_weights.335': tensor([0.6918]),\n",
       " 'fc_weights.336': tensor([0.0450]),\n",
       " 'fc_weights.337': tensor([0.4208]),\n",
       " 'fc_weights.338': tensor([0.9861]),\n",
       " 'fc_weights.339': tensor([0.7076]),\n",
       " 'fc_weights.340': tensor([0.2278]),\n",
       " 'fc_weights.341': tensor([0.6735]),\n",
       " 'fc_weights.342': tensor([0.4650]),\n",
       " 'fc_weights.343': tensor([0.5346]),\n",
       " 'fc_weights.344': tensor([0.6681]),\n",
       " 'fc_weights.345': tensor([0.0045]),\n",
       " 'fc_weights.346': tensor([0.3208]),\n",
       " 'fc_weights.347': tensor([0.6250]),\n",
       " 'fc_weights.348': tensor([0.5843]),\n",
       " 'fc_weights.349': tensor([0.4561]),\n",
       " 'fc_weights.350': tensor([0.5521]),\n",
       " 'fc_weights.351': tensor([0.4113]),\n",
       " 'fc_weights.352': tensor([0.9333]),\n",
       " 'fc_weights.353': tensor([0.7285]),\n",
       " 'fc_weights.354': tensor([0.9195]),\n",
       " 'fc_weights.355': tensor([0.8361]),\n",
       " 'fc_weights.356': tensor([0.0717]),\n",
       " 'fc_weights.357': tensor([0.2217]),\n",
       " 'fc_weights.358': tensor([0.1219]),\n",
       " 'fc_weights.359': tensor([0.1402]),\n",
       " 'fc_weights.360': tensor([0.7816]),\n",
       " 'fc_weights.361': tensor([0.6121]),\n",
       " 'fc_weights.362': tensor([0.3482]),\n",
       " 'fc_weights.363': tensor([0.6504]),\n",
       " 'fc_weights.364': tensor([0.2803]),\n",
       " 'fc_weights.365': tensor([0.9507]),\n",
       " 'fc_weights.366': tensor([0.5290]),\n",
       " 'fc_weights.367': tensor([0.7220]),\n",
       " 'fc_weights.368': tensor([0.6440]),\n",
       " 'fc_weights.369': tensor([0.2346]),\n",
       " 'fc_weights.370': tensor([0.9901]),\n",
       " 'fc_weights.371': tensor([0.7146]),\n",
       " 'fc_weights.372': tensor([0.0770]),\n",
       " 'fc_weights.373': tensor([0.9077]),\n",
       " 'fc_weights.374': tensor([0.1698]),\n",
       " 'fc_weights.375': tensor([0.8739]),\n",
       " 'fc_weights.376': tensor([0.4723]),\n",
       " 'fc_weights.377': tensor([0.2339]),\n",
       " 'fc_weights.378': tensor([0.7975]),\n",
       " 'fc_weights.379': tensor([0.6585]),\n",
       " 'fc_weights.380': tensor([0.3807]),\n",
       " 'fc_weights.381': tensor([0.3239]),\n",
       " 'fc_weights.382': tensor([0.0189]),\n",
       " 'fc_weights.383': tensor([0.2437]),\n",
       " 'fc_weights.384': tensor([0.8505]),\n",
       " 'fc_weights.385': tensor([0.4888]),\n",
       " 'fc_weights.386': tensor([0.5200]),\n",
       " 'fc_weights.387': tensor([0.6393]),\n",
       " 'fc_weights.388': tensor([0.0826]),\n",
       " 'fc_weights.389': tensor([0.2117]),\n",
       " 'fc_weights.390': tensor([0.5443]),\n",
       " 'fc_weights.391': tensor([0.9049]),\n",
       " 'fc_weights.392': tensor([0.4385]),\n",
       " 'fc_weights.393': tensor([0.0601]),\n",
       " 'fc_weights.394': tensor([0.0029]),\n",
       " 'fc_weights.395': tensor([0.6535]),\n",
       " 'fc_weights.396': tensor([0.9199]),\n",
       " 'fc_weights.397': tensor([0.5837]),\n",
       " 'fc_weights.398': tensor([0.0586]),\n",
       " 'fc_weights.399': tensor([0.6461]),\n",
       " 'fc_weights.400': tensor([0.7872]),\n",
       " 'fc_weights.401': tensor([0.1815]),\n",
       " 'fc_weights.402': tensor([0.1229]),\n",
       " 'fc_weights.403': tensor([0.8780]),\n",
       " 'fc_weights.404': tensor([0.7861]),\n",
       " 'fc_weights.405': tensor([0.6214]),\n",
       " 'fc_weights.406': tensor([0.6655]),\n",
       " 'fc_weights.407': tensor([0.0268]),\n",
       " 'fc_weights.408': tensor([0.5651]),\n",
       " 'fc_weights.409': tensor([0.7611]),\n",
       " 'fc_weights.410': tensor([0.3256]),\n",
       " 'fc_weights.411': tensor([0.2275]),\n",
       " 'fc_weights.412': tensor([0.2304]),\n",
       " 'fc_weights.413': tensor([0.6075]),\n",
       " 'fc_weights.414': tensor([0.9440]),\n",
       " 'fc_weights.415': tensor([0.8373]),\n",
       " 'fc_weights.416': tensor([0.3902]),\n",
       " 'fc_weights.417': tensor([0.1725]),\n",
       " 'fc_weights.418': tensor([0.6470]),\n",
       " 'fc_weights.419': tensor([0.3482]),\n",
       " 'fc_weights.420': tensor([0.8530]),\n",
       " 'fc_weights.421': tensor([0.9557]),\n",
       " 'fc_weights.422': tensor([0.5696]),\n",
       " 'fc_weights.423': tensor([0.3027]),\n",
       " 'fc_weights.424': tensor([0.2709]),\n",
       " 'fc_weights.425': tensor([0.1711]),\n",
       " 'fc_weights.426': tensor([0.0726]),\n",
       " 'fc_weights.427': tensor([0.9476]),\n",
       " 'fc_weights.428': tensor([0.2040]),\n",
       " 'fc_weights.429': tensor([0.7306]),\n",
       " 'fc_weights.430': tensor([0.9221]),\n",
       " 'fc_weights.431': tensor([0.1667]),\n",
       " 'fc_weights.432': tensor([0.2709]),\n",
       " 'fc_weights.433': tensor([0.5720]),\n",
       " 'fc_weights.434': tensor([0.7101]),\n",
       " 'fc_weights.435': tensor([0.8398]),\n",
       " 'fc_weights.436': tensor([0.2065]),\n",
       " 'fc_weights.437': tensor([0.7501]),\n",
       " 'fc_weights.438': tensor([0.2288]),\n",
       " 'fc_weights.439': tensor([0.0080]),\n",
       " 'fc_weights.440': tensor([0.3613]),\n",
       " 'fc_weights.441': tensor([0.1574]),\n",
       " 'fc_weights.442': tensor([0.1616]),\n",
       " 'fc_weights.443': tensor([0.1784]),\n",
       " 'fc_weights.444': tensor([0.0600]),\n",
       " 'fc_weights.445': tensor([0.0782]),\n",
       " 'fc_weights.446': tensor([0.5548]),\n",
       " 'fc_weights.447': tensor([0.3134]),\n",
       " 'fc_weights.448': tensor([0.6594]),\n",
       " 'fc_weights.449': tensor([0.0428]),\n",
       " 'fc_weights.450': tensor([0.5967]),\n",
       " 'fc_weights.451': tensor([0.9557]),\n",
       " 'fc_weights.452': tensor([0.1050]),\n",
       " 'fc_weights.453': tensor([0.8661]),\n",
       " 'fc_weights.454': tensor([0.4976]),\n",
       " 'fc_weights.455': tensor([0.3576]),\n",
       " 'fc_weights.456': tensor([0.5573]),\n",
       " 'fc_weights.457': tensor([0.2397]),\n",
       " 'fc_weights.458': tensor([0.8565]),\n",
       " 'fc_weights.459': tensor([0.2550]),\n",
       " 'fc_weights.460': tensor([0.1528]),\n",
       " 'fc_weights.461': tensor([0.3285]),\n",
       " 'fc_weights.462': tensor([0.0467]),\n",
       " 'fc_weights.463': tensor([0.5050]),\n",
       " 'fc_weights.464': tensor([0.8533]),\n",
       " 'fc_weights.465': tensor([0.5879]),\n",
       " 'fc_weights.466': tensor([0.8979]),\n",
       " 'fc_weights.467': tensor([0.2039]),\n",
       " 'fc_weights.468': tensor([0.2274]),\n",
       " 'fc_weights.469': tensor([0.7326]),\n",
       " 'fc_weights.470': tensor([0.8664]),\n",
       " 'fc_weights.471': tensor([0.8818]),\n",
       " 'fc_weights.472': tensor([0.9897]),\n",
       " 'fc_weights.473': tensor([0.1989]),\n",
       " 'fc_weights.474': tensor([0.6717]),\n",
       " 'fc_weights.475': tensor([0.6322]),\n",
       " 'fc_weights.476': tensor([0.8413]),\n",
       " 'fc_weights.477': tensor([0.3907]),\n",
       " 'fc_weights.478': tensor([0.4359]),\n",
       " 'fc_weights.479': tensor([0.9175]),\n",
       " 'fc_weights.480': tensor([0.0398]),\n",
       " 'fc_weights.481': tensor([0.7389]),\n",
       " 'fc_weights.482': tensor([0.7645]),\n",
       " 'fc_weights.483': tensor([0.2450]),\n",
       " 'fc_weights.484': tensor([0.2236]),\n",
       " 'fc_weights.485': tensor([0.7556]),\n",
       " 'fc_weights.486': tensor([0.0895]),\n",
       " 'fc_weights.487': tensor([0.5619]),\n",
       " 'fc_weights.488': tensor([0.4674]),\n",
       " 'fc_weights.489': tensor([0.2467]),\n",
       " 'fc_weights.490': tensor([0.3900]),\n",
       " 'fc_weights.491': tensor([0.9551]),\n",
       " 'fc_weights.492': tensor([0.4503]),\n",
       " 'fc_weights.493': tensor([0.3018]),\n",
       " 'fc_weights.494': tensor([0.1677]),\n",
       " 'fc_weights.495': tensor([0.7035]),\n",
       " 'fc_weights.496': tensor([0.0631]),\n",
       " 'fc_weights.497': tensor([0.3558]),\n",
       " 'fc_weights.498': tensor([0.7726]),\n",
       " 'fc_weights.499': tensor([0.8322]),\n",
       " 'fc_weights.500': tensor([0.1154]),\n",
       " 'fc_weights.501': tensor([0.3440]),\n",
       " 'fc_weights.502': tensor([0.6093]),\n",
       " 'fc_weights.503': tensor([0.1467]),\n",
       " 'fc_weights.504': tensor([0.1662]),\n",
       " 'fc_weights.505': tensor([0.2079]),\n",
       " 'fc_weights.506': tensor([0.5103]),\n",
       " 'fc_weights.507': tensor([0.2939]),\n",
       " 'fc_weights.508': tensor([0.7913]),\n",
       " 'fc_weights.509': tensor([0.8855]),\n",
       " 'fc_weights.510': tensor([0.6723]),\n",
       " 'fc_weights.511': tensor([0.8634]),\n",
       " 'fc_weights.512': tensor([0.3973]),\n",
       " 'fc_weights.513': tensor([0.6982]),\n",
       " 'fc_weights.514': tensor([0.0234]),\n",
       " 'fc_weights.515': tensor([0.2785]),\n",
       " 'fc_weights.516': tensor([0.2489]),\n",
       " 'fc_weights.517': tensor([0.0064]),\n",
       " 'fc_weights.518': tensor([0.3801]),\n",
       " 'fc_weights.519': tensor([0.3370]),\n",
       " 'fc_weights.520': tensor([0.8467]),\n",
       " 'fc_weights.521': tensor([0.3352]),\n",
       " 'fc_weights.522': tensor([0.8924]),\n",
       " 'fc_weights.523': tensor([0.4074]),\n",
       " 'fc_weights.524': tensor([0.2309]),\n",
       " 'fc_weights.525': tensor([0.3560]),\n",
       " 'fc_weights.526': tensor([0.4443]),\n",
       " 'fc_weights.527': tensor([0.6790]),\n",
       " 'fc_weights.528': tensor([0.2471]),\n",
       " 'fc_weights.529': tensor([0.2572]),\n",
       " 'fc_weights.530': tensor([0.4837]),\n",
       " 'fc_weights.531': tensor([0.6287]),\n",
       " 'fc_weights.532': tensor([0.6658]),\n",
       " 'fc_weights.533': tensor([0.5282]),\n",
       " 'fc_weights.534': tensor([0.7684]),\n",
       " 'fc_weights.535': tensor([0.1598]),\n",
       " 'fc_weights.536': tensor([0.7683]),\n",
       " 'fc_weights.537': tensor([0.9911]),\n",
       " 'fc_weights.538': tensor([0.8561]),\n",
       " 'fc_weights.539': tensor([0.2831]),\n",
       " 'fc_weights.540': tensor([0.8860]),\n",
       " 'fc_weights.541': tensor([0.3768]),\n",
       " 'fc_weights.542': tensor([0.4888]),\n",
       " 'fc_weights.543': tensor([0.1207]),\n",
       " 'fc_weights.544': tensor([0.1600]),\n",
       " 'fc_weights.545': tensor([0.6687]),\n",
       " 'fc_weights.546': tensor([0.5498]),\n",
       " 'fc_weights.547': tensor([0.2282]),\n",
       " 'fc_weights.548': tensor([0.2844]),\n",
       " 'fc_weights.549': tensor([0.1310]),\n",
       " 'fc_weights.550': tensor([0.3642]),\n",
       " 'fc_weights.551': tensor([0.8770]),\n",
       " 'fc_weights.552': tensor([0.8471]),\n",
       " 'fc_weights.553': tensor([0.7820]),\n",
       " 'fc_weights.554': tensor([0.9578]),\n",
       " 'fc_weights.555': tensor([0.5248]),\n",
       " 'fc_weights.556': tensor([0.2494]),\n",
       " 'fc_weights.557': tensor([0.5794]),\n",
       " 'fc_weights.558': tensor([0.8896]),\n",
       " 'fc_weights.559': tensor([0.7642]),\n",
       " 'fc_weights.560': tensor([0.8780]),\n",
       " 'fc_weights.561': tensor([0.6299]),\n",
       " 'fc_weights.562': tensor([0.9672]),\n",
       " 'fc_weights.563': tensor([0.6397]),\n",
       " 'fc_weights.564': tensor([0.7420]),\n",
       " 'fc_weights.565': tensor([0.6347]),\n",
       " 'fc_weights.566': tensor([0.0500]),\n",
       " 'fc_weights.567': tensor([0.7330]),\n",
       " 'fc_weights.568': tensor([0.3319]),\n",
       " 'fc_weights.569': tensor([0.7163]),\n",
       " 'fc_weights.570': tensor([0.4752]),\n",
       " 'fc_weights.571': tensor([0.5772]),\n",
       " 'fc_weights.572': tensor([0.1026]),\n",
       " 'fc_weights.573': tensor([0.0553]),\n",
       " 'fc_weights.574': tensor([0.0736]),\n",
       " 'fc_weights.575': tensor([0.8005]),\n",
       " 'fc_weights.576': tensor([0.1275]),\n",
       " 'fc_weights.577': tensor([0.3111]),\n",
       " 'fc_weights.578': tensor([0.8820]),\n",
       " 'fc_weights.579': tensor([0.5926]),\n",
       " 'fc_weights.580': tensor([0.7868]),\n",
       " 'fc_weights.581': tensor([0.1855]),\n",
       " 'fc_weights.582': tensor([0.2678]),\n",
       " 'fc_weights.583': tensor([0.1216]),\n",
       " 'fc_weights.584': tensor([0.4754]),\n",
       " 'fc_weights.585': tensor([0.4203]),\n",
       " 'fc_weights.586': tensor([0.1131]),\n",
       " 'fc_weights.587': tensor([0.3086]),\n",
       " 'fc_weights.588': tensor([0.4064]),\n",
       " 'fc_weights.589': tensor([0.2162]),\n",
       " 'fc_weights.590': tensor([0.2701]),\n",
       " 'fc_weights.591': tensor([0.0056]),\n",
       " 'fc_weights.592': tensor([0.3150]),\n",
       " 'fc_weights.593': tensor([0.3511]),\n",
       " 'fc_weights.594': tensor([0.8519]),\n",
       " 'fc_weights.595': tensor([0.1338]),\n",
       " 'fc_weights.596': tensor([0.8400]),\n",
       " 'fc_weights.597': tensor([0.0077]),\n",
       " 'fc_weights.598': tensor([0.3578]),\n",
       " 'fc_weights.599': tensor([0.9120]),\n",
       " 'fc_weights.600': tensor([0.9130]),\n",
       " 'fc_weights.601': tensor([0.8631]),\n",
       " 'fc_weights.602': tensor([0.2471]),\n",
       " 'fc_weights.603': tensor([0.4306]),\n",
       " 'fc_weights.604': tensor([0.3207]),\n",
       " 'fc_weights.605': tensor([0.7904]),\n",
       " 'fc_weights.606': tensor([0.0885]),\n",
       " 'fc_weights.607': tensor([0.9312]),\n",
       " 'fc_weights.608': tensor([0.9925]),\n",
       " 'fc_weights.609': tensor([0.9711]),\n",
       " 'fc_weights.610': tensor([0.6080]),\n",
       " 'fc_weights.611': tensor([0.8758]),\n",
       " 'fc_weights.612': tensor([0.8867]),\n",
       " 'fc_weights.613': tensor([0.5778]),\n",
       " 'fc_weights.614': tensor([0.7596]),\n",
       " 'fc_weights.615': tensor([0.5265]),\n",
       " 'fc_weights.616': tensor([0.2661]),\n",
       " 'fc_weights.617': tensor([0.1774]),\n",
       " 'fc_weights.618': tensor([0.9236]),\n",
       " 'fc_weights.619': tensor([0.8819]),\n",
       " 'fc_weights.620': tensor([0.8168]),\n",
       " 'fc_weights.621': tensor([0.9207]),\n",
       " 'fc_weights.622': tensor([0.9289]),\n",
       " 'fc_weights.623': tensor([0.0714]),\n",
       " 'fc_weights.624': tensor([0.6081]),\n",
       " 'fc_weights.625': tensor([0.8485]),\n",
       " 'fc_weights.626': tensor([0.2819]),\n",
       " 'fc_weights.627': tensor([0.5874]),\n",
       " 'fc_weights.628': tensor([0.4672]),\n",
       " 'fc_weights.629': tensor([0.0665]),\n",
       " 'fc_weights.630': tensor([0.3806]),\n",
       " 'fc_weights.631': tensor([0.2647]),\n",
       " 'fc_weights.632': tensor([0.0823]),\n",
       " 'fc_weights.633': tensor([0.9066]),\n",
       " 'fc_weights.634': tensor([0.1315]),\n",
       " 'fc_weights.635': tensor([0.8551]),\n",
       " 'fc_weights.636': tensor([0.3051]),\n",
       " 'fc_weights.637': tensor([0.6440]),\n",
       " 'fc_weights.638': tensor([0.9074]),\n",
       " 'fc_weights.639': tensor([0.7919]),\n",
       " 'fc_weights.640': tensor([0.3692]),\n",
       " 'fc_weights.641': tensor([0.2489]),\n",
       " 'fc_weights.642': tensor([0.6011]),\n",
       " 'fc_weights.643': tensor([0.2534]),\n",
       " 'fc_weights.644': tensor([0.1858]),\n",
       " 'fc_weights.645': tensor([0.8550]),\n",
       " 'fc_weights.646': tensor([0.3536]),\n",
       " 'fc_weights.647': tensor([0.7138]),\n",
       " 'fc_weights.648': tensor([0.6154]),\n",
       " 'fc_weights.649': tensor([0.2099]),\n",
       " 'fc_weights.650': tensor([0.9975]),\n",
       " 'fc_weights.651': tensor([0.6819]),\n",
       " 'fc_weights.652': tensor([0.2469]),\n",
       " 'fc_weights.653': tensor([0.7277]),\n",
       " 'fc_weights.654': tensor([0.9778]),\n",
       " 'fc_weights.655': tensor([0.9167]),\n",
       " 'fc_weights.656': tensor([0.9959]),\n",
       " 'fc_weights.657': tensor([0.5132]),\n",
       " 'fc_weights.658': tensor([0.0771]),\n",
       " 'fc_weights.659': tensor([0.0618]),\n",
       " 'fc_weights.660': tensor([0.2892]),\n",
       " 'fc_weights.661': tensor([0.3903]),\n",
       " 'fc_weights.662': tensor([0.0595]),\n",
       " 'fc_weights.663': tensor([0.5793]),\n",
       " 'fc_weights.664': tensor([0.6411]),\n",
       " 'fc_weights.665': tensor([0.6597]),\n",
       " 'fc_weights.666': tensor([0.6888]),\n",
       " 'fc_weights.667': tensor([0.1010]),\n",
       " 'fc_weights.668': tensor([0.0786]),\n",
       " 'fc_weights.669': tensor([0.6745]),\n",
       " 'fc_weights.670': tensor([0.3146]),\n",
       " 'fc_weights.671': tensor([0.5480]),\n",
       " 'fc_weights.672': tensor([0.8201]),\n",
       " 'fc_weights.673': tensor([0.4633]),\n",
       " 'fc_weights.674': tensor([0.7091]),\n",
       " 'fc_weights.675': tensor([0.9946]),\n",
       " 'fc_weights.676': tensor([0.5373]),\n",
       " 'fc_weights.677': tensor([0.6324]),\n",
       " 'fc_weights.678': tensor([0.3732]),\n",
       " 'fc_weights.679': tensor([0.9691]),\n",
       " 'fc_weights.680': tensor([0.6322]),\n",
       " 'fc_weights.681': tensor([0.7498]),\n",
       " 'fc_weights.682': tensor([0.6701]),\n",
       " 'fc_weights.683': tensor([0.6551]),\n",
       " 'fc_weights.684': tensor([0.1816]),\n",
       " 'fc_weights.685': tensor([0.3969]),\n",
       " 'fc_weights.686': tensor([0.6722]),\n",
       " 'fc_weights.687': tensor([0.0051]),\n",
       " 'fc_weights.688': tensor([0.1791]),\n",
       " 'fc_weights.689': tensor([0.5287]),\n",
       " 'fc_weights.690': tensor([0.4960]),\n",
       " 'fc_weights.691': tensor([0.6812]),\n",
       " 'fc_weights.692': tensor([0.5508]),\n",
       " 'fc_weights.693': tensor([0.6460]),\n",
       " 'fc_weights.694': tensor([0.3578]),\n",
       " 'fc_weights.695': tensor([0.0082]),\n",
       " 'fc_weights.696': tensor([0.4276]),\n",
       " 'fc_weights.697': tensor([0.1977]),\n",
       " 'fc_weights.698': tensor([0.3648]),\n",
       " 'fc_weights.699': tensor([0.6015]),\n",
       " 'fc_weights.700': tensor([0.1512]),\n",
       " 'fc_weights.701': tensor([0.5620]),\n",
       " 'fc_weights.702': tensor([0.4886]),\n",
       " 'fc_weights.703': tensor([0.3733]),\n",
       " 'fc_weights.704': tensor([0.7528]),\n",
       " 'fc_weights.705': tensor([0.4631]),\n",
       " 'fc_weights.706': tensor([0.2164]),\n",
       " 'fc_weights.707': tensor([0.7096]),\n",
       " 'fc_weights.708': tensor([0.2844]),\n",
       " 'fc_weights.709': tensor([0.8805]),\n",
       " 'fc_weights.710': tensor([0.6141]),\n",
       " 'fc_weights.711': tensor([0.4621]),\n",
       " 'fc_weights.712': tensor([0.6171]),\n",
       " 'fc_weights.713': tensor([0.0505]),\n",
       " 'fc_weights.714': tensor([0.5244]),\n",
       " 'fc_weights.715': tensor([0.6539]),\n",
       " 'fc_weights.716': tensor([0.5997]),\n",
       " 'fc_weights.717': tensor([0.1261]),\n",
       " 'fc_weights.718': tensor([0.9037]),\n",
       " 'fc_weights.719': tensor([0.9317]),\n",
       " 'fc_weights.720': tensor([0.9927]),\n",
       " 'fc_weights.721': tensor([0.9289]),\n",
       " 'fc_weights.722': tensor([0.3419]),\n",
       " 'fc_weights.723': tensor([0.4781]),\n",
       " 'fc_weights.724': tensor([0.9177]),\n",
       " 'fc_weights.725': tensor([0.7975]),\n",
       " 'fc_weights.726': tensor([0.0261]),\n",
       " 'fc_weights.727': tensor([0.9603]),\n",
       " 'fc_weights.728': tensor([0.7458]),\n",
       " 'fc_weights.729': tensor([0.2070]),\n",
       " 'fc_weights.730': tensor([0.6496]),\n",
       " 'fc_weights.731': tensor([0.0183]),\n",
       " 'fc_weights.732': tensor([0.4439]),\n",
       " 'fc_weights.733': tensor([0.2163]),\n",
       " 'fc_weights.734': tensor([0.7598]),\n",
       " 'fc_weights.735': tensor([0.4471]),\n",
       " 'fc_weights.736': tensor([0.4724]),\n",
       " 'fc_weights.737': tensor([0.3902]),\n",
       " 'fc_weights.738': tensor([0.3351]),\n",
       " 'fc_weights.739': tensor([0.4056]),\n",
       " 'fc_weights.740': tensor([0.5142]),\n",
       " 'fc_weights.741': tensor([0.5281]),\n",
       " 'fc_weights.742': tensor([0.5527]),\n",
       " 'fc_weights.743': tensor([0.5954]),\n",
       " 'fc_weights.744': tensor([0.9328]),\n",
       " 'fc_weights.745': tensor([0.0856]),\n",
       " 'fc_weights.746': tensor([0.6105]),\n",
       " 'fc_weights.747': tensor([0.9170]),\n",
       " 'fc_weights.748': tensor([0.2149]),\n",
       " 'fc_weights.749': tensor([0.2371]),\n",
       " 'fc_weights.750': tensor([0.0787]),\n",
       " 'fc_weights.751': tensor([0.8912]),\n",
       " 'fc_weights.752': tensor([0.4434]),\n",
       " 'fc_weights.753': tensor([0.1828]),\n",
       " 'fc_weights.754': tensor([0.2419]),\n",
       " 'fc_weights.755': tensor([0.4014]),\n",
       " 'fc_weights.756': tensor([0.9063]),\n",
       " 'fc_weights.757': tensor([0.2846]),\n",
       " 'fc_weights.758': tensor([0.3928]),\n",
       " 'fc_weights.759': tensor([0.7510]),\n",
       " 'fc_weights.760': tensor([0.1939]),\n",
       " 'fc_weights.761': tensor([0.2029]),\n",
       " 'fc_weights.762': tensor([0.2135]),\n",
       " 'fc_weights.763': tensor([0.6513]),\n",
       " 'fc_weights.764': tensor([0.2182]),\n",
       " 'fc_weights.765': tensor([0.9014]),\n",
       " 'fc_weights.766': tensor([0.5389]),\n",
       " 'fc_weights.767': tensor([0.0655]),\n",
       " 'fc_weights.768': tensor([0.8326]),\n",
       " 'fc_weights.769': tensor([0.4620]),\n",
       " 'fc_weights.770': tensor([0.6291]),\n",
       " 'fc_weights.771': tensor([0.3477]),\n",
       " 'fc_weights.772': tensor([0.0022]),\n",
       " 'fc_weights.773': tensor([0.0499]),\n",
       " 'fc_weights.774': tensor([0.7804]),\n",
       " 'fc_weights.775': tensor([0.0233]),\n",
       " 'fc_weights.776': tensor([0.8129]),\n",
       " 'fc_weights.777': tensor([0.5339]),\n",
       " 'fc_weights.778': tensor([0.8210]),\n",
       " 'fc_weights.779': tensor([0.9763]),\n",
       " 'fc_weights.780': tensor([0.2680]),\n",
       " 'fc_weights.781': tensor([0.3246]),\n",
       " 'fc_weights.782': tensor([0.9250]),\n",
       " 'fc_weights.783': tensor([0.2904])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(testmodel.state_dict().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(current_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "servers[0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([184, 198, 191, 212])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ParamServer",
   "language": "python",
   "name": "paramserver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

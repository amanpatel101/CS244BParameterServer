{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from filelock import FileLock\n",
    "import numpy as np\n",
    "import ray\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from consistent_hashing import ConsistentHash\n",
    "import math \n",
    "from time import time\n",
    "def get_data_loader():\n",
    "    \n",
    "    \"\"\"Safely downloads data. Returns training/validation set dataloader.\"\"\"\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    \n",
    "    class MNISTEvenOddDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, ready_data):\n",
    "            self.img_data = ready_data.data\n",
    "            self.labels = ready_data.targets % 2\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "        def __getitem__(self, ind):\n",
    "            return torch.true_divide(self.img_data[ind].view(-1, 28 * 28).squeeze(), 255), torch.tensor([self.labels[ind]])\n",
    "\n",
    "\n",
    "    \n",
    "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
    "        \n",
    "        train_dataset = datasets.MNIST(\n",
    "                \"~/data\", train=True, download=True, transform=mnist_transforms\n",
    "            )\n",
    "        \n",
    "        test_dataset = datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            MNISTEvenOddDataset(train_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "             MNISTEvenOddDataset(test_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Evaluates the accuracy of the model on a validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # This is only set to finish evaluation faster.\n",
    "            if batch_idx * len(data) > 1024:\n",
    "                break\n",
    "            outputs = nn.Sigmoid()(model(data))\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = outputs > 0.5\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    \"\"\"Small Linear Network for MNIST.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc_weights = nn.ParameterList([nn.Parameter(torch.empty(1)) for weight in range(784)])\n",
    "        init_fc = [nn.init.uniform_(x) for x in self.fc_weights]\n",
    "        \n",
    "        self.fc_bias = nn.Parameter(torch.empty(1))\n",
    "        nn.init.uniform_(self.fc_bias)\n",
    "        \n",
    "    #def __init__(self):\n",
    "    #    super(LinearNet, self).__init__()\n",
    "    #    self.fc = nn.Linear(28*28, 1)\n",
    "    #    nn.init.normal(self.fc.weight)\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    x = self.fc(x)\n",
    "    #    return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #fc_layer = torch.cat(tuple(self.fc_weights)).unsqueeze(0)\n",
    "        #x = x @ fc_layer.T + self.fc_bias\n",
    "        for i, param in enumerate(self.fc_weights):\n",
    "            if i==0:\n",
    "                p=x[:,i]*param\n",
    "            else:\n",
    "                p += x[:,i]*param\n",
    "        x = p.unsqueeze(1) + self.fc_bias\n",
    "        return x\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "\n",
    "    def set_weights(self, keys, weights): \n",
    "        flatten_weights =  [item for sublist in weights for item in sublist]\n",
    "        self.load_state_dict({keys[i]:flatten_weights[i] for i in range(len(keys))})\n",
    "        \n",
    "    def get_gradients(self, keys):\n",
    "        grads = {}\n",
    "\n",
    "        for name, p in self.named_parameters():\n",
    "            if name in keys:\n",
    "                grad = None if p.grad is None else p.grad.data.cpu().numpy()\n",
    "                grads[name] = grad\n",
    "\n",
    "        return [grads[key] for key in keys]\n",
    "\n",
    "    def set_gradients(self, gradients):\n",
    "        for g, p in zip(gradients, self.parameters()):\n",
    "            if g is not None:\n",
    "                p.grad = torch.from_numpy(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote  \n",
    "class ParameterServer(object):\n",
    "    def __init__(self, keys, values):\n",
    "        self.weights = dict(zip(keys, values))\n",
    "\n",
    "    def apply_gradients(self, keys, lr, *values):\n",
    "\n",
    "        summed_gradients = [\n",
    "            np.stack(gradient_zip).sum(axis=0) for gradient_zip in zip(*values)\n",
    "        ]\n",
    "    \n",
    "        idx = 0\n",
    "        for key, value in zip(keys, summed_gradients):\n",
    "            self.weights[key] -= lr * torch.from_numpy(summed_gradients[idx])\n",
    "            idx+=1\n",
    "\n",
    "        return [self.weights[key] for key in keys]\n",
    "\n",
    "    def get_weights(self, keys):\n",
    "        return [self.weights[key] for key in keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class DataWorker(object):\n",
    "    def __init__(self, keys):\n",
    "        self.model = LinearNet()\n",
    "        self.data_iterator = iter(get_data_loader()[0])\n",
    "        self.keys = keys\n",
    "        self.key_set = set(self.keys)\n",
    "        for key, value in dict(self.model.named_parameters()).items():\n",
    "            if key not in self.key_set:\n",
    "                value.requires_grad=False\n",
    "\n",
    "        \n",
    "    def update_weights(self, keys, *weights):\n",
    "        self.model.set_weights(keys, weights)\n",
    "\n",
    "    def compute_gradients(self):\n",
    "        #self.model.set_weights(keys, weights)\n",
    "        try:\n",
    "            data, target = next(self.data_iterator)\n",
    "        except StopIteration:  # When the epoch ends, start a new epoch.\n",
    "            self.data_iterator = iter(get_data_loader()[0])\n",
    "            data, target = next(self.data_iterator)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        output = self.model(data)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, target.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        return self.model.get_gradients(self.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 12:54:45,538\tINFO worker.py:963 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "iterations = 100\n",
    "num_workers = 1 # number of workers per server\n",
    "num_servers = 10 # number of servers\n",
    "hashes_per_server = 100\n",
    "\n",
    "def Scheduler(num_servers, hashes_per_server=50):\n",
    "    \n",
    "    model = LinearNet()\n",
    "    key_values = model.get_weights()\n",
    "    keys = np.array(list(key_values.keys()))\n",
    "    #print(keys)\n",
    "    #print(key_values) z\n",
    "    values = [key_values[key] for key in keys]\n",
    "    #values = [key_values[key] for key in keys]\n",
    "    \n",
    "    key_indices = {key: x for x, key in enumerate(keys)}\n",
    "   \n",
    "    # distributing weights across servers - do this using consistency hashing\n",
    "    server_ids = [\"server\" + str(ind) for ind in range(num_servers)]\n",
    "    hasher = ConsistentHash(keys, server_ids, hashes_per_server)\n",
    "    servers = [ParameterServer.remote(keys[[key_indices[key] for key in hasher.get_keys_per_node()[serv]]], \n",
    "                                      [values[key_indices[key]] for key in hasher.get_keys_per_node()[serv]]) for serv in server_ids]\n",
    "    # servers = [ParameterServer.remote(keys[0:1], values[0:1]), ParameterServer.remote(keys[1:2], values[1:2])]\n",
    "    \n",
    "    return servers, keys, model, hasher.get_keys_per_node()\n",
    "\n",
    "servers, keys, model, weight_assignments =  Scheduler(num_servers, hashes_per_server)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# creating equal workers per server\n",
    "\n",
    "workers = [[DataWorker.remote(weight_assignments[\"server\" + str(j)]) for i in range(num_workers)] for j in range(num_servers)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight_assignments[\"server0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_data_loader()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running synchronous parameter server training.\n",
      "Iter 0: \taccuracy is 51.9\n",
      "Iter 10: \taccuracy is 51.9\n",
      "Iter 20: \taccuracy is 50.8\n",
      "Iter 30: \taccuracy is 49.1\n",
      "Iter 40: \taccuracy is 57.8\n",
      "Iter 50: \taccuracy is 67.8\n",
      "Iter 60: \taccuracy is 71.4\n",
      "Iter 70: \taccuracy is 73.6\n",
      "Iter 80: \taccuracy is 75.8\n",
      "Iter 90: \taccuracy is 77.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Running synchronous parameter server training.\")\n",
    "lr=0.1\n",
    "\n",
    "# we need to get a new keys order because we are not assuming a ordering in keys\n",
    "current_weights = []\n",
    "keys_order = []\n",
    "\n",
    "for j in range(num_servers):\n",
    "    keys_order.extend(weight_assignments[\"server\" + str(j)])\n",
    "    current_weights.extend(ray.get(servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]))) \n",
    "\n",
    "\n",
    "time_per_iteration = []\n",
    "total_time = []\n",
    "\n",
    "for i in range(iterations):\n",
    " \n",
    "    start_t = time()\n",
    "    \n",
    "    # sync all weights on workers\n",
    "    if i % 1 == 0:\n",
    "  \n",
    "        # get weights from server\n",
    "        #current_weights = [servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]) for j in range(num_servers)] \n",
    "\n",
    "        # update weights on all workers\n",
    "        [workers[j][idx].update_weights.remote(keys_order, *current_weights) for  idx  in range(num_workers) for j in range(num_servers)]\n",
    "    \n",
    "        \n",
    "    # use local cache of weights and get gradients from workers\n",
    "    gradients = [[workers[j][idx].compute_gradients.remote() for  idx  in range(num_workers)] for j in range(num_servers)]\n",
    "\n",
    "    start = time()\n",
    "    # Updates gradients to specfic parameter servers\n",
    "    current_weights_t = [servers[j].apply_gradients.remote(weight_assignments[\"server\" + str(j)], lr, *gradients[j]) for j in range(num_servers)]\n",
    "    \n",
    "    current_weights = ray.get(current_weights_t)\n",
    "    \n",
    "    end = time()\n",
    "    time_per_iteration.append(end-start)\n",
    "    total_time.append(end-start_t)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        # Evaluate the current model.\n",
    "        # current_weights = [servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]) for j in range(num_servers)] \n",
    "      \n",
    "        # we are once again using the server to key mapping to set the weight back\n",
    "        model.set_weights(keys_order, current_weights)\n",
    "        accuracy = evaluate(model, test_loader)\n",
    "        print(\"Iter {}: \\taccuracy is {:.1f}\".format(i, accuracy))\n",
    "    #rint(\"\\n\")\n",
    "\n",
    "#print(\"Final accuracy is {:.1f}.\".format(accuracy))\n",
    "# Clean up Ray resources and processes before the next example.\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17058638370398319"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(time_per_iteration[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6638362600345804"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(total_time[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014091162111100973"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(time_per_iteration[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019220059785417763"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(total_time[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.6862423419952393,\n",
       " 0.17242765426635742,\n",
       " 0.16946005821228027,\n",
       " 0.16181612014770508,\n",
       " 0.15832233428955078,\n",
       " 0.1652061939239502,\n",
       " 0.16287565231323242,\n",
       " 0.16765737533569336,\n",
       " 0.16626596450805664,\n",
       " 0.1544497013092041]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_per_iteration[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.935597658157349,\n",
       " 0.6603708267211914,\n",
       " 0.6554489135742188,\n",
       " 0.6534936428070068,\n",
       " 0.6458759307861328,\n",
       " 0.6504831314086914,\n",
       " 0.6508438587188721,\n",
       " 0.6521992683410645,\n",
       " 0.6536099910736084,\n",
       " 0.6401145458221436]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_time[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

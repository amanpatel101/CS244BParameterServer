{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from filelock import FileLock\n",
    "import numpy as np\n",
    "\n",
    "import ray\n",
    "\n",
    "def get_data_loader():\n",
    "    \n",
    "    \"\"\"Safely downloads data. Returns training/validation set dataloader.\"\"\"\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    \n",
    "    class MNISTEvenOddDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, ready_data):\n",
    "            self.img_data = ready_data.data\n",
    "            self.labels = ready_data.targets % 2\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "        def __getitem__(self, ind):\n",
    "            return torch.true_divide(self.img_data[ind].view(-1, 28 * 28).squeeze(), 255), torch.tensor([self.labels[ind]])\n",
    "\n",
    "\n",
    "    \n",
    "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
    "        \n",
    "        train_dataset = datasets.MNIST(\n",
    "                \"~/data\", train=True, download=True, transform=mnist_transforms\n",
    "            )\n",
    "        \n",
    "        test_dataset = datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            MNISTEvenOddDataset(train_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "             MNISTEvenOddDataset(test_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Evaluates the accuracy of the model on a validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # This is only set to finish evaluation faster.\n",
    "            if batch_idx * len(data) > 1024:\n",
    "                break\n",
    "            outputs = nn.Sigmoid()(model(data))\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = outputs > 0.5\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    \"\"\"Small Linear Network for MNIST.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc = nn.Linear(28*28, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "\n",
    "    def set_weights(self, keys, weights):  \n",
    "        self.load_state_dict({keys[i]:weights[i] for i in range(len(keys))})\n",
    "        \n",
    "    def get_gradients(self, keys):\n",
    "        grads = []\n",
    "        for name, p in self.named_parameters():\n",
    "            if name in keys: # this will return everything in the order of named_params we want this in order of keys\n",
    "                grad = None if p.grad is None else p.grad.data.cpu().numpy()\n",
    "                grads.append(grad)\n",
    "        return grads\n",
    "\n",
    "    def set_gradients(self, gradients):\n",
    "        for g, p in zip(gradients, self.parameters()):\n",
    "            if g is not None:\n",
    "                p.grad = torch.from_numpy(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote  \n",
    "class ParameterServer(object):\n",
    "    def __init__(self, keys, values):\n",
    "        values = [value.clone().detach() for value in values]\n",
    "        self.weights = dict(zip(keys, values))\n",
    "\n",
    "    def apply_gradients(self, keys, lr, *values):\n",
    "\n",
    "        summed_gradients = [\n",
    "            np.stack(gradient_zip).sum(axis=0) for gradient_zip in zip(*values)\n",
    "        ]\n",
    "    \n",
    "        idx = 0\n",
    "        for key, value in zip(keys, summed_gradients):\n",
    "            self.weights[key] -= lr * torch.from_numpy(summed_gradients[idx])\n",
    "            idx+=1\n",
    "            \n",
    "        return [self.weights[key] for key in keys]\n",
    "\n",
    "    def get_weights(self, keys):\n",
    "        return [self.weights[key] for key in keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class DataWorker(object):\n",
    "    def __init__(self):\n",
    "        self.model = LinearNet()\n",
    "        self.data_iterator = iter(get_data_loader()[0])\n",
    "        \n",
    "    def update_weights(self, keys, weights):\n",
    "        self.model.set_weights(keys, weights)\n",
    "\n",
    "    def compute_gradients(self, keys):\n",
    "        #self.model.set_weights(keys, weights)\n",
    "        try:\n",
    "            data, target = next(self.data_iterator)\n",
    "        except StopIteration:  # When the epoch ends, start a new epoch.\n",
    "            self.data_iterator = iter(get_data_loader()[0])\n",
    "            data, target = next(self.data_iterator)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        output = self.model(data)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, target.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        return self.model.get_gradients(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 17:23:44,059\tINFO worker.py:963 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "iterations = 200\n",
    "num_workers = 2 # number of workers per server\n",
    "num_servers = 2 # number of servers\n",
    "\n",
    "def Scheduler(num_servers):\n",
    "    \n",
    "    model = LinearNet()\n",
    "    key_values = model.get_weights()\n",
    "    keys = list(key_values.keys())\n",
    "    values = [key_values[key] for key in keys]\n",
    "    \n",
    "    # assuming keys are ordered\n",
    "    #approx_partition = len(keys)//num_servers\n",
    "    #servers = [ParameterServer.remote(1e-2, keys[i*approx_partition,(i+1)*approx_partition], values[i*approx_partition,(i+1)*approx_partition]) for i in range(num_servers)]\n",
    "\n",
    "    # distributing weights across servers - do this using consistency hashing\n",
    "    servers = [ParameterServer.remote(keys[0:1], values[0:1]), ParameterServer.remote(keys[1:2], values[1:2])]\n",
    "    \n",
    "    return servers, keys, model\n",
    "\n",
    "ps, keys, model =  Scheduler(2)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# creating equal workers per server\n",
    "\n",
    "workers = [[DataWorker.remote() for i in range(num_workers)] for j in range(num_servers)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_data_loader()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running synchronous parameter server training.\n",
      "Iter 0: \taccuracy is 75.3\n",
      "Iter 10: \taccuracy is 79.6\n",
      "Iter 20: \taccuracy is 80.0\n",
      "Iter 30: \taccuracy is 82.4\n",
      "Iter 40: \taccuracy is 82.9\n",
      "Iter 50: \taccuracy is 83.3\n",
      "Iter 60: \taccuracy is 82.9\n",
      "Iter 70: \taccuracy is 83.2\n",
      "Iter 80: \taccuracy is 83.8\n",
      "Iter 90: \taccuracy is 83.7\n",
      "Iter 100: \taccuracy is 83.8\n",
      "Iter 110: \taccuracy is 84.3\n",
      "Iter 120: \taccuracy is 84.1\n",
      "Iter 130: \taccuracy is 84.4\n",
      "Iter 140: \taccuracy is 84.7\n",
      "Iter 150: \taccuracy is 84.3\n",
      "Iter 160: \taccuracy is 84.5\n",
      "Iter 170: \taccuracy is 85.2\n",
      "Iter 180: \taccuracy is 85.3\n",
      "Iter 190: \taccuracy is 85.0\n",
      "Final accuracy is 85.0.\n"
     ]
    }
   ],
   "source": [
    "print(\"Running synchronous parameter server training.\")\n",
    "lr=0.01\n",
    "\n",
    "current_weights = []\n",
    "for j in range(num_servers):\n",
    "    current_weights.extend(ray.get(ps[j].get_weights.remote(keys[j:j+1]))) \n",
    "                       \n",
    "for i in range(iterations):\n",
    "    \n",
    "     # update weights on all workers for a server\n",
    "        \n",
    "    for j in range(num_servers):\n",
    "        for  idx  in range(num_workers):\n",
    "            workers[j][idx].update_weights.remote(keys, current_weights)\n",
    "    \n",
    "    \n",
    "    # get gradients\n",
    "    # we are using the key mapping here\n",
    "    gradients = [[workers[j][idx].compute_gradients.remote(keys[j:j+1]) for  idx  in range(num_workers)] for j in range(num_servers)]\n",
    "    \n",
    "    # Calculate updates after all gradients are available.\n",
    "    current_weights = []\n",
    "    for j in range(num_servers):\n",
    "        current_weights.extend(ray.get(ps[j].apply_gradients.remote(keys[j:j+1], lr, *gradients[j])))\n",
    "           \n",
    "    if i % 10 == 0:\n",
    "        # Evaluate the current model.\n",
    "\n",
    "        # we are once again using the server to key mapping to set the weight back\n",
    "        model.set_weights(keys, current_weights)\n",
    "        accuracy = evaluate(model, test_loader)\n",
    "        print(\"Iter {}: \\taccuracy is {:.1f}\".format(i, accuracy))\n",
    "\n",
    "print(\"Final accuracy is {:.1f}.\".format(accuracy))\n",
    "# Clean up Ray resources and processes before the next example.\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from filelock import FileLock\n",
    "import numpy as np\n",
    "import ray\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from consistent_hashing import ConsistentHash\n",
    "import math \n",
    "def get_data_loader():\n",
    "    \n",
    "    \"\"\"Safely downloads data. Returns training/validation set dataloader.\"\"\"\n",
    "    mnist_transforms = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "\n",
    "    # We add FileLock here because multiple workers will want to\n",
    "    # download data, and this may cause overwrites since\n",
    "    # DataLoader is not threadsafe.\n",
    "    \n",
    "    class MNISTEvenOddDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, ready_data):\n",
    "            self.img_data = ready_data.data\n",
    "            self.labels = ready_data.targets % 2\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "        def __getitem__(self, ind):\n",
    "            return torch.true_divide(self.img_data[ind].view(-1, 28 * 28).squeeze(), 255), torch.tensor([self.labels[ind]])\n",
    "\n",
    "\n",
    "    \n",
    "    with FileLock(os.path.expanduser(\"~/data.lock\")):\n",
    "        \n",
    "        train_dataset = datasets.MNIST(\n",
    "                \"~/data\", train=True, download=True, transform=mnist_transforms\n",
    "            )\n",
    "        \n",
    "        test_dataset = datasets.MNIST(\"~/data\", train=False, transform=mnist_transforms)\n",
    "        \n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            MNISTEvenOddDataset(train_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "             MNISTEvenOddDataset(test_dataset),\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    \"\"\"Evaluates the accuracy of the model on a validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # This is only set to finish evaluation faster.\n",
    "            if batch_idx * len(data) > 1024:\n",
    "                break\n",
    "            outputs = nn.Sigmoid()(model(data))\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = outputs > 0.5\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    \"\"\"Small Linear Network for MNIST.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc_weights = nn.ParameterList([nn.Parameter(torch.empty(1)) for weight in range(784)])\n",
    "        init_fc = [nn.init.uniform_(x) for x in self.fc_weights]\n",
    "        \n",
    "        self.fc_bias = nn.Parameter(torch.empty(1))\n",
    "        nn.init.uniform_(self.fc_bias)\n",
    "        \n",
    "    #def __init__(self):\n",
    "    #    super(LinearNet, self).__init__()\n",
    "    #    self.fc = nn.Linear(28*28, 1)\n",
    "    #    nn.init.normal(self.fc.weight)\n",
    "\n",
    "    #def forward(self, x):\n",
    "    #    x = self.fc(x)\n",
    "    #    return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #fc_layer = torch.cat(tuple(self.fc_weights)).unsqueeze(0)\n",
    "        #x = x @ fc_layer.T + self.fc_bias\n",
    "        for i, param in enumerate(self.fc_weights):\n",
    "            if i==0:\n",
    "                p=x[:,i]*param\n",
    "            else:\n",
    "                p += x[:,i]*param\n",
    "        x = p.unsqueeze(1) + self.fc_bias\n",
    "        return x\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return {k: v.cpu() for k, v in self.state_dict().items()}\n",
    "\n",
    "    def set_weights(self, keys, weights):  \n",
    "        self.load_state_dict({keys[i]:weights[i] for i in range(len(keys))})\n",
    "        \n",
    "    def get_gradients(self, keys):\n",
    "        grads = {}\n",
    "        #grads = []\n",
    "        for name, p in self.named_parameters():\n",
    "            if name in keys:\n",
    "                grad = None if p.grad is None else p.grad.data.cpu().numpy()\n",
    "                grads[name] = grad\n",
    "                #grads.append(grad)\n",
    "        return [grads[key] for key in keys]\n",
    "        #return grads\n",
    "\n",
    "    def set_gradients(self, gradients):\n",
    "        for g, p in zip(gradients, self.parameters()):\n",
    "            if g is not None:\n",
    "                p.grad = torch.from_numpy(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote  \n",
    "class ParameterServer(object):\n",
    "    def __init__(self, keys, values):\n",
    "        self.weights = dict(zip(keys, values))\n",
    "\n",
    "    def apply_gradients(self, keys, lr, *values):\n",
    "\n",
    "        summed_gradients = [\n",
    "            np.stack(gradient_zip).sum(axis=0) for gradient_zip in zip(*values)\n",
    "        ]\n",
    "    \n",
    "        idx = 0\n",
    "        for key, value in zip(keys, summed_gradients):\n",
    "            self.weights[key] -= lr * torch.from_numpy(summed_gradients[idx])\n",
    "            idx+=1\n",
    "            \n",
    "      #  return [self.weights[key] for key in keys]\n",
    "\n",
    "    def get_weights(self, keys):\n",
    "        return [self.weights[key] for key in keys]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class DataWorker(object):\n",
    "    def __init__(self, keys):\n",
    "        self.model = LinearNet()\n",
    "        self.data_iterator = iter(get_data_loader()[0])\n",
    "        self.keys = keys\n",
    "        self.key_set = set(self.keys)\n",
    "        for key, value in dict(self.model.named_parameters()).items():\n",
    "            if key not in self.key_set:\n",
    "                value.requires_grad=False\n",
    "\n",
    "    def update_weights(self, keys, weights):\n",
    "        self.model.set_weights(keys, weights)\n",
    "\n",
    "    def compute_gradients(self):\n",
    "        #self.model.set_weights(keys, weights)\n",
    "        try:\n",
    "            data, target = next(self.data_iterator)\n",
    "        except StopIteration:  # When the epoch ends, start a new epoch.\n",
    "            self.data_iterator = iter(get_data_loader()[0])\n",
    "            data, target = next(self.data_iterator)\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        output = self.model(data)\n",
    "        loss = nn.BCEWithLogitsLoss()(output, target.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        return self.model.get_gradients(self.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-06 19:01:31,136\tINFO worker.py:963 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "iterations = 500\n",
    "num_workers = 1 # number of workers per server\n",
    "num_servers = 2 # number of servers\n",
    "hashes_per_server = 50\n",
    "\n",
    "def Scheduler(num_servers, hashes_per_server=50):\n",
    "    \n",
    "    model = LinearNet()\n",
    "    key_values = model.get_weights()\n",
    "    keys = np.array(list(key_values.keys()))\n",
    "    #print(keys)\n",
    "    #print(key_values)\n",
    "    values = [key_values[key] for key in keys]\n",
    "    #values = [key_values[key] for key in keys]\n",
    "    \n",
    "    key_indices = {key: x for x, key in enumerate(keys)}\n",
    "   \n",
    "    # distributing weights across servers - do this using consistency hashing\n",
    "    server_ids = [\"server\" + str(ind) for ind in range(num_servers)]\n",
    "    hasher = ConsistentHash(keys, server_ids, hashes_per_server)\n",
    "    servers = [ParameterServer.remote(keys[[key_indices[key] for key in hasher.get_keys_per_node()[serv]]], \n",
    "                                      [values[key_indices[key]] for key in hasher.get_keys_per_node()[serv]]) for serv in server_ids]\n",
    "    # servers = [ParameterServer.remote(keys[0:1], values[0:1]), ParameterServer.remote(keys[1:2], values[1:2])]\n",
    "    \n",
    "    return servers, keys, model, hasher.get_keys_per_node()\n",
    "\n",
    "servers, keys, model, weight_assignments =  Scheduler(num_servers, hashes_per_server)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "# creating equal workers per server\n",
    "\n",
    "workers = [[DataWorker.remote(weight_assignments[\"server\" + str(j)]) for i in range(num_workers)] for j in range(num_servers)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight_assignments[\"server0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_data_loader()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running synchronous parameter server training.\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "Iter 0: \taccuracy is 51.9\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "Iter 10: \taccuracy is 51.9\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "Iter 20: \taccuracy is 52.3\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "Iter 30: \taccuracy is 51.0\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17358)\u001b[0m 432\n",
      "\u001b[2m\u001b[36m(DataWorker pid=17360)\u001b[0m 353\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12937/829771251.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_servers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mkeys_order\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_assignments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"server\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mcurrent_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mservers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_assignments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"server\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# update weights on all workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1803\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebugger_breakpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1804\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m                     ]\n\u001b[1;32m    375\u001b[0m         return (\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_metadata_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_refs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mdebugger_breakpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mdeserialize_objects\u001b[0;34m(self, data_metadata_pairs, object_refs)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_actor_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_serialization_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_metadata_pairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_refs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/ray/serialization.py\u001b[0m in \u001b[0;36mdeserialize_objects\u001b[0;34m(self, data_metadata_pairs, object_refs)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;31m# the object ref knows where it comes from.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_ref_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deserialize_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/ray/serialization.py\u001b[0m in \u001b[0;36m_deserialize_object\u001b[0;34m(self, data, metadata, object_ref)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mray_constants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJECT_METADATA_TYPE_PYTHON\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             ]:\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deserialize_msgpack_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0;31m# Check if the object should be returned as raw bytes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmetadata_fields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mray_constants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJECT_METADATA_TYPE_RAW\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/ray/serialization.py\u001b[0m in \u001b[0;36m_deserialize_msgpack_data\u001b[0;34m(self, data, metadata_fields)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadata_fields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mray_constants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJECT_METADATA_TYPE_PYTHON\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mpython_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deserialize_pickle5_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle5_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mpython_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/ray/serialization.py\u001b[0m in \u001b[0;36m_deserialize_pickle5_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_band\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_band\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;31m# cloudpickle does not provide error types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    711\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ParamServer/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    938\u001b[0m         typed_storage._storage._set_from_file(\n\u001b[1;32m    939\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             torch._utils._element_size(typed_storage.dtype))\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Running synchronous parameter server training.\")\n",
    "lr=0.1\n",
    "\n",
    "# we need to get a new keys order because we are not assuming a ordering in keys\n",
    "current_weights = []\n",
    "keys_order = []\n",
    "\n",
    "for j in range(num_servers):\n",
    "    keys_order.extend(weight_assignments[\"server\" + str(j)])\n",
    "    current_weights.extend(ray.get(servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]))) \n",
    "                       \n",
    "for i in range(iterations):\n",
    "    \n",
    "    # sync all weights on workers\n",
    "    if i % 1 == 0:\n",
    "        current_weights = []\n",
    "        keys_order = []\n",
    "\n",
    "        # get weights from server\n",
    "        for j in range(num_servers):\n",
    "            keys_order.extend(weight_assignments[\"server\" + str(j)])\n",
    "            current_weights.extend(ray.get(servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]))) \n",
    "   \n",
    "        # update weights on all workers\n",
    "        for j in range(num_servers):\n",
    "            for  idx  in range(num_workers):\n",
    "                workers[j][idx].update_weights.remote(keys_order, current_weights)\n",
    "    \n",
    "    \n",
    "    # use local cache of weights and get gradients from workers\n",
    "    gradients = [[workers[j][idx].compute_gradients.remote() for  idx  in range(num_workers)] for j in range(num_servers)]\n",
    "    \n",
    "    # Updates gradients to specfic parameter servers\n",
    "    [servers[j].apply_gradients.remote(weight_assignments[\"server\" + str(j)], lr, *gradients[j]) for j in range(num_servers)]\n",
    "           \n",
    "    if i % 10 == 0:\n",
    "        # Evaluate the current model.\n",
    "\n",
    "        for j in range(num_servers):\n",
    "            keys_order.extend(weight_assignments[\"server\" + str(j)])\n",
    "            current_weights.extend(ray.get(servers[j].get_weights.remote(weight_assignments[\"server\" + str(j)]))) \n",
    "   \n",
    "        # we are once again using the server to key mapping to set the weight back\n",
    "        model.set_weights(keys_order, current_weights)\n",
    "        accuracy = evaluate(model, test_loader)\n",
    "        print(\"Iter {}: \\taccuracy is {:.1f}\".format(i, accuracy))\n",
    "\n",
    "print(\"Final accuracy is {:.1f}.\".format(accuracy))\n",
    "# Clean up Ray resources and processes before the next example.\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_bias Parameter containing:\n",
      "tensor([0.0276], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for key, value in dict(model.named_parameters()).items():\n",
    "    print(key, value)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ParamServer",
   "language": "python",
   "name": "paramserver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
